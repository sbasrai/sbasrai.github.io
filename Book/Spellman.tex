\documentclass{article}
%\usepackage[english]{babel}%
\usepackage{graphicx}
\usepackage{tabulary}
\usepackage{tabularx}
\usepackage[table,xcdraw]{xcolor}
\usepackage{pdflscape}
\usepackage{lastpage}
\usepackage{multirow}
\usepackage{cancel}
\usepackage{amsmath}
\usepackage[table]{xcolor}
\usepackage{fixltx2e}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{ifthen}
\usepackage{fancyhdr}
\usepackage[document]{ragged2e}
\usepackage[margin=1in,top=1.2in,headheight=57pt,headsep=0.1in]
{geometry}
\usepackage{ifthen}
\usepackage{fancyhdr}
\everymath{\displaystyle}
\usepackage[document]{ragged2e}
\usepackage{fancyhdr}
\usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\everymath{\displaystyle}
\linespread{2}%controls the spacing between lines. Bigger fractions means crowded lines%
%\pagestyle{fancy}
%\usepackage[margin=1 in, top=1in, includefoot]{geometry}
%\everymath{\displaystyle}
\linespread{1.3}%controls the spacing between lines. Bigger fractions means crowded lines%
%\pagestyle{fancy}
\pagestyle{fancy}
\setlength{\headheight}{56.2pt}


\chead{\ifthenelse{\value{page}=1}{\includegraphics[scale=0.3]{BassettCTCLogo}\\ \textbf \textbf Water Treatment I}}
\rhead{\ifthenelse{\value{page}=1}{Shabbir Basrai}{Shabbir Basrai}}
\lhead{\ifthenelse{\value{page}=1}{}{\textbf Water Treatment I}}


\cfoot{}
\lfoot{Page \thepage\ of \pageref{LastPage}}
\rfoot{Module 8}
\renewcommand{\headrulewidth}{2pt}
\renewcommand{\footrulewidth}{1pt}
\begin{document}

The term sick water was coined by the United Nations in a 2010 press
release addressing the need to recognize that it is time to arrest the
global tide of sick water. The gist of the United Nation's report was
that transforming waste from a major health and environmental hazard
into a clean, safe, and economically attractive resource is emerging as
a key challenge in the 21st century. Practitioners of environmental
health certainly support the United Nation's view on this important
topic. When discussing sick water in the context of this text, however,
it is necessary to go a few steps further than the United Nations did to
describe the real essence and tragic implications of supposedly potable
water that makes people or animals sick or worse. Water that is sick is
actually filthy spent water or wastewater---a cocktail of fertilizer
runoff and sewage disposal alongside animal, industrial, agricultural,
and other wastes. In addition to these listed wastes of concern, other
wastes are beginning to garner widespread attention. What are these
other wastes? Any waste or product that we dispose of in our waters,
that we flush down the toilet, pour down the sink or bathtub drain, or
pour down the drain of a worksite deep sink. Consider the following
example of pollutants we routinely discharge to our wastewater treatment
plants or septic tanks---wastes we don't often consider as waste
products but that in reality are. Each morning a family of four, two
adults and two teenagers, wakes up and prepares for the day that lies
ahead. Fortunately, this family has three upstairs bathrooms to
accommodate everyone's needs, and each day the family's natural wastes,
soap suds, cosmetics, hair treatments, vitamins, sunscreen, fragrances,
and prescribed medications end up down the various drains. In addition,
the overnight deposits of cat and dog waste are routinely picked up and
flushed down the toilet. Let's examine a short inventory of what this
family of four has disposed of or has applied to them- selves during
their morning rituals:

• Toilet-flushed animal wastes • Prescription and over-the-counter
therapeutic drugs • Veterinary drugs • Fragrances • Soap • Shampoo,
conditioner, other hair treatment products • Body lotion, deodorant,
body powder • Cosmetics • Sunscreen products • Diagnostic agents •
Nutraceuticals (e.g., vitamins, medical foods, functional foods)

Even though these bioactive substances have been around for decades,
today they are all (with the exception of animal wastes) grouped under
the title of pharmaceuticals and personal care products, or PPCPs (see
Figure 1). Returning to our family of four, after having applied, used,
or ingested the various substances mentioned earlier, they also add at
least traces of these products (PPCPs) to the environment through
excretion (the elimination of waste material from the body) and bathing,
as well as through disposal of any unwanted medications to sewers and
trash. How many of us have found old prescriptions in the family
medicine cabinet and disposed of them with a single toilet flush? Many
of these medications (e.g., antibiotics) are not normally found in the
environment. Earlier we stated that wastewater is a cocktail of
fertilizer runoff and sewage disposal with additions of animal,
industrial, agricultural, and other wastes. When we add PPCPs to this
cocktail we can state that we are simply adding mix to the mix. This
mixed-waste cocktail raises many questions: Does the disposal of
antibiotics or other medications into the local wastewater treatment
system cause problems for anyone or anything downstream? When we drink
locally treated tap water are we also ingesting flushed-down-the-toilet
or rinsed-down-the-drain antibiotics, other medications, illicit drugs,
animal excretions, cosmetics, vitamins, personal or household cleaning
products, sunscreen products, diagnostic agents, crankcase oil, grease,
oil, fats, and veterinary drugs and hormones? If the family of four were
shown the list of pharmaceuticals and personal care products they
routinely used and disposed of each morning, they might be surprised,
impressed, or totally uninterested. Suppose, however, that after this
family was shown the list of products they used and disposed of in the
wastestream that left their home, they were asked if they would be
willing to drink from that same wastestream. It is very likely that they
would no longer be disinterested. In fact, they would probably be
experiencing a queasy feeling in their stomachs that is commonly
referred to as the ``yuck factor.'' The yuck factor raised by drinking
human-created wastestreams is grossly overstated. The fact is we have
been drinking from these wastestreams from time immemorial---beyond time
or memory. Consider the mythical hero Hercules (arguably the world's
first environmental engineer), who performed his fifth labor by cleaning
up King Augeas' stables. Hercules, faced literally with a mountain of
horse and cattle waste piled high in the stable area, had to devise some
method to dispose of the waste. He diverted a couple of rivers to the
stable interior, and they carried off all of the animal waste: Out of
sight, out of mind. The waste followed the laws of gravity and flowed
downstream, becoming someone else's problem. Hercules understood the
principal point in pollution control technology, one that is pertinent
to this very day: Dilution is the solution to pollution. Apart from
Hercules' reasonable approach to disposing of waste in his time, another
factor to consider is de facto water recycling. When the family of four
and others say they would never drink toilet water, they have no idea
what they are saying. As pointed out in the third edition of my
textbook, The Science of Water, the fact is we drink recycled wastewater
every day via the de facto water cycle, which turns the yuck factor into
an ``awe'' factor. Wastewater treatment plants throughout the
industrialized world treat wastewater or used water and then discharge
the treated water to major rivers or other local water bodies. Many of
those region's rivers are sources of local drinking water supplies, and
local groundwater supplies are routinely infiltrated with surface water
inputs, which, again, are commonly supplied by treated wastewater (and
sometimes infiltrated by raw sewage that is accidentally spilled).
Nature takes care of the water pollution problem---to an extent;
however, PPCPs may be the exception. The water cycle process results in
some uncertainty regarding their fate, which is discussed in the text.
The jury is still out on the topic of PPCPs. We simply do not know what
we do not know about the fate of PPCPs or their impact on the
environment once they enter our wastewater treatment systems, the water
cycle, and eventually our drinking water supply systems. We do know that
some PPCPs are easily broken down and processed by the human body or
degraded quickly in the environment, but the disposal of certain wastes
can be problematic for quite some time. Another issue related to
contaminants in our water systems is one many of us never think about.
Water is used by all living organisms, including wildlife and aquatic
life. The fact is water pollution can severely harm marine, avian, and
land animals. In addition to PPCPs, other contaminants found in our
water bodies include the following:

• Raw sewage running into lakes, rivers or streams • Industrial waste
spills contaminating groundwater • Radiation spills or nuclear accidents
• Illegal dumping of substances or items within bodies of water •
Biological contamination, such as bacterial growth • Farm runoff into
nearby bodies of water It is also important to point out that drinking
water sources and animal usage represent a double-edged sword. Animals
have a right to drink from unpolluted and safe drinking water sources,
but wildlife, farm animals, and other animals also contribute to
drinking water pollution. This is why when in the woods or at campsites
or other remote locates it is always wise to filter river, stream, or
lake water with a 1-micron filter prior to drinking the water. The fly
in this pollution solution ointment is today's modern PPCPs. Although
Hercules was able to dispose of animal waste into a running water system
where eventually the water's self-purification process cleaned the
stream, he did not have to deal with today's personal pharmaceuticals
and the hormones that are given to many types of livestock to enhance
health and growth. Studies show that pharmaceuticals are present in our
nation's water bodies, and research suggests that certain drugs may
cause ecological harm. The USEPA and other research agencies are
committed to investigating this topic and developing strategies to help
protect the health of both the environment and the public. To date,
scientists have found no hard evidence of adverse human health effects
from PPCPs in the environment. Some might argue that these PPCPs
represent only a small fraction (expressed in parts per trillion,
10--12) of the total volume of water, that we are speaking of a
proportion equivalent to 1/20 of a drop of water diluted into an
Olympic-size swimming pool. One student in an environmental health class
stated that he did not think the water should be called ``sick water,''
as it was evident to him that water containing so many medications could
not be sick. Instead, it might be termed ``well water,'' with the
potential to make anyone who drinks it well. It is important to point
out that the term sick water can be applied not only to
PPCP-contaminated water but also to any filthy, dirty, contaminated,
polluted, pathogen-filled drinking water sources. The fact is dirty or
sick water means that, worldwide, more people now die from contaminated
and polluted water than from all forms of violence, including wars.* The
United Nations observed that dirty or sick water is a key factor in the
rise of deoxygenated dead zones that have been emerging in seas and
oceans across the globe. Preface to Third Edition The first and second
editions of The Drinking Water Handbook were industrywide bestsellers
hailed as masterly accounts written in an engaging, highly readable
style. The third edition continues where the first two editions
began---that is, stressing that notwithstanding our absolute need to
breathe untainted air nothing is more important to us than the quality
of the water we drink, although, of course, we need clean water for
other uses as well. Written with the practitioner, student, novice, or
sophisticated consumer in mind, this new edition of The Drinking Water
Handbook has been thoroughly revised and updated and includes a
comprehensive discussion of the Flint, Michigan, lead contamination
event, pharmaceuticals and personal care products (PCPPs), and endocrine
disruptors. With regard to our absolute need for air, water, and food,
it is important to keep the 5--5--5 Rule in mind: Human beings can
survive approximately 5 minutes without breathable air, 5 days without
water, and 5 weeks without food. Keep in mind that this rule varies for
each individual simply because we are all different and have different
requirements. As a whole, however, the 5--5--5 Rule points to our
absolute need for the three basic necessities needed to maintain life as
we know it. All of the major cities of the modern world grew up on
waterfronts but not because people require such large amounts of water
for survival. People typically require no more than 10 pounds of water
to create each pound of flesh, but to make a pound of paper requires
approximately 250 pounds of water and to produce one pound of fertilizer
requires 600 pounds. It is obvious that large cities developed near
water primarily because of industry demands for a reliable water supply.
In the United States, industry uses over 100 cubic miles of water every
year to cool, wash, and circulate its materials, an amount equal to 30\%
of all the water in the rivers of the world. Of this water we use, very
little goes back cleaner than when taken from its source, because as
water travels it bears with it the story of where it has been and what
it has been used for. This text recognizes the value of water for use in
industry but is not about the industrial use of freshwater; instead, the
focus here is on the use of freshwater by humans, who need pure, sweet,
clean water to sustain them. This text is about the technology available
and required to ensure that the water from our taps is safe. The
Drinking Water Handbook focuses on keeping our drinking water supplies
safe, on current problems with our drinking water supply, and on the
technologies available to mitigate the problems. The discussion in this
text relating to solutions and technologies is not the result of a
``feel good'' approach but rather is based on science and technology.
Concern over water quality is not new. Throughout the history of human
civilization, concern over the availability of clean drinking water has
played an instrumental role in determining where people chose to settle
and how these settlements grew into the cities of today. Those of us who
reside in the United States are blessed with an abundant freshwater
supply. Technology has even allowed us to provide for our arid areas;
however, even with that abundance, economic development and population
growth are straining the quality and quantity of water available for
drinking. Trillions of gallons of precipitation fall on the United
States every day, filling streams, rivers, ponds, lakes, and marshes.
That water then percolates through the natural filter that is soil to
recharge underground freshwater aquifers. Each day, agricultural
irrigators, industrial users, factories, and homeowners withdraw
hundreds of billions of gallons from this finite water supply. We use
this water for everything from washing dishes and watering the garden to
cooling the equipment of industrial complexes. After we are finished
with it, the water (a substance always and forever in motion) finds a
path back into the water cycle---into a stream, river, pond, lake,
marsh, or groundwater supply---along with whatever contaminants it
picked up along the way. When we open our taps to fill our glasses with
drinking water, we expect good quality of the water as a basic right. As
far as most of us are concerned, what comes from the tap is safe and
will cause us no harm. Is this really the case, though? Is the water
from our taps really safe? We are hearing now that cancer-causing
chemicals exist in virtually every public water supply in the United
States. As water pours forth from the tap into our drinking glasses,
another point of concern arises. Has the water been tested in accordance
with applicable standards or requirements? Were the tests reliable, or
not? Most public health officials claim that our drinking water is safe,
but do they really know that for sure? Are federal and state standards
for water safety adequate? This revised and updated edition of The
Drinking Water Handbook provides technical information regarding what
can be found in many tap-water supplies and the measures taken to ensure
the health and well-being of consumers. The Drinking Water Handbook
starts at the source itself, and describes the water purification
process through distribution to the tap, to our actual use and reuse of
water. Water, a substance we constantly use and reuse, is recycled via
the hydrologic (water) cycle. This text focuses on a particular water
cycle, the artificial water cycle that we have created, control, and are
utterly dependent on. Called the urban water cycle, it consists of the
water supply, water purification, water use, and water disposal for
reuse common in major metropolitan areas---a manmade cycle that mimics
the natural water cycle. As water users directly affected by the quality
of our water, we must take the necessary steps to protect our health by
making sure that the water available for drinking is safe---a task not
easy to accomplish. We cannot tell the quality of our water just by
looking at it; we know that water can look clear in the glass and still
contain toxic chemicals or bacterial and viral pathogens that can make
users sick. To purify water, communities rely on municipal treatment
plants and a variety of technologies ranging from simple screens, sand
filtration, and disinfection to complex chemical and mechanical
processes. These systems are not fail safe, though. When they do fail
(more often than you might think), water users are left vulnerable to a
wide variety of biological and chemical hazards, whether they know it or
not. In 1993, a microscopic organism called Cryptosporidium caused more
than 400,000 illnesses in Milwaukee, Wisconsin, and left 100 people
dead. In 1994, two more outbreaks of the same protozoa killed 19 and
sickened more than 100 in Las Vegas, Nevada. Panic over Cryptosporidium
and Giardia caused a 2-month-long boil-alert crisis in Sydney,
Australia, between July and September of 1998, one that

ended up costing millions of dollars, although no illnesses resulted.
That treatment facility paid enormous penalties for incompetent
testing---and for not following the maxim, ``Better safe than sorry.''
Many water users and technologists are no longer ignorant of the current
drinking water crisis; the publicity generated by the events in
Milwaukee, Las Vegas, and Sydney took care of that. The reappearance of
Cryptosporidium had an immediate effect. Microbiological parameters and
controls returned to the forefront, after having been demoted in the
1970s; disinfection, along with more sophisticated water treatment, came
back into favor. Overnight, Cryptosporidium and Giardia became urgent
targets of concern, and the fear of carcinogens (e.g., radon, lead,
arsenic) was no longer at the top of the regulatory agenda. In late
1998, concerns about Cryptosporidium and Giardia were joined by not
necessarily a new concern but a concern with new emphasis: disinfection
byproducts, including halogenated chloroorganic compounds such as
trihalomethanes (THMs). A partial result of these concerns has been the
emergence of a new bottled-water industry, one growing at tremendous
speed. Consumers want assurance that their water is safe, no matter
what, and the current perception is that bottled water is safer than tap
water. Another problem, although not a new one, is pharmaceuticals and
personal care products (PPCPs) as pollutants. These pollutants are
derived from products used by individuals for personal health or
cosmetic reasons or used by agribusiness to enhance the growth or health
of livestock. Comprising a diverse collection of thousands of chemicals
substances, PPCPs include prescription and over-the-counter therapeutic
drugs, veterinary drugs, fragrances, and cosmetics. PPCPs have been
present in water and the environment for as long as humans have been
using them. The problem with PPCPs in our water supplies is that we do
not know what we do not know about their possible impact on human health
and the environment. Recent advances in technology, however, have
improved our ability to detect and quantify these chemicals, so we can
now begin to identify what effects, if any, these chemicals have on
human and environmental health. We discuss each of these pressing
concerns in this edition of the text. In the not too distant past,
determining whether a surface water source for drinking water was
contaminated was accomplished by placing a healthy fish into a stream.
If the fish died, the source was contaminated and therefore had to be
purified. The degree of contamination was calculated by dividing 100 by
the survival time in minutes. Our testing is by far more complex today,
but sometimes not much more reliable. Although primarily designed as an
information source and presented in simple, straightforward,
easy-to-understand plain English, The Drinking Water Handbook also
provides a level-headed account, based on years of extensive research,
of drinking water quality. The Drinking Water Handbook is suitable for
use by both the technical practitioner in the field and by students in
the classroom. Here is all the information you need to make technical or
personal decisions about drinking water.

\hfill\break

Introduction SETTING THE STAGE Our goal in this book is to take a
prosaic subject and make it endlessly fascinating \ldots{} and totally
appreciated. Many might question how anyone could make the topic of
water fascinating, but consider that water is a contradiction, a riddle.
A case in point is an old Chinese proverb: ``Water can both float and
sink a boat.'' Water's presence everywhere feeds these contradictions.
Lewis (1996) pointed out that, ``Water is the key ingredient of mother's
milk and snake venom, honey and tears.'' Leonardo da Vinci gave us
insight into more of these apparent contradictions:

Water is sometimes sharp and sometimes strong, sometimes acid and
sometimes bitter. Water is sometimes sweet and sometimes thick or thin.
Water sometimes is seen bringing hurt or pestilence, sometimes
health-giving, sometimes poisonous. Water suffers changes into as many
natures as are the different places through which it passes. Water, as
with the mirror that changes with the color of its object, so it alters
with the nature of the place, becoming: noisome, laxative, astringent,
sulfurous, salt, incarnadined, mournful, raging, angry, red, yellow,
green, black, blue, greasy, fat or slim. Water sometimes starts a
conflagration, sometimes it extinguishes one. Water is warm and is cold.
Water carries away or sets down.

Water hollows out or builds up. Water tears down or establishes. Water
empties or fills. Water raises itself or burrows down. Water spreads or
is still. Water is the cause at times of life or death, or increase of
privation, nourishes at times and at others does the contrary. Water, at
times has a tang, at times it is without savor. Water sometimes
submerges the valleys with great flood. In time and with water,
everything changes.

We can sum up water's contradictions by stating that, although the globe
is awash in it, water is no single thing but an elemental force that
shapes our existence. Leonardo's last observation, ``In time and with
water, everything changes,'' concerns us most in this text. Why? We
stated in the Preface that, next to the air we breathe, the water we
drink is most important to us---to all of us. Water is no less important
than air, simply less urgent. For all of us, although we treat it
casually, unthinkingly, water is not a novelty but a necessity. We
simply cannot live without water. Some might view such statements about
the vital importance of water (commonly and incorrectly considered a
rather plain and simple substance) as nothing more than hyperbole,
exaggeration, panic, or overstatement. But are they? Is the concern for
safe drinking water really an exaggeration? No, it is not, because we
were all born of water and to live we must be sustained by it. The
development of safe drinking water supplies is a major concern today,
which might seem strange to those who might literally be surrounded by
various bodies of water. Drinking water practitioners---those
responsible for finding a source of drinking water, certifying its
safety, and providing it to the consumer---know that two key concerns
drive the development of safe drinking water supplies: quantity and
quality. Herein lies the problem. Quantity may indeed be a major issue
(a limiting factor) for a particular location, often simply because
water suitable for consumption is not evenly distributed throughout the
world. Those locations fortunate to have an ample supply of surface
water or groundwater may not have a quantity problem, as long as the
quantity is large enough to fulfill the needs of all its consumers. But,
again, not every geographical location is fortunate enough to have an
adequate water supply---that is, a quantity of water available to
satisfy the residents' needs. Of course, this is one of the primary
reasons why major portions of the globe are either uninhabited or
sparsely populated at best. Not all the news on the topic of water is
bad. Extreme water quantity issues do not necessarily suggest that a
location will fall victim to scarcity. Water quantity issues can be
managed. Given the right information and a high degree of common sense,
locations facing extremely significant quantity issues can implement
management and conservation strategies to secure their water supplies.
Take Singapore, for example. The area is highly populated with a booming
economic capacity, even though the location lacks adequate surface or
groundwater sources of water. Singapore's demand for water far exceeds
its naturally occurring supply, so if one wants to look at an example of
effective water management, then Singapore is the place. To meet its
freshwater needs, Singapore invests heavily in technology, responsible
management, and international agreements. For example, forward-thinking
and innovative management practices, including the use of advanced
rainwater capture systems, account for at least 20\% of Singapore's
water supply, where the total annual precipitation averages 2150 mm
(84.6 in.), equivalent to 2150 L/m2 or 52.73 gal/ft2. Another 40\% of
Singapore's water supply is imported from Malaysia. The use of gray
water or sullage (i.e., wastewater generated in homes and offices that
is non-toilet water) adds 30\% to the total, with desalinization
producing the remaining 10\% of the supply necessary to meet the
location's total demand. Unfortunately, Singapore is the exception
today, not the trend. For this reason, among other contributing factors,
major portions of the globe remain either uninhabited or sparsely
populated. The other key concern (and the main focus of this text) is
water quality. Obviously, having a sufficient quantity of freshwater
available does little good if the water is unsafe for consumption or for
other uses. There is another issue; namely, it is rather easy to
determine the quantity of a substance such as water. We can say there is
too little, enough, or too much. A quantity is a metric that indicates
or signifies a number. In the case of water, quantity can be expressed
as the number of gallons or acre-feet of water available or not
available. Trying to quantify the quality of water is an entirely
different matter, though. Quality is a characteristic that often is a
judgment call, but the problem with judging quality is that it can be
subjective. To make our point about quality being a judgment call,
consider Figure 1.1. The figure shows three glasses, labeled A, B, and
C. Each glass contains icewater. The glasses were filled from three
different household taps and three different household ice-cube trays
from three different locations. If you had a choice of which of the
three glasses of icewater to drink, which would you choose? Discerning
individuals, people who are concerned about the appearance of their
water, would probably make a judgment call and choose glass A because it
looks like it contains cleaner water. Others might select either glass A
or glass B, because there appears to be such a slight difference between
the appearances of each that it is not a big deal. Again, a judgment
call. On the other hand, glass C would probably only be chosen by
someone so thirsty, so desperate, or so blind as to not care which glass
they are drinking from---a nonjudgmental call, for sure. Sometimes,
there might not be a real choice. Maybe only the water shown in glass C
is available. Is that not what happened in Flint, Michigan? Figure 1.1
makes our point about quality and personal judgment calls. Again,
although quantity can usually be expressed using simple metrics (a
gallon or ounce or teaspoon of this or that, etc.), quality is a
completely different issue. The appearance of anything we intend to put
into our mouths is certainly a quality factor, but keep in mind that
quality judgments can be based on other factors, as well. In the case of
drinking water, we expect that our glass of tap water will contain water
that is odorless, tasteless, and transparent. We also expect our glass
of tap water to be safe. We expect it to be pure and free from
biological or chemical contamination. This point brings us to another
important aspect concerning the three glasses of water shown in Figure
1.1. It was suggested that the discerning person would most likely
choose glass A simply because it looks like it contains the cleanest
water of the three glasses. But is it really the safest water? How would
one know for sure that glass A does not contain deadly bacteria? How
would one know for sure that the same glass of water does not contain
invisible and harmful chemicals? In addition, a person who is both
discerning and knowing might question why none of the ice is floating in
any of the three glasses. Why aren't the ice cubes floating? Is there
something in the water used to make the ice cubes that is preventing
them from floating? Because the ice cubes are not floating, water
scientists would avoid all three glasses. Later discussion addresses
water contaminants and how they are prevented from entering or are
removed from our tap water supplies. The bottom line: Obviously, having
a sufficient quantity of freshwater available does little good if the
water is unsafe for consumption or for other uses. Let's return to our
stage setting for the contents of this book. We began this chapter by
revealing some of the contradictions water presents, but there is
another one--- one that human beings bear considerable responsibility
for. Consider that most of the early settlements of the world began
along waterways. Waterways were important primarily because of the ease
of transportation they afforded and because of their industrial value
(e.g., water power), in addition to serving as a source of food for the
settlers. And, of course, such waterways provided a natural, relatively
clean, relatively safe source of drinking water. Unfortunately, these
early waterways soon became polluted. This should not be surprising,
though, as humans have always been polluters. We can actually say that
pollution is a natural byproduct of civilization. We eat, we work, we do
whatever is necessary to sustain our existence, and in doing so we
pollute. Are we the only freshwater polluter? Not really. Natural events
also pollute our water sources, especially our surface waters; for
example, a stream that flows through a heavily wooded area (such as a
deciduous forest) suffers from the effects of natural pollution each
year during leaf-fall. When leaves fall from their lofty perches and
make their sinuous descent into the blue--green phantoms we call surface
streams below, they are carried with the flow, drifting until they sink,
are saturated, or become lodged in an obstruction in the stream. Leaves
are organic and eventually degrade. During this process, the microbes
degrading the leaves take up and use dissolved oxygen in the water. In
some cases, the amount of oxygen used during degradation is of such
quantity, especially in slowmoving or stagnant areas of water, that the
natural biota in the stream suffer from

a lack of oxygen. When this happens, they either move on to healthier
parts of the stream or succumb because of a lack of oxygen. Note that
other natural water polluters also affect water quality, including
forest fires, earthquakes, and floods. Remember that Nature understands
the contradictions of water \ldots{} and because she understands, she is
also well suited and equipped to deal with such problems. When a stream
becomes polluted (for whatever reason), Nature immediately goes to work
to set in motion natural processes---known as
self-purification---designed to restore the stream to its normal,
healthy state. Only when such streams become overloaded with pollution
or with nonbiodegradable manmade contaminants does Nature have
difficulty in restoring the stream to its normal quality. Legend also
has it that Alexander the Great discovered Adam's will, which indicated
that God had created a spring beyond the mountains surrounding the
world, in the Land of Darkness. This spring was unique---``whiter than
milk, colder than ice, sweeter than honey, softer than butter, and
sweeter smelling than musk'' (Varner, 2006). Those who drank from it
would be granted eternal life. Will this book lead

you to that spring and eternal life? Probably not. Instead, this text is
intended to serve as a convenient, quick reference and to provide
technical support in the hands of the general public, sanitary
engineers, public health administrators, public works engineers, water
treatment operators, and college students in environmental health or
public health engineering. The purpose of this handbook is to evaluate
and emphasize drinking water quality control, from the source to the
treatment plant, from the distribution system to the consumer. The goal
is to look closely at the factors that affect water quality, including
trihalomethanes, Cryptosporidium, viruses, carcinogens, and
polychlorinated biphenyls, in addition to the traditional physical,
chemical, and bacteriological parameters. This handbook addresses the
challenges faced by drinking water practitioners striving to provide the
best drinking water quality to the consumer. Specifically, it deals with
the nature of these challenges (unsafe drinking water) and their
solutions (how to make it safe), in addition to exploring techniques
that can be employed to mitigate the problem of unsafe drinking water
through technology. The focus is on science and technology rather than
``feel good'' approaches simply because of a clear understanding that
technology and politics are seldom a rational mix. All About Water:
Basic Concepts EARTH'S BLOOD The watery environment in which single-cell
organisms live provides them food and removes their wastes, a function
that the human circulatory system similarly provides for the 60 to 100
trillion cells in a human body. The circulatory system brings each cell
its daily supply of nutritive amino acids and glucose and carries away
waste carbon dioxide and ammonia, which will be filtered out of our
systems and flushed away through micturition and excretory functions.
The heart, the center of our circulatory system, keeps blood moving on
its predetermined circular path, a function so essential that if the
pump fails we quickly fail as well---and we die. As single-celled
organisms no longer, humans sometimes assume that they no longer need a
watery environment in which to live---but they aren't paying close
attention to the world around them. Actually, those of us who live on
Earth are as dependent upon the Earth's circulatory system as we are on
our own circulatory system. Just as the human heart pumps blood,
circulating it through a series of vessels, and just as our lives are
dependent upon that flow of blood, so life on Earth is dependent on the
Earth's water cycle.

7

This cycle is so automatic that we generally ignore it until we are
slapped in the face by it. Just as we do not pay attention to the
beating of our heart unless it skips a beat or falters, until we are
confronted by flood or drought or until our plans are disrupted by rain,
we ignore the water cycle, preferring to believe that the water we drink
comes out of the faucet, not from deep within the belly of the Earth,
placed there by a process we only dimly comprehend. But, water is as
essential to us and to the Earth as blood is in our bodies, and the
constant cycle that water travels through makes our lives possible.
Earth's blood, water, is pumped, not by a heart, but by the hydrologic
cycle---the water cycle. A titanic force of nature, the water cycle is
beyond our control---a fact that we ignore until weather patterns shift
and inundated rivers suddenly flow where they will and not within
human-engineered banks, floodwalls, dikes, or levees. In the water
cycle, water evaporates from the oceans; falls as rain, hail, sleet, or
snow; and strikes the Earth again \ldots{} thus, the cycle continues. In
cities, in summer, rain strikes hot cement and asphalt and evaporates or
runs into storm drains, swiftly rejoining the cycle. In fields, rain
brings essential moisture to crops and, sinking deeper into the Earth,
ends up as groundwater. If water strikes a forested area, the forest
canopy breaks the force of the falling drops. The forest floor, carpeted
in twigs, leaves, moss, and dead and decaying vegetation, keeps the soil
from splashing away as the water returns to the depths of the Earth or
runs over the land to join a stream. Whenever water strikes the Earth,
it flows along four pathways that carry water through the cycle just as
our veins, arteries, and capillaries carry our blood to our cells. Water
may evaporate directly back into the air. It may flow overland into a
stream as runoff. It may soak into the ground and be taken up by plants
for evapotranspiration. Or, water may seep down to become groundwater.
Whatever pathway it takes, one fact is certain: Water is dynamic, vital,
constantly on the move. And, like human blood, which sustains our lives,
Earth's blood, to sustain us as well, must continue to flow (Spellman,
2007).

INTRODUCTION Take a moment to perform an action most people never think
about doing. Hold a glass of water (like the one shown in Figure 2.1)
and think about the substance within the glass---about the substance you
are getting ready to drink. Also, think about the ice cubes floating in
the glass, unlike those shown in Figure 1.1. The drinking glass full of
water and ice cubes---the water we are about to drink, the water we do
drink---is not one of those items people usually spend much thought on,
unless they are tasked with providing that drinking water or are dying
of thirst.\\
Let's think about that water now, though. Water is special, strange, and
different. Water can be a fascinating subject worthy of considerable
interest because of its unique behavior, endless utility, and ultimate
and intimate connection with our existence. The author agrees with
Robbins (1976), whose description of water follows:

Stylishly composed in any situation---solid, gas, or liquid---speaking
in penetrating dialects understood by all things---animal, vegetable or
mineral---water travels intrepidly through four dimensions, sustaining
(Kick a lettuce in the field and it will yell ``Water!'') destroying
(The Dutch

boy's finger remembered the view from Ararat) and creating (It has even
been said that human beings were invented by water as a device for
transporting itself from one place to another, but that's another
story). Always in motion, ever-flowing (whether at stream rate or
glacier speed), rhythmic, dynamic, ubiquitous, changing and working its
changes, a mathematics turned wrong side out, a philosophy in reverse,
the ongoing odyssey of water is irresistible.

Let's review a few basic facts about that glass of water: Water is
liquid between 0°C and 100°C (32°F and 212°F), solid at or below 0°C
(32°F), and gaseous at or above 100°C (212°F). One gallon of water
weighs 8.33 pounds (3.778 kilograms), and one gallon of water equals
3.785 liters. One cubic foot of water equals 7.50 gallons (28.35
liters). One ton of water equals 240 gallons. One acre-foot of water
equals 43,560 cubic feet (325,900 gallons). Earth's rate of rainfall
equals 340 cubic miles per day (16 million tons per second). As Robbins
observed, water is always in motion. The one most essential
characteristic of water is that it is dynamic: Water constantly
evaporates from seas, lakes, and soil and transpires from foliage. It is
transported through the atmosphere and falls to Earth, where it runs
across the land and filters down to flow along rock strata into
aquifers. Eventually, water finds its way to the sea again---indeed,
water never stops moving. A thought that might not have occurred to most
people looking at a glass of water is, ``Has someone tasted this same
water before us?'' Absolutely. Remember, water is essentially a finite
entity. What we have now is what we have had in the past. We

are drinking the same water consumed by Cleopatra, Aristotle, Leonardo
da Vinci, Napoleon, Joan of Arc (and several billion other folks who
preceded us)---because water is dynamic (never at rest) and because
water constantly cycles and recycles, as discussed in the next section.

THE WATER CYCLE The natural water cycle or hydrologic cycle (the journey
water takes during its constant, inevitable motion) is the means by
which water in all three forms---solid, liquid, and vapor---circulates
through the biosphere. Water lost from the Earth's surface to the
atmosphere, either by evaporation from the surface of lakes, rivers, and
oceans or through the transpiration of plants, forms clouds that
condense to deposit moisture on the land and sea. A drop of water may
travel thousands of miles between the time it evaporates and the time it
falls to Earth again as rain, sleet, or snow. The water that collects on
land flows to the ocean in streams and rivers or seeps into the Earth,
joining groundwater. Even groundwater eventually flows toward the ocean
for recycling (see Figure 2.2). Note: Only about 2\% of the water
absorbed into plant roots is used in photosynthesis. Nearly all of it
travels through the plant to the leaves, where transpiration to the
atmosphere begins the cycle again. Note: The hydrologic cycle describes
water's circulation through the environment. Evaporation, transpiration,
runoff, and precipitation describe specific water movements. When humans
intervene in the natural water cycle, they generate artificial water
cycles or urban water cycles (local subsystems of the water cycle---an
integrated water cycle) (see Figure 2.3). Although many communities
withdraw groundwater for public supply, the majority rely on surface
sources. After treatment, water is distributed to households and
industries. Water that is wasted (wastewater) is collected in a sewer
system and transported to a treatment plant for processing prior to
disposal. Current processing technologies provide only partial recovery
of the original water quality. The upstream community (the first water
user, shown in Figure 2.3) is able to achieve additional quality
improvement by dilution into a surface water body and natural
purification; however, as shown in the figure, the next community
downstream is likely to withdraw the water for a drinking water supply
before complete restoration. This practice is intensified and further
complicated as existing communities continue to grow and new communities
spring up along the same watercourse. Obviously, increases in the number
of users bring an additional need for increased quantities of water.
This withdrawal and return process by successive communities in a river
basin results in indirect water reuse (use of used toilet water via
water recycling).

The indirect water reuse process (demonstrated in Figure 2.3) is a clear
example of combining an artificial water cycle with the natural
hydrologic scheme and involves (1) surface water withdrawal, processing,
and distribution; (2) wastewater collection, treatment, and disposal
back to surface water by dilution; (3) natural purification in a river;
and (4) repetition of this scheme by communities downstream (Hammer and
Hammer, 1996). WATER SUPPLY: THE Q AND Q FACTORS Whereas drinking water
practitioners must have a clear and complete understanding of the
natural and manmade water cycles, they must also factor in the two major
considerations of quantity and quality, the Q and Q factors. They are
responsible for (1) providing a quality potable water supply---one that
is clean, wholesome, and safe to drink; and (2) finding a water supply
in adequate quantities to meet the anticipated demand. Note: Two central
facts important to our discussion of freshwater supplies are that (1)
water is very much a local or regional resource, and (2) problems of its
shortage or pollution are equally local problems. Human activities
affect the quantity of water available at a locale at any time by
changing either the total volume that exists there or aspects of quality
that restrict or devalue it for a particular use.

Thus, the total human impact on water supplies is the sum of the
separate human impacts on the various drainage basins and groundwater
aquifers. In the global system, the central, critical fact about water
is the natural variation in its availability (Meyer, 1996). Simply put,
not all lands are watered equally. To meet the Q and Q requirements of a
potential water supply, the drinking water practitioner (whether it be
the design engineer, community planner, plant manager, plant
administrator, plant engineer, or other responsible person in charge)
must determine the answers to a number of questions, such as

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Does a potable water supply exist nearby that has the capacity for
  water to be distributed in sufficient quantity and pressure at all
  times?
\item
  Will constructing a centralized treatment and distribution system for
  the entire community be best, or would using individual well supplies
  be better?
\item
  If a centralized water treatment facility is required, will the
  storage capacity at the source as well as at intermediate points of
  the distribution system maintain the water pressure and flow
  (quantity) within the conventional limits, particularly during
  loss-of-pressure events, such as major water main breaks,
  rehabilitation of the existing system, or major fires, for example?
\item
  Is a planned or preventive maintenance program in place (or
  anticipated) for the distribution system that can be properly planned,
  implemented, and controlled at the optimum level possible?
\item
  Is the type of water treatment process selected in compliance with
  federal and state drinking water standards? Note: Water from a river
  or a lake usually requires more extensive treatment than groundwater
  does to remove bacteria and suspended particles. Note: The primary
  concern for drinking water practitioners involved with securing an
  appropriate water supply, treatment process, and distribution system
  must be the protection of public health. Contaminants must be
  eliminated or reduced to a safe level to minimize menacing waterborne
  diseases (to prevent another Milwaukee Cryptosporidium event) and to
  avoid long-term or chronic injurious health effects.
\item
  When the source and treatment processes are selected, has the optimum
  hydraulic design of the storage, pumping, and distribution network
  been determined to ensure that sufficient quantities of water can be
  delivered to consumers at adequate pressures?
\item
  Have community leaders and the consumer (the general public) received
  continuing and realistic information about the functioning of the
  proposed drinking water service? Note: Drinking water practitioners
  are wise to direct their attention toward considering point 7, simply
  because public buy-in for any proposed drinking water project that
  involves new construction or retrofitting, expansion, or upgrade of an
  existing facility is essential to ensure that necessary financing is
  forthcoming. In addition to the finances required for any type of
  waterworks construction project, public and financial support is also
  required to ensure the safe operation, maintenance, and control of the
  entire water supply system. The acronym POTW stands for ``publicly
  owned treatment works,'' and the public foots the bills.
\item
  Does planning include steps to ensure elimination of waste, leakages,
  and unauthorized consumption? Note: Industrywide operational
  experience has shown that the cost per cubic foot, cubic meter, liter,
  or gallon of water delivered to the customer has steadily increased
  because of manpower, automation, laboratory, and treatment costs. To
  counter these increasing costs, treatment works must meter consumers,
  measure the water supply flow, and evaluate the entire system
  annually.
\item
  Does the water works or proposed water works physical plant include
  adequate laboratory facilities to ensure proper monitoring of water
  quality? Note: Some water works facilities routinely perform
  laboratory work; however, water pollution control technologists must
  ensure that the water works laboratory or other laboratory used is
  approved by the appropriate health authority. Keep in mind that the
  laboratory selected to test and analyze the waterworks samples must be
  able to analyze chemical, microbiologic, and radionuclide parameters.
\item
  Are procedures in place to evaluate specific problems such as the lead
  content in the distribution systems and at the consumer's faucet or
  suspected contamination due to cross-connection potentials?
\item
  Is a cross-connection control program in place to make sure that the
  distribution system (in particular) is protected from plumbing errors
  and illegal connections that may lead to injection of nonpotable water
  into public or private supplies of drinking water?
\item
  Are waterworks operators and laboratory personnel properly trained and
  licensed?
\item
  Are waterworks managers properly trained and licensed?
\item
  Are proper operating records and budgetary records kept?
\end{enumerate}

DRINKING WATER Q AND Q: KEY DEFINITIONS As with any other technical
presentation, understanding the information presented is difficult
unless a common vocabulary is established. Voltaire said it best: ``If
you wish to converse with me, please define your terms.'' Many key terms
used in the text are defined in this section, but others are defined
where they appear in the text. Before defining these key terms, however,
it is necessary to first define the key term that this text is all
about: drinking water. Drinking, or potable, water can be defined as the
water delivered to the consumer that can be safely used for drinking,
cooking, washing, and other household applications. In the past,
drinking water suitable for use (i.e., safe) was simply certified as
safe by a professional engineer specialized in the field, but times have
changed. This practice is no longer accepted. Why? Because public health
aspects have reached such a high level of importance and complexity that
local licensed health officials usually must be designated as those with
the authority and jurisdiction in the community to review, inspect,
sample, monitor, and evaluate the water supplied to a community on a
continuing basis. This professional scrutiny is driven by updated
drinking water standards, of course. When you factor in the importance
of providing a safe, palatable product to the public, the fact that
public health control is required to help ensure a continuous supply of
safe drinking water makes real sense. On many occasions, drinking water
practitioners have attempted to explain to interested parties the
complexities of providing safe drinking water to their household taps.
Of course, such practitioners know that the process is complex but still
continue to be surprised when listeners express astonishment at the
complex procedures and processes involved. It becomes readily apparent
that a commonly held view is that the provision of drinking water to the
consumer tap involves nothing more than going down to the local river or
stream, installing a suction pipe into the watercourse, and pumping the
water out and into a distribution network that somehow delivers clean,
safe drinking water to the household tap. Yes, water often is taken from
a local river or stream, and that water does eventually find its way to
the household tap. However (isn't there always a ``however'' in any
explanation?), the water drawn from any local surface water supply must
pass through certain processes to ensure its wholesomeness and safety.
Aside from the physical treatment processes in place to screen, filter,
and disinfect the water, additoinal processes include continually
inspecting, sampling, monitoring, and evaluating the water supply.
Drinking water practitioners learn the ins and outs of drinking water
primarily from experience. The main lesson learned is that supplying
drinking water to the household tap is a complex and demanding process;
for example, it is obvious that water analysis is required but it alone
is not sufficient to maintain quality. It must be combined with the
periodic review and acceptance of the facilities involved. What does
this mean? Simply stated, acceptance or approval requires evaluation,
maintenance, and proper protection of the water source; qualified
waterworks personnel; adequate monitoring procedures by water suppliers;
and evaluation of the quality and performance of laboratory work
(DeZuane, 1997). Thus, when we attempt to define drinking water, we must
define it in all-encompassing terms. Drinking or potable water is a
product from an approved source that falls within certain physical,
chemical, bacteriological, and radionuclide parameters and is delivered
to a treatment works for processing and disinfecting. Such a treatment
works must be properly designed, constructed, and operated. Drinking
water must be delivered to the consumer in sufficient quantity and
pressure and must meet stringent quality standards. It must be
palatable, be within reasonable temperature limits, and have the
complete confidence of the consumer. The bottom line is that drinking
water is a substance available to the consumer at the household tap that
can perform one essential function: It can satisfy thirst without
threatening life and health.

Definitions Absorption---A process where one substance penetrates the
interior of another substance. Acid rain---Precipitation with higher
than normal acidity, caused primarily by sulfur and nitrogen dioxide air
pollution. Activated carbon---A very porous material that can be used to
adsorb pollutants from water after the material has been subjected to
intense heat to drive off impurities. Adsorption---The process by which
one substance is attracted to and adheres to the surface of another
substance, without actually penetrating its internal structure.
Aeration---A physical treatment method that promotes biological
degradation of organic matter. The process may be passive (when waste is
exposed to air) or active (when a mixing or bubbling device introduces
air). Aerobic bacteria---A type of bacteria that requires free oxygen to
carry out metabolic function. Biochemical oxygen demand (BOD)---The
amount of oxygen required by bacteria to stabilize decomposable organic
matter under aerobic conditions. Biological treatment---A process that
uses living organisms to bring about chemical changes. Breakpoint
chlorination---The addition of chlorine to water until the chlorine
demand has been satisfied and free chlorine residual is available for
disinfection. Chemical treatment---A process that results in the
formation of a new substance or substances. The most common chemical
water treatment processes include coagulation, disinfection, water
softening, and filtration. Chlorination---The process of adding chlorine
to water to kill disease-causing organisms or to act as an oxidizing
agent. Chlorine demand---A measure of the amount of chlorine that will
combine with impurities and is therefore unavailable to act as a
disinfectant. Clean Water Act (CWA)---Federal law dating to 1972 (with
several amendments) with the objective to restore and maintain the
chemical, physical, and biological integrity of the nation's waters. Its
long-range goal is to eliminate the discharge of pollutants into
navigable waters and to make national waters fishable and swimmable.
Coagulants---Chemicals that cause small particles to stick together to
form larger particles. Coagulation---A chemical water treatment method
that causes very small suspended particles to attract one another and
form larger particles. This is accomplished by the addition of a
coagulant that neutralizes the electrostatic charges that cause
particles to repel each other. Coliform bacteria---A group of bacteria
predominantly inhabiting the intestines of humans or animals, but also
occasionally found elsewhere. The presence of these bacteria in water is
used as an indication of fecal contamination (contamination by animal or
human wastes). Color---A physical characteristic of water. Color is most
commonly tan or brown from oxidized iron, but contaminants may cause
other colors, such as green or blue. Color differs from turbidity, which
is a measure of the cloudiness of the water. Communicable
diseases---Usually caused by microbes, which are microscopic organisms
including bacteria, protozoa, and viruses. Most microbes are essential
components of our environment and do not cause disease. Those that do
are called pathogenic organisms, or simply pathogens.

Community water system---A public water system that serves at least 15
service connections used by year-round residents or regularly serves at
least 25 year-round residents. Composite sample---A series of individual
or grab samples taken at different times from the same sampling point
and mixed together. Contaminant---A toxic material found as an unwanted
residue in or on a substance. Cross-connection---Any connection between
safe drinking water and a nonpotable water or fluid. C×T value---The
product of the residual disinfectant concentration (C), in milligrams
per liter, and the corresponding disinfectant contact time (T), in
minutes. Minimum C×T values are specified by the Surface Water Treatment
Rule as a means of ensuring adequate killing or inactivation of
pathogenic microorganisms in water. Disinfectants and disinfection
byproducts (DBPs)---A term used in connection with state and federal
regulations designed to protect public health by limiting the
concentration of either disinfectants or the byproducts formed by the
reaction of disinfectants with other substances in the water (such as
trihalomethanes, THMs). Disinfection---A chemical treatment method; the
addition of a substance (e.g., chlorine, ozone, hydrogen peroxide) that
destroys or inactivates harmful microorganisms or inhibits their
activity. Dissociation---The process of ion separation that occurs when
an ionic solid is dissolved in water. Dissolved oxygen (DO)---The oxygen
dissolved in water, usually expressed in milligrams per liter, parts per
million, or percent of saturation. Dissolved solids---Any material that
can dissolve in water and be recovered by evaporating the water after
filtering the suspended material. Drinking water standards---Water
quality standards that must be met in terms of suspended solids,
unpleasant taste, and microbes harmful to human health. Drinking water
standards are included in state water quality rules. Drinking water
supply---Any raw or finished water source that is or may be used as a
public water system or as drinking water by one or more individuals.
Effluent limitations---Standards developed by the U.S. Environmental
Protection Agency (USEPA) to define the levels of pollutants that could
be discharged into surface waters. Electrodialysis---The process of
separating substances in a solution by dialysis, using an electric field
as the driving force. Electronegativity---The tendency for atoms that do
not have a complete octet of electrons in their outer shell to become
negatively charged. Enhanced Surface Water Treatment Rule (ESWTR)---A
revision of the original Surface Water Treatment Rule that includes new
technology and requirements to deal with newly identified problems.
Facultative bacteria---A type of anaerobic bacteria that can metabolize
its food either aerobically or anaerobically.

Federal Water Pollution Control Act (1972)---The objective of the Act
was ``to restore and maintain the chemical, physical, and biological
integrity of the nation's waters.'' This Act and subsequent Clean Water
Act amendments are the most far-reaching water pollution control
legislation ever enacted. They provide for comprehensive programs for
water pollution control, uniform laws, and interstate cooperation, as
well as grants for research, investigation, training, surveillance, and
gathering information on the effects of pollutants, pollution control,
and the identification and measurement of pollutants. Additionally, they
allot grants and loans for the construction of treatment works. The Act
established national discharge standards with enforcement provisions and
established several milestone achievement dates. It required secondary
treatment of domestic waste by publicly owned treatment works (POTW) and
the application of ``best practicable'' water pollution control
technology by 1977. Virtually all industrial sources achieved
compliance. (Because of economic difficulties and cumbersome federal
requirements, certain POTWs obtained an extension to July 1, 1988, to
achieve compliance.) The Act also called for new levels of technology to
be imposed during the 1980s and 1990s, particularly for controlling
toxic pollutants. The Act mandates a strong pretreatment program to
control toxic pollutants discharged by industry into POTWs. The 1987
amendments required that stormwater from industrial activity must be
regulated. Filtration---A physical treatment method for removing solid
(particulate) matter from water by passing the water through porous
media such as sand or a manmade filter. Flocculation---The water
treatment process following coagulation; it uses gentle stirring to
bring suspended particles together so they will form larger, more
settleable clumps called floc. Grab sample---A single water sample
collected at one time from a single point. Groundwater---The freshwater
found under the Earth's surface, usually in aquifers. Groundwater is a
major source of drinking water and a source of growing concern in areas
where leaching agricultural or industrial pollutants or substances from
leaking underground storage tanks are contaminating groundwater.
Hardness---A characteristic of water caused primarily by the salts of
calcium and magnesium. It leads to the deposition of scale in boilers,
damages equipment in industrial processes, and sometimes causes an
objectionable taste. It may also decrease the effectiveness of soap.
Hydrogen bonding---The term used to describe the weak but effective
attraction that occurs between polar covalent molecules. Hydrologic
cycle---Literally the water--Earth cycle; the movement of water in all
three physical forms through the various environmental mediums (air,
water, biota, and soil). Hygroscopic---Refers to a substance that
readily absorbs moisture. Influent---Water flowing into a reservoir,
basin, or treatment plant. Inorganic chemical---A chemical substance of
mineral origin not having carbon in its molecular structure.

Ionic bond---The attractive forces between oppositely charged ions---for
example, the forces between sodium and chloride ions in a sodium
chloride crystal. Maximum contaminant level (MCL)---The maximum
allowable concentration of a contaminant in drinking water, as
established by state and federal regulations. Primary MCLs are health
related and mandatory. Secondary MCLs are related to the aesthetics of
the water and are highly recommended, but not required. Membrane filter
method---A laboratory method used for coliform testing. The procedure
uses an ultrathin filter with a uniform pore size smaller than bacteria
(less than 1 micron). After water is forced through the filter, the
filter is incubated in a special medium that promotes the growth of
coliform bacteria. Bacterial colonies with a green--gold sheen indicate
the presence of coliform bacteria. Modes of transmission of
disease---The ways in which diseases spread from one person to another.
Multiple-tube fermentation method---A laboratory method used for
coliform testing that uses a nutrient broth placed in a culture tube.
Gas production indicates the presence of coliform bacteria. National
Pollutant Discharge Elimination System (NPDES)---A requirement of the
CWA that discharges meet certain requirements prior to discharging waste
to any water body. It sets the highest permissible effluent limits, by
permit, prior to making any discharge. National Primary Drinking Water
Regulations (NPDWRs)---Regulations developed under the Safe Drinking
Water Act that establish maximum contaminant levels, monitoring
requirements, and reporting procedures for contaminants in drinking
water that endanger human health. Near Coastal Water Initiative---An
initiative that was developed in 1985 to provide for the management of
specific problems in waters near coastlines that are not dealt with in
other programs. Nonbiodegradable---Refers to substances that do not
break down easily in the environment. Nonpolar covalently
bonded---Refers to molecules composed of atoms that share their
electrons equally, resulting in molecules that do not have polarity.
Organic chemical---A chemical substance of animal or vegetable origin
hav- ing carbon in its molecular structure. Oxidation---Occurs when a
substance either gains oxygen or loses hydrogen or electrons in a
chemical reaction; one of the chemical treatment methods. Oxidizer---A
substance that oxidizes another substance. Parts per million (PPM)---The
number of weight or volume units of a constituent present within each 1
million units of the solution or mixture. PPM was formerly used to
express the results of most water and wastewater analyses, but it is
being replaced by milligrams per liter (mg/L). For drinking water
analyses, concentration in parts per million and milligrams per liter
can be considered to be equivalent. A single PPM can be compared to a
shot glass full of water inside a swimming pool. Pathogens---Types of
microorganisms that can cause disease.

Physical treatment---Any process that does not produce a new substance
(e.g., screening, adsorption, aeration, sedimentation, filtration).
Polar covalent bond---Occurs when the shared pair of electrons between
two atoms are not shared equally; thus, one of the atoms becomes
slightly positively charged and the other atom becomes slightly
negatively charged. Polar covalent molecule---One or more polar covalent
bonds result in a molecule that is polar covalent. Polar covalent
molecules exhibit partial positive and negative poles, causing them to
behave like tiny magnets; water is the most common polar covalent
substance. Pollutant---Any substance introduced into the environment
that adversely affects the usefulness of the resource. Pollution---The
presence of matter or energy whose nature, location, or quantity
produces undesired environmental effects. Under the Clean Water Act, for
example, the term is defined as a manmade or human-induced alteration of
the physical, biological, and radiological integrity of water.
Pretreatment---Any physical, chemical, or mechanical process used before
the main water treatment processes. It can include screening,
pre-sedimentation, and chemical addition. Primary drinking water
standards---Regulations on drinking water quality defined under the Safe
Drinking Water Act that are considered essential for the preservation of
public health. Primary treatment---The first step of treatment at a
municipal wastewater treatment plant. It typically involves screening
and sedimentation to remove materials that float or settle. Public water
system (PWS)---As defined by the Safe Drinking Water Act, any publicly
or privately owned system serving at least 15 service connections 60
days out of the year or an average of 25 people at least 60 days out of
the year. Publicly owned treatment works (POTW)---A waste treatment
works owned by a state or local government unit or Indian tribe, usually
designed to treat domestic wastewaters. Receiving water---A river, lake,
ocean, stream, or other water source into which wastewater or treated
effluent is discharged. Recharge---The process by which water is added
to a zone of saturation, usually by percolation from the soil surface.
Reference dose (RfD)---An estimate of the amount of a chemical that a
person can be exposed to on a daily basis that is not anticipated to
cause adverse systemic health effects over the person's lifetime.
Representative sample---A sample containing all the constituents present
in the water from which it was taken. Reverse osmosis (RO)---Solutions
of differing ion concentration are separated by a semipermeable
membrane. Typically, water flows from the chamber with lesser ion
concentration into the chamber with the greater ion concentration,
resulting in hydrostatic or osmotic pressure. In RO, enough external
pressure is applied to overcome this hydrostatic pressure, thus
reversing the flow of water. This results in the water on the other side
of the membrane becoming depleted in ions and demineralized.

Safe Drinking Water Act (SDWA)---A federal law passed in 1974 with the
goal of establishing federal standards for drinking water quality,
protecting underground sources of water, and setting up a system of
state and federal cooperation to ensure compliance with the law.
Screening---A pretreatment method that uses coarse screens to remove
large debris from the water to prevent clogging of pipes or channels to
the treatment plant. Secondary drinking water standards---Regulations
developed under the Safe Drinking Water Act that established maximum
levels of substances affecting the aesthetic characteristics (taste,
color, or odor) of drinking water. Secondary treatment---The second step
of treatment at a municipal wastewater treatment plant. This step uses
growing numbers of microorganisms to digest organic matter and reduce
the amount of organic waste. Water leaving this process is chlorinated
to destroy any disease-causing microorganisms before its release.
Sedimentation---A physical treatment method that involves reducing the
velocity of water in basins so the suspended material can settle out by
gravity. Solvated---Refers to when either a positive or a negative ion
becomes completely surrounded by polar solvent molecules. Surface
tension---The attractive forces exerted by the molecules below the
surface upon those at the surface, resulting in them crowding together
and forming a higher density. Surface water---All water naturally open
to the atmosphere; all springs, wells, or other collectors that are
directly influenced by surface water. Surface Water Treatment Rule
(SWTR)---A federal regulation established by the USEPA under the Safe
Drinking Water Act that imposes specific monitoring and treatment
requirements on all public drinking water systems that draw water from a
surface water source. Synthetic organic chemicals (SOCs)---Generally
applied to manufactured chemicals that are not as volatile as volatile
organic chemicals. Included are herbicides, pesticides, and chemicals
widely used in industries. Total suspended solids (TSS)---Solids present
in wastewater. Trihalomethanes (THMs)---A group of compounds formed when
natural organic compounds from decaying vegetation and soil (such as
humic and fulvic acids) react with chlorine. Turbidity---A measure of
the cloudiness of water caused by the presence of suspended matter,
which shelters harmful microorganisms and reduces the effectiveness of
disinfecting compounds. Vehicle of disease transmission---Any nonliving
object or substance contaminated with pathogens. Wastewater---The spent
or used water from individual homes, a community, a farm, or an industry
that contains dissolved or suspended matter. Waterborne disease---Water
is a potential vehicle of disease transmission, and waterborne disease
is possibly one of the most preventable types of communicable illness.
The application of basic sanitary principles and technology has
virtually eliminated serious outbreaks of waterborne diseases in
developed countries. The most prevalent waterborne diseases include
typhoid fever, dysentery, cholera, infectious hepatitis, and
gastroenteritis. Note: Waterborne diseases are also called intestinal
diseases, because they affect the human intestinal tract. If pathogens
excreted in the feces of infected people are inadvertently ingested by
others (in contaminated water, for example), the cycle of disease can
continue, possibly in epidemic proportions. Symptoms of intestinal
disease include diarrhea, vomiting, nausea, and fever. Intestinal
diseases can incapacitate large numbers of people in an epidemic and
sometimes result in the deaths of many infected individuals. Water
contaminated with untreated sewage is generally the most common cause of
this type of disease (Nathanson, 1997). In practice, ``hazard
identification in the case of pathogens is complicated because several
outcomes---from asymptomatic infections to death---are possible, and
their outcomes depend upon the complex integration between the agent and
the host. This interaction, in turn, depends on the characteristics of
the host as well as the nature of the pathogen. Host factors, for
example, include preexisting immunity, age, nutrition, ability to mount
an immune response, and other nonspecific host factors. Agent factors
include types and strains of the organism as well as its capacity to
elicit an immune response'' (Gerba, 1996). Water softening---A chemical
treatment method that uses either chemicals to precipitate or a zeolite
to remove the metal ions (typically Ca2+, Mg2+, and Fe3+) responsible
for hard water. Watershed---The land area that drains into a river,
river system, or other body of water. Wellhead protection---The
protection of the surface and subsurface areas surrounding a water well
or well field supplying a public water system from contamination by
human activity.

CLEAN, FRESH, AND PALATABLE: A HISTORICAL PERSPECTIVE An early human,
wandering alone from place to place, hunting and gathering to subsist,
probably would have had little difficulty in obtaining drinking water,
because such a person would---and could---only survive in an area where
drinking water was available with little travail. The search for clean,
fresh, and palatable water has been a human priority from the very
beginning. The author is taking no risk in stating that when humans
first walked the Earth many of the steps they took were in the direction
of a water supply. When early humans were alone or gathered in small
numbers, finding drinking water was a constant priority, to be sure, but
it is difficult for us to imagine today just how big a priority finding
drinking water became as the number of humans proliferated. Eventually
communities formed, and with their formation came an increasing need to
find clean, fresh, and palatable drinking water, as well as a means of
delivering it from the source to the point of use. Archeological digs
are replete with the remains of ancient water systems that reflect
humans' early attempts to satisfy their never-ending need for drinking
water. For well over 2000 years, piped water supply systems have been in
existence. Whether the pipes were fashioned from logs or clay or carved
from stone or other

materials is not the point---the point is they were fashioned to serve a
vital purpose, one universal to all human communities: to deliver clean,
fresh, and palatable water to where it was needed. These early systems
were not arcane. Their intended purpose is readily understood today. As
we might expect, they could be rather crude, but they were reasonably
effective, although they lacked in two general areas we take for granted
today. First, of course, they were not pressurized but instead relied on
gravity flow, as the means to pressurize the mains was not known at the
time. Even if such pressurized systems were known, they certainly would
not have been used to pressurize water delivered via hollowed-out logs
and clay pipe. The second general area in which they lacked was
sanitation, which is generally taken for granted by those in the
industrialized world of today. Remember, to recognize that a need for
something exists (in this case, the ability to sanitize or disinfect
water supplies), it is necessary to define the nature of the problem.
Not until the middle of the 1800s (after countless millions of deaths
from waterborne disease over the centuries) did people realize that a
direct connection between contaminated drinking water and disease
existed. At that point, sanitation of water supplies became an issue.
When the relationship between waterborne diseases and the consumption of
drinking water was established, evolving scientific discoveries led the
way toward development of the technology necessary for processing and
disinfection. Drinking water standards were developed by health
authorities, scientists, and sanitary engineers. With the current lofty
state of effective technology that we in the United States and the rest
of the developed world enjoy today, we could sit on our laurels, so to
speak, and assume that because of the discoveries developed over time
(at the cost of countless people who died and are still dying from
waterborne diseases), all is well with us---that problems related to
providing a clean, fresh, palatable drinking water are problems of the
past. Are they really, though? Have we solved all the problems related
to ensuring that our drinking water is clean, fresh, and palatable? Is
the water delivered to our tap as clean, fresh, and palatable as we
think it is \ldots{} as we hope it is? Does anyone really know? What we
do know is that we have made progress. We have come a long way from the
days of gravity-flow water delivered via mains of logs and clay or
stone, and many of us on this planet have come a long way from the days
of cholera epidemics. Still, perhaps we should consider those who have
suffered and survived onslaughts of Cryptosporidium delivered to them
through their tap---in Sydney, Australia, in 1998; in Milwaukee,
Wisconsin, in 1993; in Las Vegas, Nevada, in the early 1990s. How safe
do they think our drinking water supply is? If we could, we would ask
this same question of a little boy named Robbie, who died of acute
lymphatic leukemia, the probable cause of which is far less
understandable to us: toxic industrial chemicals, unknowingly delivered
to him via his local water supply.

Drinking Water Regulations Drinking water regulations have undergone
major and dramatic changes during the past two decades, and trends
indicate that they will continue to become more stringent and
complicated. It is important that all water system operators understand
the basic reasons for having regulations, how they are administered, and
why compliance with them is so essential. AWWA (2005)

Indeed, the 2010 volume of the Federal Register, the ``newspaper'' of
regulatory agencies, stands at an all-time record-high 81,405 pages
composed of final rules, proposed rules, meeting notices and regulatory
studies. ``There is something like 180 million words of binding federal
law and regulation. It would take a lifetime just to read it,'' said
Philip K. Howard, founder of Common Good. Fox News (2011)

Some say we are a Regulation Nation. To a point we fully agree with this
statement \ldots{} and in most cases we feel these narrow-minded rules
and regs are adverse and stymieing economic progress \ldots{} which is
sorely lacking at present. Again, we feel this way to a point. However,
there is no point reached when it comes to maintaining personal safety
and health. Thus, if we have to be a Regulation Nation to ensure the tap
water we drink is clean, safe, and palatable then we say, ``Please,
regulate to the extreme, thank you very much!''

REGULATION NATION As stated above, many consider us to be a Regulation
Nation. A 2011 Fox News story pointed out that, even though President
Obama had acknowledged the need to minimize regulations, the number
appeared to be growing. The Obama administration introduced regulations
at a rate equivalent to 10 per week. Whether the reader believes this is
good or bad practice is not the point. The point is that adapting the
workforce to the challenges of constantly changing regulations and
standards for water treatment is a major concern. Drinking water
standards are regulations that the U.S. Environmental Protection Agency
(USEPA) sets to control the level of contaminants in the nation's
drinking water. These standards are part of the Safe Drinking Water
Act's multiple-barrier approach to drinking water protection. Why do we
need regulations? Most of us, having no taste for anarchy, would have
little trouble answering this question. We regulate ourselves and others
for a variety of reasons, but in our attempts to do so we generally
strive to attain similar results. Most governments, for example,
regulate their population to provide direction, to manage, to monitor,
and to literally govern whatever it is they are attempting to regulate
(including us). We also regulate to confine, to control, to limit, and
to restrict ourselves within certain parameters to maintain the
peace---with the goal of providing equal and positive social conditions
for us all. Regulations are not foreign to us \ldots{} we are literally
driven by them from birth through our final internment---you could say
that we are literally regulated to death. Some regulations are
straightforward. The 70-mph speed limit on some interstates is
simple---the regulation establishes measurable limits. Other regulations
are not so simple, such as regulations designed to ensure the safe and
correct operation of nuclear reactors that are complex and difficult to
meet. Whether straightforward or complex, however, enforcement presents
special problems. As to safe drinking water regulations, we can only
hope that the regulations in place to ensure our safety and health are
more effectively enforced than that 70-mph speed limit. In this chapter,
we discuss U.S. federal regulations designed to protect our health and
well-being: the so-called drinking water regulations. Control of the
quality of our drinking water is accomplished by establishing certain
regulations, which in turn require compliance within an established set
of guidelines or parameters. The guidelines are the regulations
themselves; the parameters are the water quality factors important to
providing drinking water that is safe and palatable.

WHY REGULATE? Consider what might be an absurd question: ``Why do we
need to regulate water quality?'' And another question that is perhaps a
bit more logical: ``Aren't we already regulated enough?'' The first
question requires a compound answer, the explanation of which is
provided in this chapter---in the hope that it will clear the water, so
to speak. The second question? This question must be answered with
another question: ``When it comes to ensuring a safe and palatable
drinking water supply, are we (or can we be) regulated enough?'' This
text concentrates on answering the first question because it goes to the
heart of our discussion---the necessity of providing safe and palatable
drinking water to the user. Again, why do we need to regulate water
quality? Let's start at the beginning. In the beginning (the ancient
beginning), humans really had no reason to give water quality much of a
thought. Normally, nearly any water supply available was only nominally
naturally polluted. Exceptions existed of course; for example, a
prehistoric human flattened out on the ground alongside a watercourse to
drink would not ingest too much of the water (as little of it as
possible, in fact) if it was salty. Our intrepid (but thirsty) ancestor
would probably move on to find another water source, one a bit more
palatable. Determining water's fitness to drink was a matter of sight,
smell, and a quick taste. If the relevant criteria were met, the water
was used. Our early kinfolk were likely to have gulped down water that
looked perfectly clear, smelled all right, and did not taste all that
bad. Later that day, though, the water could have made them sick, very
sick---sickened by waterborne pathogens that were residents of that
perfectly clear, not too bad tasting water ingested a few hours earlier.
Of course, early humans would not have had the foggiest notion what
caused the sickness, but they would have become very sick, indeed. Let's
take a look at more recent times, at another scenario that helps
illustrate the point that we are making here (Spellman, 1996): An
excursion to the local stream can be a relaxing and enjoyable
undertaking. On the other hand, when you arrive at the local stream,
spread your blanket on the stream bank, and then look out upon the
stream's flowing mass only to discover a parade of waste and discarded
rubble bobbing along the stream's course and cluttering the adjacent
shoreline and downstream areas, any feeling of relaxation or enjoyment
is quickly extinguished. Further, the sickening sensation the observer
feels is not lessened but made worse as he gains closer scrutiny of the
putrid flow. He easily recognizes the rainbow-colored shimmer of an oil
slick, interrupted here and there by dead fish and floating refuse, and
the slimy fungal growth that prevails. At the same time, the observer's
sense of smell is alerted to the noxious conditions. Along with the
fouled water and the stench of rot-filled air, the observer notices the
ultimate insult and tragedy: The signs warn: ``DANGER---NO SWIMMING or
FISHING.'' The observer soon realizes that the stream before him is not
a stream at all; it is little more than an unsightly drainage ditch. The
observer has discovered what ecologists have known and warned about for
years. That is, contrary to popular belief, rivers and streams do not
have an infinite capacity for pollution.

This relatively recent scenario makes an important point for us: The
qualities of water that directly affect our senses are the first to
disturb us. This certainly was the case with ancient humans, before the
discovery of what causes disease and waterborne disease in particular.
Even before the mid-1850s, when Dr.~John Snow, in London, made the
connection between water and disease (i.e., the waterborne disease
cholera), rumblings could be heard in that city about the terribly
polluted state of the Thames River. Dr.~Snow's discovery of the
connection between cholera and drinking water obtained from the Broad
Street pump that was ingested by those who became ill or died lit the
fire of reform, and revulsion set in motion steps to clean up the water
supply. Since Snow's discovery, many subsequent actions taken to clean
up a particular water supply resulted from incidents related to public
disgust with the sorry state of the watercourse. For example, in the
1960s, the burgeoning environmental movement found many ready examples
of the deplorable state and vulnerability of America's waters. In
Cleveland, the Cuyahoga River burst into flames, so polluted was it with
chemicals and industrial wastes; historic Boston Harbor was a veritable
cesspool; raw sewage spewed into San Francisco Bay. A 1969 oil spill off
scenic Santa Barbara, California, proved an especially telegenic
disaster due to the resulting oil-soaked seals and pelicans and miles of
hideously fouled beaches. These and other incidents were disturbing to
many Americans and brought calls for immediate reform. Awareness of the
state of our environment was at an all-time high. A grassroots crusade
for environmental action was set into motion by the words of a brilliant
writer, a writer whose penetrating scientific views and poetic prose
captured the imagination of the nation. Rachel Carson became the flag
bearer for our environment. By making the connections between isolated
incidents and the actions of industry, research, and government, she
brought the clear light of day into the dark abyss of environmental
degradation, revealing widespread horrible environmental conditions and
the future they could lead to. The public lost trust in the ability of
government and industry to self-govern with regard to choosing between
money and the benefits of a clean environment for us all. Industry and
government's close connections and financial self-interest were revealed
as poor criteria for determining realistic levels of environmental
protection. With Rachel Carson's Silent Spring came the sobering
awareness that environmental conditions and the prevailing governmental
attitude demanded radical change. Individual incidents disturbed many
Americans to the point that they demanded immediate reforms.

CLEAN WATER REFORM IS BORN To understand the history (and thus the
impetus) behind the reform movement intent on cleaning up our water
supplies, we can trace a chronology of some of the significant events
precipitated by environmental organizations and citizens groups that
have occurred since the mid-1960s:*

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Americans came face-to-face with the grim condition of the nation's
  waterways in 1969, when the industrial-waste-laden Cuyahoga River
  caught on fire. That same year, waste from food-processing plants
  killed almost 30 million fish in Lake Thonotosassa, Florida.
\item
  In 1972, Congress enacted the Clean Water Act (after having overridden
  President Nixon's veto). The passage of the Clean Water Act has been
  called ``literally a life-or-death proposition for the nation.'' The
  Act set the goals of achieving water that is ``fishable and
  swimmable'' by 1983 and zero discharges of pollutants by 1985, in
  addition to prohibiting the discharge of toxic pollutants in toxic
  amounts.
\item
  In 1974, the Safe Drinking Water Act (SDWA) was passed, requiring the
  U.S. Environmental Protection Agency (USEPA) to establish national
  standards for contaminants in drinking water systems, underground
  wells, and sole-source aquifers, in addition to several other
  requirements.
\item
  In 1984, an alliance of the Natural Resources Defense Council, the
  Sierra Club, and others successfully sued Phillips ECG, a New York
  industrial polluter that had dumped waste into the Seneca River.
  According to the Sierra Club's water committee chair, Samuel Sage, the
  case ``tested the muscles of citizens against polluters under the
  Clean Water Act.'' During this same timeframe, a Clean Water Act
  reauthorization bill drew the wrath of environmental groups, who
  dubbed it the ``Dirty Water Act'' after lawmakers added last-minute
  pork and weakened wetland protection and industrial pretreatment
  provisions. Grassroots action led to most of these pork provisions
  being dropped. That same year also saw the highest environmental
  penalty to date---\$70,000, which was imposed on Alcoa Aluminum in
  Messina, New York (for polluting the St.~Lawrence River), as a result
  of a suit filed by the Sierra Club.
\item
  In 1986, Tip O'Neill, Speaker of the House of Representatives, stated
  that he would not let a Clean Water Act reauthorization bill on the
  floor without the blessing of environmental groups. Later, after the
  bill was crafted and passed by Congress, President Reagan vetoed the
  bill. Also in 1986, amendments to the Safe Drinking Water Act directed
  the USEPA to publish a list of drinking water contaminants that
  require legislation.
\item
  In 1987, the Clean Water Act was reintroduced. It became law after
  Congress overrode President Reagan's veto. A new provision established
  the National Estuary Program.
\item
  From 1995 to 1996, the House passed H.R. 961 (again dubbed the ``Dirty
  Water Act''), which in some cases eliminated standards for water
  quality, wetlands protection, sewage treatment, and agricultural and
  urban runoff. The Sierra Club collected over 1 million signatures in
  support of the Environmental Bill of Rights and released ``Danger on
  Tap,'' a report that revealed polluter contributions to friends in
  Congress who wanted to gut the Clean Water Act. Due in part to these
  efforts, the bill was stopped in the Senate.
\item
  In 1997, the USEPA reported that more than a third of the country's
  rivers and half of its lakes were still unfit for swimming or fishing.
  The Sierra Club successfully sued the USEPA to enforce Clean Water Act
  regulations in Georgia. The state was required to identify polluted
  waters and establish their pollution-load capacity. Similar suits were
  filed in other states; for example, in Virginia, Smithfield Foods was
  assessed a penalty of more than \$12 million---the highest ever---for
  violating the Clean Water Act by discharging phosphorus and other hog
  waste products into a tributary of the Chesapeake Bay.
\end{enumerate}

This chronology of events presents only a handful of the significant
actions taken by Congress (with the helpful prodding and guidance of the
Sierra Club and the National Resources Defense Council, as well as
others) in enacting legislation and regulations to protect our nation's
waters. No law has been more important to furthering this effort than
the Clean Water Act, which we discuss in the following section.

CLEAN WATER ACT* Concern with the disease-causing pathogens residing in
many of our natural waterways was not what grabbed Joe and Nancy
Citizen's attention with regard to the condition and health of the
country's waterways. Instead, it was the aesthetic qualities of
watercourses. Americans in general have a strong emotional response to
the beauty of nature, and they acted to prevent pollution and
degradation of our nation's waterways simply because many of us expect
rivers, waterfalls, and mountain lakes to be natural and naturally
beautiful---in the state they were intended to be, pure and clean.

Much of this emotional attachment to the environment can be traced back
to the sentimentality characteristic of the popular literature and art
of American writers and painters in the early 19th century. From
Longfellow's Song of Hiawatha to Twain's Huckleberry Finn to the
landscapes of Winslow Homer and the vistas of the Hudson River School
painters, American culture abounds with expressions of this singularly
strong attachment. As the saying goes: ``Once attached, detachment is
never easy.'' Federal water pollution legislation dates back to the turn
of the century, to the Rivers and Harbors Act of 1899, although the
Clean Water Act stems from the Federal Water Pollution Control Act,
which was originally enacted in 1948 to protect surface waters such as
lakes, rivers, and coastal areas. That act was significantly expanded
and strengthened in 1972 in response to growing public concern over
serious and widespread water pollution problems. The 1972 legislation
provided the foundation for our dramatic progress in reducing water
pollution over the past several decades. Amendments to the 1972 Clean
Water Act were made in 1977, 1981, and 1987. The Clean Water Act focuses
on improving water quality by maintaining and restoring the physical,
chemical, and biological integrity of the nation's waters. It provides a
comprehensive framework of standards, technical tools, and financial
assistance to address the many stressors that can cause pollution and
adversely affect water quality, including municipal and industrial
wastewater discharges, polluted runoff from urban and rural areas, and
habitat destruction. The Clean Water Act requires national performance
standards for major industries (such as iron and steel manufacturing and
petroleum refining) that provide a minimum level of pollution control
based on the best technologies available. These national standards
result in the removal of over a billion pounds of toxic pollution from
our waters every year. The Clean Water Act also establishes a framework
whereby states and Indian tribes survey their waters, determine an
appropriate use (such as recreation or water supply), then set specific
water quality criteria for various pollutants to protect those uses.
These criteria, together with the national industry standards, are the
basis for permits that limit the amount of pollution that can be
discharged to a water body. Under the National Pollutant Discharge
Elimination System (NPDES), sewage treatment plants and industries that
discharge wastewater are required to obtain permits and to meet the
specified limits in those permits. Note: The Clean Water Act requires
the USEPA to set effluent limitations. All dischargers of wastewaters to
surface waters are required to obtain NPDES permits, which require
regular monitoring and reporting. The Clean Water Act also provides
federal funding to help states and communities meet their clean water
infrastructure needs. Since 1972, federal funding has provided billions
of dollars in loans and grants, primarily for building or upgrading
wastewater treatment plants. In 2015, for example, \$314 million in
loans and grants became available for 141 projects to build and improve
water and wastewater infrastructure in rural communities (USDA, 2015).
Funding is also provided to address another major water quality
problem---polluted runoff from urban and rural areas. Protecting
valuable aquatic habitat---wetlands, for example---is another important
component of this law. American waterways have suffered loss and
degradation of biological habitat, a widespread cause of the decline in
the health of aquatic resources. When Europeans colonized this
continent, North America held approximately 221 million acres of
wetlands. Today, most of those wetlands are gone. At least 22 states
have lost 50\% or more of their original acreage of wetlands, and 10
states have lost about 70\% of their wetlands. The Clean Water Act
sections dealing with wetlands have become extremely controversial.
Wetlands are among our nation's most fragile ecosystems and play a
valuable role in maintaining regional ecology and preventing flooding,
while serving as home to numerous species of insects, birds, and
animals; however, wetlands also represent significant potential monetary
value in the eye of private landowners and developers. Herein lies the
major problem. Many property owners feel they are being unfairly
penalized by a Draconian regulation that restricts their right to
develop their own property. Alternative methods that do not involve
destroying the wetlands are available. These methods include wetlands
mitigation and mitigation banking. Since 1972, when the Clean Water Act
was passed, permits from the U.S. Army Corps of Engineers have been
required to work in wetland areas. To obtain these permits, builders
must agree to restore, enhance, or create an equal number of wetland
acres (generally in the same watershed) as those that would be damaged
or destroyed in the construction project. Landowners are given the
opportunity to balance the adverse effects by replacing environmental
values that are lost. This concept is known as wetlands mitigation.
Mitigation banking allows developers or public bodies that seek to build
on wetlands to make payments to a ``bank'' for use in the enhancement of
other wetlands at a designated location. The development entity
purchases ``credit'' in the bank and transfers full mitigation
responsibility to an agency or environmental organization that runs the
bank. Environmental professionals design, construct, and maintain a
specific natural area using these funds.

The history of the Clean Water Act is much like that of the
environmental movement itself. Once widely supported and buoyed by its
initial success, the Clean Water Act has encountered increasingly
difficult problems, such as polluted stormwater runoff and
non-point-source pollution, as well as unforeseen legalistic challenges,
such as debate regarding wetlands and property rights. Unfortunately,
the Clean Water Act has achieved only part of its goal. At least
one-third of the U.S. rivers, one-half of the U.S. estuaries, and more
than one-half of the lakes are still not safe for such uses as swimming
or fishing. At least 31 states have reported toxins in fish exceeding
the action levels set by the Food and Drug Administration (FDA). Every
pollutant cited in an USEPA study on chemicals in fish showed up in at
least one location. Water quality is seen as deteriorated and viewed as
the cause of the decreasing number of shellfish in the waters.

clean Water act facts • The Clean Water Act does not regulate most
ditches. • The Clean Water Act does not change exemptions for
agriculture. • The Clean Water Act does not regulate erosional features.
• The Clean Water Act does not regulate groundwater. • The Clean Water
Act does not regulate farm ponds. • The Clean Water Act does not
regulate land use. • The Clean Water Act does not change policy on
irrigation. • The Clean Water Act does not regulate puddles. • The Clean
Water Act does not change policy on stormwater. • The Clean Water Act
does not regulate water in tile drains. • The Clean Water Act does not
change policy on water transfers.

SAFE DRINKING WATER ACT When we get the opportunity to travel the world,
one of the first things we learn to ask is whether or not the water is
safe to drink. Unfortunately, in most of the places in the world, the
answer is ``no.'' As much as 80\% of all sickness in the world is
attributable to inadequate water or sanitation (Masters, 1991). The
American ecologist William C. Clark probably summed it up best during a
speech in Wisconsin in 1988: ``If you could tomorrow morning make water
clean in the world, you would have done, in one fell swoop, the best
thing you could have done for improving human health by improving
environmental quality.'' It has been estimated that threefourths of the
population in Asia, Africa, and Latin America lack a safe supply of
water for drinking, washing, and sanitation (Morrison, 1983). Money,
technology, education, and attention to the problem are essential for
improving these statistics and to solving the problem that this West
African proverb succinctly states: ``Filthy water cannot be washed.''
Left alone, Nature provides for us. Left alone, Nature feeds us. Left
alone, Nature refreshes and sustains us with untainted air. Left alone,
Nature provides and cleans the water we need to ingest to survive. As
Elliot A. Norse put it, ``In every glass of water we drink, some of the
water has already passed through fishes, trees, bacteria,

worms in the soil, and many other organisms, including people. \ldots{}
Living systems cleanse water and make it fit, among other things, for
human consumption'' (Hoage, 1985). Left alone, Nature performs at a
level of efficiency and perfection we cannot imagine. The problem, of
course, is that our human populations have grown too large to allow
Nature to be left alone. Our egos allow us to think that humans are the
real reason Nature exists at all. In our eyes, our infinite need for
water is why Nature works its hydrologic cycle---to provide the constant
supply of drinking water we need to sustain life---but the hydrologic
cycle itself is unstoppable, human activity or not. Bangs and Kallen
(1985) summed it up best: ``Of all our planet's activities---geological
movements, the reproduction and decay of biota, and even the disruptive
propensities of certain species (elephants and humans come to mind)---no
force is greater than the hydrologic cycle.'' Nature, through the
hydrologic cycle, provides us with an apparently endless supply of
water; however, developing and maintaining an adequate supply of safe
drinking water requires the coordinated efforts of scientists,
technologists, engineers, planners, water plant operators, and
regulatory officials. In this section, we concentrate on the regulations
that have been put into place in the United States to protect our water
supplies and ensure that they are safe, fresh, and palatable.
Legislation to protect drinking water quality in the United States dates
back to the Public Health Service Act of 1912. With time, the Act
evolved, but not until passage of the Safe Drinking Water Act (SDWA) in
1974 (which has since been amended several times) was federal
responsibility extended beyond interstate carriers to include all
community water systems serving 15 or more outlets, or 25 or more
customers. Prompted by public concern over findings of harmful chemicals
in drinking water supplies, the law established the basic federal--state
partnership for drinking water that is used today. It focuses on
ensuring safe water from public water supplies and on protecting the
nation's aquifers from contamination. Before examining the basic tenets
of the SDWA, it is first necessary to define several of the terms used
in the Act.

sDWa Definitions* Action level (AL)---The amount required to trigger
treatment or other action. Best management practices (BMPs)---Schedules
of activities, prohibitions of practices, maintenance procedures, and
other management practices to prevent or reduce the pollution of waters
of the United States. Consumer Confidence Report (CCR) or annual
drinking water quality report---An annual water quality report that a
community water system is required to provide to its customers. The CCR
helps people make informed choices about the water they drink by letting
people know what contaminants, if any, are in their drinking water and
how these contaminants may affect their health. The CCR also gives the
system a chance to tell customers what it takes to deliver safe drinking
water. Contaminant---Any physical, chemical, biological, or radiological
substance or matter in water. Discharge of a pollutant---Any addition of
any pollutant to navigable waters from any point source. Exemption---A
document issued to water systems having technical and financial
difficulty meeting the National Primary Drinking Water Regulations; it
is effective for 1 year and is granted by the USEPA due to compelling
factors. Likely source---Where a contaminant could have come from.
Maximum contaminant level (MCL)---The maximum permissible level of a
contaminant in water that is delivered to any user of a public water
system. Maximum contaminant level goal (MCLG)---The level at which no
known or anticipated adverse effects on the health of persons occur and
which allows an adequate margin of safety. Maximum residual disinfectant
level (MRDL)---The highest level of a disinfectant allowed in drinking
water. Maximum residual disinfectant level goal (MRDLG)---The level of a
drinking water disinfectant below which there is no known or expected
risk to health. Microbiological contaminants---Used as indicators that
other, potentially harmful bacteria may be present National Pollutant
Discharge Elimination System (NPDES)---The national program for issuing,
modifying, revoking and reissuing, terminating, monitoring, and
enforcing permits, in addition to imposing and enforcing pretreatment
requirements, under Sections 307, 402, 318, and 405 of the Clean Water
Act. Navigable waters---Waters of the United States, including
territorial seas. pCi/L---Picocuries per liter (measure of
radioactivity). Person---An individual, corporation, partnership,
association, state, municipality, commission, or political subdivision
of a state or any interstate body. Point source---Any discernible,
confined, and discrete conveyance, including but not limited to any
pipe, ditch, channel, tunnel, conduit, well, discrete fissure,
container, rolling stock, concentrated animal feeding operation, or
vessel or other floating craft from which pollutants are or may be
discharged. This term does not include agricultural stormwater
discharges and return flows from irrigated agriculture.
Pollutant---Dredged soil, solid waste, incinerator residue, filter
backwash, sewage, garbage, sewage sludge, munitions, chemical wastes,
biological materials, radioactive materials (except those regulated
under the Atomic Energy Act of 1954), heat, wrecked or discarded
equipment, rock, sand, cellar dirt, or industrial, municipal, and
agricultural waste discharged into water. It does not include: (a)
sewage from vessels, or (b) water, gas, or other material that is
injected into a well to facilitate production of oil or gas, or water
derived in association with oil and gas production and disposed of in a
well, if the well used either to facilitate production or for disposal
purposes is approved by authority of the state in which the well is
located, and if the state determines that the injection or disposal will
not result in the degradation of ground or surface water sources. Public
water system (PWS)---A system for the provision to the public of piped
water for human consumption, if such system has at least 15 service
connections or regularly serves at least 25 individuals.

Publicly owned treatment works (POTW)---Any device or system used in the
treatment of municipal sewage or industrial wastes of a liquid nature
which is owned by a state or municipality; includes sewer, pipes, or
other conveyances only if they convey wastewater to a POTW providing
treatment. Recharge zone---The area through which water enters a sole or
principal source aquifer. Regulated substances---Substances regulated by
the USEPA and that cannot be present at levels above the MCL.
Significant hazard to public health---Any level of contaminant that
causes or may cause the aquifer to exceed any maximum contaminant level
set forth in any promulgated National Primary Drinking Water Regulations
at any point where the water may be used for drinking purposes or which
may otherwise adversely affect the health of persons, or which may
require a public water system to install additional treatment to prevent
such adverse effect. Sole or principal source aquifer---An aquifer that
supplies 50\% or more of the drinking water for an area. Streamflow
source zone---Upstream headwaters areas that drain into an aquifer
recharge zone. Toxic pollutants---Pollutants that after discharge and
upon exposure, ingestion, inhalation, or assimilation into any organism
will, on the basis of the information available, cause death, disease,
behavioral abnormalities, cancer, genetic mutations, physiological
malfunctions, or physical deformations in such organisms or their
offspring. Treatment technique (TT)---A required process intended to
reduce the level of a substance in drinking water. Turbidity---A measure
of the cloudiness of water; turbidity is not necessarily harmful but can
interfere with the disinfection of drinking water. Unregulated monitored
substances---Substances that are not regulated by the USEPA but must be
monitored so information about their presence in drinking water can be
used to develop limits. Variance---Document issued to water systems
having technical and financial difficulty meeting National Primary
Drinking Water Regulations; it postpones compliance when such a delay
will not result in an unreasonable risk to health. Waters of the United
States---(a) All waters that are currently used, were used in the past,
or may be susceptible to use in interstate or foreign commerce,
including all waters that are subject to the ebb and flow of the tide;
(b) all interstate waters, including interstate wetlands; (c) all other
waters, such as interstate lakes, rivers, streams, mudflats, sandflats,
wetlands, sloughs, prairie potholes, wet meadows, playa lakes, or
natural ponds, the use, degradation, or destruction of which would
affect or could affect interstate or foreign commerce. Wetlands---Areas
that are inundated or saturated by surface or groundwater at a frequency
and duration sufficient to support a prevalence of vegetation typically
adapted for life in saturated soil conditions. Wetlands generally
include swamps, marshes, bogs, and similar areas.

sDWa specific proVisions To ensure the safety of public water supplies,
the Safe Drinking Water Act requires the USEPA to set safety standards
for drinking water. Standards are now in place for over 80 different
contaminants. The USEPA sets a maximum level for each contaminant;
however, in cases where making this distinction is not economically or
technologically feasible, the USEPA specifies an appropriate treatment
technology instead. Water suppliers must test their drinking water
supplies and maintain records to ensure quality and safety. Most states
carry the responsibility for ensuring that their public water supplies
are in compliance with the national safety standards. Provisions also
authorize the USEPA to conduct basic research on drinking water
contamination, to provide technical assistance to states and
municipalities, and to provide grants to states to help them manage
their drinking water programs. To protect groundwater supplies, the law
provides a framework for managing underground injection compliance. As
part of that responsibility, the USEPA may disallow new underground
injection wells based on concerns over possible contamination of a
current or potential drinking water aquifer. Each state is expected to
administer and enforce the SDWA regulations for all public water
systems. Public water systems must provide water treatment, ensure
proper drinking water quality through monitoring, and provide public
notification of contamination problems. The 1986 amendments to the SDWA
significantly expanded and strengthened its protection of drinking
water. Under the 1986 provisions, the SDWA required six basic
activities:

\begin{enumerate}
\item
  Establishment and enforcement of maximum contaminant levels (MCLs)---
  These are the maximum levels of certain contaminants that are allowed
  in drinking water from public systems. Under the 1986 amendments, the
  USEPA has set numerical standards or treatment techniques for an
  expanded number of contaminants.
\item
  Monitoring---The USEPA requires monitoring of all regulated and
  certain unregulated contaminants, depending on the number of people
  served by the system, the source of the water supply, and the
  contaminants likely to be found.
\item
  Filtration---The USEPA has criteria for determining which systems are
  obligated to filter water from surface water sources.
\item
  Disinfection---The USEPA must develop rules requiring all public water
  supplies to disinfect their water.
\item
  Use of lead materials---The use of solder or flux containing more than
  0.2\% lead or pipes and pipe fittings containing more than 8\% lead is
  prohibited in public water supply systems. Public notification is
  required where lead is used in construction materials of the public
  water supply system or where water is sufficiently corrosive to cause
  leaching of lead from the distribution system or lines.
\item
  Wellhead protection---All states are required to develop wellhead
  protection programs designed to protect public water supplies from
  sources of contamination. The USEPA developed national drinking water
  regulations to meet the requirements of the SDWA. These regulations
  are subdivided into National Primary Drinking Water Regulations (40
  CFR 141), which specify maximum contaminant levels (MCLs) based on
  health-related criteria, and the National Secondary Drinking Water
  Regulations (40 CFR 143), which are unenforceable guidelines based on
  aesthetic qualities, such as taste, odor, and color of drinking water,
  as well as on nonaesthetic qualities, such as corrosivity and
  hardness. In setting MCLs, the USEPA is required to balance the public
  health benefits of the standard against what is technologically and
  economically feasible. In this way, MCLs are different from other set
  standards, such as National Ambient Air Quality Standards (NAAQS),
  which must be set at levels that protect public health regardless of
  cost or feasibility (Masters, 1991). Note: If monitoring the
  contaminant level in drinking water is not economically or technically
  feasible, the USEPA must specify a treatment technique that will
  effectively remove the contaminant from the water supply or reduce its
  concentration. The MCLs currently cover a number of volatile organic
  chemicals, organic chemicals, inorganic chemicals, and radionuclides,
  as well as microorganisms and turbidity (cloudiness or muddiness). The
  MCLs are based on an assumed human consumption of 2 liters (roughly 2
  quarts) of water per day. The USEPA also creates unenforceable maximum
  contaminant level goals (MCLGs), which are set at levels that present
  no known or anticipated health effects and include a margin of safety,
  regardless of technological feasibility or cost. The USEPA is also
  required (under SDWA) to periodically review the actual MCLs to
  determine whether they can be brought closer to the desired MCLGs.
  Note: For noncarcinogens, MCLGs are arrived at in a three-step
  process. The first step is calculating the reference dose (RfD) for
  each contaminant. The RfD is an estimate of the amount of a chemical
  that a person can be exposed to on a daily basis that is not
  anticipated to cause adverse systemic health effects over the person's
  lifetime. A different assessment system is used for chemicals that are
  potential carcinogens. If toxicological evidence leads to the
  classification of the contaminant as a human or probable human
  carcinogen, the MCLG is set at zero (Boyce, 1997).
\end{enumerate}

National Primary Drinking Water Regulations Categories of primary
contaminants include organic chemicals, inorganic chemicals,
microorganisms, turbidity, and radionuclides. Except for some
microorganisms and nitrates, water that exceeds the listed MCLs will
pose no immediate threat to public health; however, all of these
substances must be controlled, because drinking water that exceeds the
standards over long periods of time may be harmful. Note: As we learn
more from research about the health effects of various contaminants, the
number of regulated organics is likely to grow. Public drinking water
supplies must be sampled and analyzed for organic chemicals at least
every 3 years.

Organic Chemicals Organic contaminants for which MCLs are being
promulgated are classified into the following three groupings: synthetic
organic chemicals (SOCs), volatile organic chemicals (VOCs), and
trihalomethanes (THMs). Table 3.1 provides a partial list

of maximum allowable levels for several selected organic contaminants.
Synthetic organic chemicals are manmade and are often toxic to living
organisms. These compounds are used in the manufacture of a wide variety
of agricultural and industrial products. This group includes primarily
PCBs, carbon tetrachloride, pesticides and herbicides such as 2,4-D,
aldicarb, chlordane, dioxin, xylene, phenols, and thousands of other
synthetic chemicals. Note: As we learn more from research about the
health effects of various contaminants, the number of regulated organics
is likely to grow. Public drinking water supplies must be sampled and
analyzed for organic chemicals at least every 3 years. Note: A study of
29 Midwestern cities and towns by the Washington, DC-based nonprofit
Environmental Working Group found pesticide residues in the drinking
water in nearly all of them. In Danville, Illinois, the level of
cyanazine, a weed killer manufactured by DuPont, was 34 times the
federal standard. In Fort Wayne, Indiana, one glass of tap water
contained nine kinds of pesticides. The fact is, each year,
approximately 2.6 billion pounds of pesticides are used in the United
States (Lewis, 1996). These pesticides find their way into water
supplies and thus present increased risk to public health. Volatile
organic chemicals are synthetic chemicals that readily vaporize at room
temperature. Chemicals used in degreasing agents, paint thinners, glues,
dyes, and some pesticides fall into this category. VOCs include benzene,
carbon tetrachloride, 1,1,1-trichloroethane (TCA), trichloroethylene
(TCE), and vinyl chloride. Note: VOCs are particularly dangerous in
water. They are absorbed through the skin through contact with
water---for example, every shower or bath. Hot water allows these
compounds to evaporate rapidly, and they are harmful if inhaled. VOCs
can be present in any tap water, regardless of location or water source.
If tap water contains significant levels of these compounds, they pose a
health threat from skin contact, even if the water is not ingested
(Ingram, 1991). Trihalomethanes (THMs) are created in the water itself
as byproducts of water chlorination. Chlorine (present in essentially
all U.S. tap water) combines with organic chemicals to form THMs (see
Figure 3.1). They include chloroform, bromodichloromethane,
dibormochloromethane, and bromoform. Note: THMs are known
carcinogens---substances that increase the risk of getting cancer---and
they are present at varying levels in all public tap water.

Inorganic Chemicals Several inorganic substances (particularly lead,
arsenic, mercury, and cadmium) are of public health importance. These
inorganic contaminants and others contaminate drinking water supplies as
a result of natural processes, environmental factors, or, more commonly,
human activity. Some of these are listed in Table 3.2. For most
inorganics, MCLs are the same as MCLGs, but the MCLG for lead is zero.

Microorganisms (Microbiological Contaminants) This group of contaminants
includes bacteria, viruses, and protozoa, which can cause typhoid,
cholera, and hepatitis, as well as other waterborne diseases. Bacteria
are closely monitored in water supplies because they can be dangerous
and because their presence is easily detected. Because tests designed to
detect individual microorganisms in water are difficult to perform, in
actual practice a given water supply is not tested by individually
testing for specific pathogenic microorganisms. Instead, a simpler
technique is used, based on testing water for evidence of any fecal
contamination. Coliform bacteria are used as indicator organisms whose
presence suggests that the water is contaminated. In testing for total
coliforms, the number of monthly samples required is based on the
population served and the size of the distribution system. Because the
number of coliform bacteria excreted in feces is on the order of 50
million per gram and the concentration of coliforms in untreated
domestic wastewater is usually several million per 100 mL, it is highly
unlikely that water contaminated with human wastes would have no
coliforms. That conclusion is the basis for the drinking water standard
for microbiological contaminants, which specifies in essence that, on
the average, water should contain no more than 1 coliform per 100 mL.
The SDWA standards now require that coliforms not be found in more than
5\% of the samples examined during a 1-month period. Known as the
presence/absence concept, it replaces previous MCLs based on the number
of coliforms detected in the sample. Viruses are very common in water.
If we removed a teaspoonful of water from an unpolluted lake, over 1
billion viruses would be present in the water (Figure 3.2). The two most
common and troublesome protozoa found in water are Giardia and
Cryptosporidium (or Crypto). In water, these protozoa occur in the form
of hardshelled cysts. Their hard covering makes them resistant to
chlorination and chlorine residual that kills other organisms. We cover
microorganisms commonly found in water in much greater detail in Chapter
6.

Turbidity Turbidity, a measure of fine suspended matter in water, is
primarily caused by clay, silt, organic particulates, plankton, and
other microscopic organisms, ranging in size from colloidal to coarse
dispersion. Turbidity in the water is expressed in nephelometric
turbidity units (NTUs), which represent the amount of light scattered or
reflected from the water. Turbidity is officially reported in standard
units known as Jackson turbidity units, which are equivalent to
milligrams per liter of silica (diatomaceous earth) that could cause the
same optical effect. Turbidity testing is not required for groundwater
sources. Radionuclides Radioactive contamination of drinking water is a
serious matter. Radionuclides (the radioactive metals and minerals that
cause this contamination) come from both natural and manmade sources.
Naturally occurring radioactive minerals move from underground rock
strata and geologic formations into the underground streams flowing
through them and primarily affect groundwater. In water, radium-226,
radium-228, radon-222, and uranium are the natural radionuclides of most
concern. Uranium is typically found in groundwater and, to a lesser
degree, in some surface waters. Radium in water is found primarily in
groundwater. Radon, a colorless, odorless gas and a known cancer-causing
agent, is created by the natural decay of minerals. Radon is an unusual
contaminant in water, because the danger arises not from drinking
radon-contaminated water but from breathing the gas after it has been
released into the air. Radon dissipates rapidly when exposed to air (see
Figure 3.3). When present in household water, it evaporates easily into
the air, where household members may inhale it. Some experts believe
that the effects of radon inhalation are more dangerous than those of
any other environmental hazard. Manmade radionuclides (more than 200 are
known) are believed to be potential drinking water contaminants. Manmade
sources of radioactive minerals in water are nuclear power plants,
nuclear weapons facilities, radioactive materials disposal sites, and
docks for nuclear-powered ships.

National Secondary Drinking Water Regulations The National Secondary
Drinking Water Regulations are non-enforceable guidelines regulating
contaminants that may cause cosmetic effects (such as skin or tooth
discoloration) or aesthetic effects (such as taste, odor, or color) in
drinking water. A range of concentrations is established for substances
that affect water only aesthetically and have no direct effect on public
health. Table 3.3 presents some secondary standards.

amenDments to the sDWa After more than 3 years of effort, the Safe
Drinking Water Act Reauthorization (one of the most significant pieces
of environmental legislation passed to date) was adopted by Congress and
signed into law by President Clinton in 1996. The new streamlined
version of the original SDWA gave states greater flexibility in
identifying and considering

the likelihood for contamination in potable water supplies and in
establishing monitoring criteria. It established increased reliance on
sound science instead of feel-good science, paired with more consumer
information presented in readily understandable form, and it called for
increased attention to the assessment and protection of source waters.
The significance of the 1996 SDWA amendments lies in the fact that they
were a radical rewrite of the law that the USEPA, states, and water
systems had been trying to implement for the previous 10 years. In
contrast to the 1986 amendments (which were crafted with little
substantive input from the regulated community and embraced a
command-and-control approach with compliance costs rooted in water
rates), the 1996 amendments were developed with significant
contributions from water suppliers and state and local officials and
embodied a partnership approach that included major new infusions of
federal funds to help water utilities---especially the thousands of
smaller systems---comply with the law. Table 3.4 provides a summary of
many of the major provisions of the 1996 amendments, which are as
complex as they are comprehensive. Additional SDWA amendments have been
proposed and passed since 1996.

implementing the sDWa On December 3, 1998, at the oceanfront of Fort
Adams State Park, Newport, Rhode Island, President Clinton announced
significant updates of the 1996 SDWA amendments, with the expectation
that the new requirements would protect most of the nation from
dangerous contaminants while adding only about \$2 to many monthly water
bills. The rules required approximately 13,000 municipal water suppliers
to use better filtering systems to screen out Cryptosporidium and other
microorganisms, ensuring that U.S. community water supplies would be
safe from microbial contamination. In his speech, President Clinton
said:

This past summer I announced a new rule requiring utilities across the
country to provide their customers regular reports on the quality of
their drinking water. When it comes to the water our children drink,
Americans cannot be too vigilant. Today I want to announce three other
actions I am taking. First, we're escalating our attack on the invisible
microbes that sometimes creep into the water supply. \ldots{} Today, the
new standards we put in place will significantly reduce the risk from
Cryptosporidium and other microbes, to ensure that no community ever has
to endure an outbreak like the one Milwaukee suffered. Second, we are
taking steps to ensure that when we treat our water, we do it as safely
as possible. One of the great health advances to the 20th century is the
control of typhoid, cholera, and other diseases with disinfectants. Most
of the children in this audience have never heard of typhoid and
cholera, but their grandparents cowered in fear of it, and their
great-grandparents took it as a fact of life that it would take away
significant numbers of the young people of their generation. But as with
so many advances, there are tradeoffs. We now see that some of the
disinfectants we use to protect our water can actually combine with
natural substances to create harmful compounds. So today I'm announcing
standards to significantly reduce our exposure to these harmful
byproducts, to give our families greater peace of mind with their water.
The third thing we are doing today is to help communities meet higher
standards, releasing almost \$800 million to help communities in all 50
states to upgrade their drinking water systems \ldots{} to give 140
million Americans safer drinking water. Consumer Confidence Reports In
his comments, President Clinton mentioned the requirement of community
water systems to put annual drinking water quality reports into the
hands of their customers (see Table 3.4). While water systems are free
to enhance their reports in any useful way, each report must provide
consumers with the following fundamental information about their
drinking water:

• The lake, river, aquifer, or other source of the drinking water.
Consider the following actual report provided by a city referred to here
as Capital City: Capital City receives its raw (untreated) water from
eight reservoirs, two rivers, and four deep wells. From these sources,
raw water is pumped to one of the Department of Utilities' two water
treatment plants, where it is filtered and disinfected. Once tested for
top quality, Capital City drinking water is pumped on demand to your
tap. • A brief summary of the susceptibility to contamination of the
local drinking water source, based on the source water assessments by
states. Consider the following actual information provided by Capital
City: Contaminants that may be present in source (raw) water include --
Microbial contaminants, such as viruses and bacteria, which may come
from sewage treatment plants, septic systems, agricultural livestock
operations, wildlife -- Inorganic contaminants, such as salts and
metals, which can be naturally occurring or result from urban stormwater
runoff, industrial or domestic; wastewater discharges; oil and gas
production; mining; or farming -- Pesticides and herbicides, which may
come from a variety of sources such as agriculture, urban stormwater
runoff, and residential uses -- Radioactive contaminants, which can be
naturally occurring or the result of oil and gas production and mining
activities • How to get a copy of the water system's complete source
water assessment. Consider the following taken from Capital City's
Consumer Confidence Report (annual drinking water quality report): For a
copy of the water system's complete source water assessment and
questions regarding this report contact Capital City's Water Quality Lab
at xxx-xxx-xxxx. For more information about decisions affecting your
drinking water quality, you may attend Capital City city council
meetings. For times and agendas, call Capital City Clerk's Office at
xxx-xxx.xxxx. • The level (or range of levels) of any contaminant found
in local drinking water, as well as USEPA's health-based standard
(maximum contaminant level) for comparison.

The likely source of that contaminant in the local drinking water
supply. • The potential health effects of any contaminant detected in
violation of an USEPA health standard and an accounting of the system's
actions to restore safe drinking water. • The water system's compliance
with other drinking water-related rules. • An educational statement for
vulnerable populations about avoiding Cryptosporidium. • Educational
information on nitrate, arsenic, or lead in areas where these
contaminants may be a concern. With regard to the levels of
contaminants, their possible sources, and the levels detected in local
drinking water supplies, consider the information provided by Capital
City to its ratepayers in the city's annual drinking water quality
report for 2011, which is shown in Table 3.5. • Phone numbers of
additional sources of information, including the water system and
USEPA's Safe Drinking Water Hotline (800-426-4791).

This information supplements public notification that water systems must
provide to their customers upon discovering any violation of a
contaminant standard. This annual report should not be the primary
notification of potential health risks posed by drinking water; rather,
it provides customers with water quality information from the previous
year.

Drinking Water Supplies INTRODUCTION Where do we get our drinking water
from? What is the source? The answer would most likely turn to one of
two possibilities: groundwater or surface water. This answer seems
simple enough, because these two sources are, indeed, the primary
sources of most water supplies. From the earlier discussion about the
hydrologic or water cycle, we know that, from whichever source we obtain
our drinking water, the source is constantly being replenished (we hope)
with a supply of freshwater. This water cycle phenomenon was best summed
up by Heraclitus of Ephesus, who said, ``You could not step twice into
the same river; for other waters are ever flowing on to you.'' This
chapter discusses one of the drinking water practitioner's primary
duties---finding and securing a source of potable water for human use.

WATER SOURCES* In the real estate business, location is everything. The
same can be said when it comes to sources of water. In fact, the
presence of water defines ``location'' for communities. Although
communities differ widely in character and size, all have the common
concerns of finding water for industrial, commercial, and residential
use. Freshwater sources that can provide stable and plentiful supplies
for a community do not always occur where we wish. Simply put, on land,
the availability of a regular supply of

potable water is the most important factor affecting the presence---or
absence---of many life forms. A map of the world immediately shows us
that surface waters are not uniformly distributed over the surface of
the Earth. American lands hold rivers, lakes, and streams on only about
4\% of their surface. The heaviest populations of any life forms,
including humans, are found in regions of the United States (and the
rest of the world) where potable water is readily available, because
lands barren of water simply cannot support large populations. Note: The
volume of freshwater sources depends on geographic, landscape, and
temporal variations, as well as on the impact of human activities.

Just hoW reaDily aVailable is potable Water? Approximately 326 million
cubic miles of water comprise Earth's entire water supply. Although it
provides us indirectly with freshwater through evaporation from the
oceans, only about 3\% of this massive amount of water is fresh, and
most of that minute percentage of freshwater is locked up in polar ice
caps and glaciers. The rest is held in lakes, in flows through soil, and
in river and stream systems. Only 0.027\% of Earth's freshwater is
available for human consumption (see Table 4.1 for the

distribution percentages of Earth's water supply). We see from Table 4.1
that the major sources of drinking water are from surface water,
groundwater, and from groundwater under the direct influence of surface
water (i.e., springs or shallow wells).

surface Water supplies Most surface water originates directly from
precipitation---rainfall or snow. To gain an appreciation for the impact
of this runoff on surface water supplies, let's take a look at the water
balance in the United States. Over the U.S. mainland, rainfall averages
about 4250 billion gallons per day. Of this massive amount, about 66\%
returns to the atmosphere through evaporation directly from the surface
of lakes and rivers and transpiration from plants. This leaves about
1250 billion gallons per day to flow across or through the ground to
return to the sea (see Table 4.2). Although municipal usage of water is
only a small fraction of this great volume, the per capita consumption
of water in the United States is rather high---about 80 to 100 gallons
per person per day (USGS, 2016), probably because public water is
relatively inexpensive here. In areas where water supplies are less
readily available and thus more costly, per capita consumption is much
lower, due to both financial and conservation concerns.

The 1250 billion gallons per day of surface runoff water, exposed and
open to the atmosphere, results from the movement of water on and just
beneath the surface of the Earth, referred to as overland flow. Simply
put, overland flow and surface runoff are the same---water flow that has
not yet reached a definite stream channel. This occurs when the rate of
precipitation exceeds either the rate of interception and
evapotranspiration or the amount of rainfall readily absorbed by the
surface of the Earth. The total land area that contributes runoff to a
stream or river is called a watershed, drainage basin, or catchment
area. Specific sources of surface water include

• Rivers • Streams • Lakes • Impoundments (manmade lakes made by damming
a river or stream) • Very shallow wells that receive input via
precipitation • Springs affected by precipitation (flow or quantity
directly dependent upon precipitation) • Rain catchments (drainage
basins) • Tundra ponds or muskegs (peat bogs)

Surface water has advantages as a source of potable water. Surface water
sources are usually easy to locate; unlike groundwater, finding surface
water does not require a geologist or hydrologist. Normally, surface
water is not tainted with minerals precipitated from the Earth's strata.
Ease of discovery aside, surface water also presents some disadvantages.
Surface water sources are easily contaminated with microorganisms that
can cause waterborne diseases and are polluted by chemicals that enter
from surrounding runoff and upstream discharges. Water rights can also
present problems. As we have said, most surface water is the result of
surface runoff. The amount and flow rate of this surface water are
highly variable for two main reasons: (1) human interference, and (2)
natural conditions. In some cases, surface water quickly runs off land
surfaces. From a water resources standpoint, this is generally
undesirable, because quick runoff does not provide enough time for the
water to infiltrate into the ground and recharge groundwater aquifers.
Also, surface water that quickly runs off land causes erosion and
flooding problems. Probably the only good thing that can be said about
surface water that runs off quickly is that it usually does not have
enough contact time to increase in mineral content. Surface water that
drains slowly off land has all the opposite effects. Drainage basins
collect surface water and direct it on its gravitationally influenced
path to the ocean. The drainage basin is normally characterized as an
area measured in square miles, acres, or sections. Obviously, if a
community is drawing water from a surface water source, the size of its
drainage basin is an important consideration. Surface water runoff, like
the flow of electricity, flows or follows the path of least resistance.
Surface water within the drainage basin normally flows toward one
primary watercourse (e.g., river, stream, brook, creek), unless some
manmade distribution system (canal or pipeline) diverts the flow.
Surface water runoff from land surfaces depends on several factors (see
Figure 4.1):

• Rainfall duration---Even a light, gentle rain, if it lasts long
enough, can, with time, saturate soil and allow runoff to take place. •
Rainfall intensity---With increases in intensity, the surface of the
soil quickly becomes saturated. This saturated soil can hold no more
water; as more rain falls and water builds up on the surface, surface
runoff is produced. • Soil composition---The composition of the surface
soil directly affects the amount of runoff; for example, it is obvious
that hard rock surfaces result in 100\% runoff. Clay soils have very
small void spaces that swell when wet; when these void spaces fill and
close, they do not allow infiltration. Coarse sand possesses large void
spaces that allow water to flow easily through it, thus producing the
opposite effect of clay soil, even in a torrential downpour. • Soil
moisture---The amount of existing moisture in the soil has a definite
impact on surface water runoff. Soil already wet or saturated from a
previous rain causes surface water runoff to occur sooner than if the
soil were dry. Surface water runoff from frozen soil can be as high as
100\% of the snow melt or rain runoff because frozen ground is basically
impervious. • Vegetation cover---Groundcover limits runoff. Roots of
vegetation and pine needles, pine cones, leaves, and branches create a
porous layer (a sheet of decaying natural organic substances) above the
soil. This porous, organic layer readily allows water into the soil.
Vegetation and organic waste also act as cover to protect the soil from
hard, driving rains, which can compact bare soils, close off void
spaces, and increase runoff. Vegetation and groundcover work to maintain
the infiltration and water-holding capacity of the soil and also work to
reduce soil moisture evaporation. • Ground slope---When rain falls on
steeply sloping ground, 80\% or more may become surface runoff. Gravity
moves the water down the surface more quickly than it can infiltrate the
surface. Water flow off flat land is usually slow enough to provide
opportunity for a higher percentage of the rainwater to infiltrate the
ground. • Human influences---Various human activities have a definite
impact on surface water runoff. Most human activities tend to increase
the rate of water flow; for example, canals and ditches are usually
constructed to provide

steady flow, and agricultural activities generally remove groundcover
that would work to retard the runoff rate. At the opposite extreme,
manmade dams are generally built to retard the flow of runoff.

grounDWater supply Unbeknownst to many of us, our Earth possesses an
unseen ocean. This ocean, unlike the surface oceans that cover most of
the globe, is freshwater: the groundwater that lies contained in
aquifers beneath Earth's crust. This gigantic water source forms a
reservoir that feeds all the natural fountains and springs of this
planet, but how does water travel into the aquifers that lie under the
surface of the Earth? Groundwater sources are replenished from a
percentage of the average approximately 3 feet of water that falls to
Earth each year on every square foot of land. Water falling to Earth as
precipitation follows three courses. Some runs off directly to rivers
and streams (roughly 6 inches of that 3 feet), eventually working its
way back to the sea. Evaporation and transpiration through vegetation
takes up about 2 feet. The remaining 6 inches seeps into the ground,
entering and filling every interstice, each hollow and cavity. Although
groundwater accounts for only 1/6 of the total 1,680,000 miles of water,
if we could spread this water out over the land, it would blanket it to
a depth of 1000 feet. Almost all groundwater is in constant motion
through the pores and crevices of the aquifer in which it occurs. The
water table is rarely level; it generally follows the shape of the
ground surface. Groundwater flows in the downhill direction of the
sloping water table. The water table sometimes intersects low points of
the ground, where it seeps out into springs, lakes, or streams. Usual
groundwater sources include wells and springs that are not influenced by
surface water or local hydrologic events. As a potable water source,
groundwater has several advantages over surface water. Unlike surface
water, groundwater is not easily contaminated. Groundwater sources are
usually lower in bacteriological contamination than surface waters.
Groundwater quality and quantity usually remain stable throughout the
year. In the United States, groundwater is available in most locations.
As a potable water source, groundwater does present some disadvantages
compared to surface water sources. Operating costs are usually higher,
because groundwater supplies must be pumped to the surface. Any
contamination is often hidden

from view, and removing contaminants can be very difficult. Groundwater
often possesses high mineral levels and thus an increased level of
hardness, because it is in contact longer with minerals. Near coastal
areas, groundwater sources may be subject to saltwater intrusion. Note:
Groundwater quality is influenced by the quality of its source. Changes
in source waters or degraded quality of source supplies may seriously
impair the quality of the groundwater supply.

SUMMARY Our freshwater supplies are constantly renewed through the
hydrologic cycle, but the normal ratio of freshwater to saltwater is not
subject to change. As our population grows and we move into lands
without ready freshwater supplies, we place ecological strain upon those
areas and on their ability to support life. Communities that build in
areas without adequate local water supplies are putting themselves at
risk. Proper attention to our surface and groundwater sources, including
remediation, pollution control, and water reclamation and reuse, can
help to ease the strain, but technology cannot fully replace adequate
local freshwater supplies, whether from surface water or groundwater
sources. Drinking Water Conveyance and Distribution

INTRODUCTION Before beginning our discussion of drinking water
conveyance and distribution systems, let's review the information
covered to this point. In many cases, a municipal water supply provides
drinking water for use in homes and industries. This same water supply
source may also be used for irrigation, for extinguishing fires, for
street cleaning, for carrying wastes to treatment facilities, and for
many other purposes. The two most important factors in any water supply
are its quantity and quality (Q and Q factors). We now need to add a
third factor to the mix: the location of the water supply relative to
points of use. Note that each type of water use has its own
prerequisites. Food processing plants, for example, require large
volumes and high water quality. Waste conveyance systems, on the other
hand, require only quantity or volume. Recall that the earliest
communities were almost exclusively located near a water source (see
Figure 5.1). Further, the evolution of public water supply systems is
tied directly to the growth of cities. In cases where a surface water
source was not available, settlers dug wells to supply water to
community residents.

A typical water supply system consists of six functional elements: (1) a
source or sources of supply; (2) storage facilities (e.g., impoundment
reservoirs); (3) transmission facilities to transport water from the
point of storage to the treatment plant; (4) treatment facilities for
altering water quality; (5) transmission and storage facilities for
transporting water to intermediate points (such as water towers or
standpipes); and (6) distribution facilities for bringing water to
individual users (see Figure 5.2). Recall that when precipitation falls
on a watershed or catchment area, it either flows as runoff above ground
to streams and rivers or soaks into the ground to reappear in springs or
where it can be drawn from wells. A water supply can come from a
catchment area that may contain several thousands of acres (or hectares)
of land, draining to streams whose flow is retained in impoundment
reservoirs. If a water supply is drawn from a large river or lake, the
catchment area is the entire area upstream from the point of intake.
Obviously, the amount of water that enters a water supply system depends
on the amount of precipitation and the volume of the runoff. The annual
average precipitation in the United States is about 30 inches, of which
two-thirds is lost to the atmosphere by evaporation and transpiration.
The remaining water becomes runoff into rivers and lakes or, through
infiltration, replenishes groundwater. Precipitation and runoff vary
greatly with geography and season. Drinking water comes from surface
water and groundwater. Large-scale water supply systems tend to rely on
surface water resources; smaller water systems tend to use groundwater.
If surface water is the source of supply for a particular drinking water
supply system, the water is obtained from lakes, streams, rivers, or
ponds. Storage reservoirs (artificial lakes created by constructing dams
across stream valleys) can hold back higher than average flows and
release them when greater flows are needed. Water supplies may be taken
directly from reservoirs or from locations downstream of the dams.
Reservoirs may serve other purposes in addition to water supply,
including flood mitigation, hydroelectric power, and water-based
recreation. Groundwater is pumped from natural springs, from wells, and
from infiltration galleries, basins, or cribs. Most small and some large
U.S. water systems use groundwater as their source of supply.
Groundwater may be drawn from the pores of alluvial, glacial, or aeolian
deposits of granular unconsolidated material (such as sand and gravel);
from the solution passages, caverns, and cleavage planes of sedimentary
rocks (such as limestone, slate, and shale); and from combinations of
these geologic formations. Groundwater sources may have intake or
recharge areas that are miles away from points of withdrawal
(water-bearing stratum or aquifer). Water quality in aquifers (geologic
formations that contain water) and water produced by wells depend on the
nature of the rock, sand, or soil in the aquifer from which the well
withdraws water. Drinking water wells may be shallow (50 feet or less)
or deep (more than 1000 feet). Including the approximately 23 million
Americans who use groundwater as private drinking water sources,
slightly more than half of the U.S. population receives its drinking
water from groundwater sources. As the title of this chapter indicates,
the sections that follow focus on the conveyance and distribution of
drinking water. For the purposes of simplifying the discussion about
this stage of potable water systems and treatment, the treatment process
that most surface water undergoes before it is conveyed and distributed
to the consumer will be covered later in Chapter 10.

SURFACE WATER AND GROUNDWATER DISTRIBUTION SYSTEMS Major water supply
systems can generally be divided into two categories based on the source
of water they use. The water source, in turn, impacts the design,
construction, and operation of the water distribution systems. The types
of systems, classified by source (see Figures 5.3 and 5.4), are

• Surface water supply systems • Groundwater supply systems

Surface water (acquired from rivers, lakes, or reservoirs) flows through
an intake structure into the transmission system (Figure 5.3). From a
groundwater source, flow moves through an intake pipe and then is pumped
through a transmission conduit that conveys the water to a distribution
system (Figure 5.4). Groundwater is generally available in most of the
United States; however, the amount available for withdrawal at any
particular location is usually limited. Surface water and groundwater
supply systems may be comprised of canals, pipes, and other conveyances;
pumping plants; distribution reservoirs or tanks to assist in balancing
supplies and demands for water and to control pressures; other
appurtenances; and treatment works. Note: To illustrate the process of
drinking water supply conveyance and distribution in its most basic
form, we concentrate here on the major components of a typical surface
water supply and distribution system. In a typical community water
supply system, water is transported under pressure through a
distribution network of buried pipes. Smaller pipes (house service
lines) attached to the main water lines bring water from the
distribution network to households. In many community water supply
systems, pumping water up into storage tanks that store water at higher
elevations than the households they serve provides water pressure. The
force of gravity then pushes the water into homes when household taps
open. Households on private supplies usually get their water from
private wells. A pump brings the water out of the ground and into a
small tank within the home, where the water is stored under pressure. In
cities, the distribution system generally follows street patterns, but
it is also affected by topography and by the types of residential,
commercial, and industrial development, as well as the location of
treatment facilities and storage works. A distribution system is often
divided into zones that correspond to different ground elevations and
service pressures. The water pipes (mains) are generally enclosed loops,
so supply to any point can be provided from at least two directions.
Street mains usually have a minimum diameter of 6 to 8 inches to provide
adequate flows for buildings and for fighting fires. The pipes connected
to buildings may range down in size to as little as 1 inch for small
residences.

surface Water intake Withdrawing water from a lake, reservoir, or river
requires an intake structure. Because surface sources of water are
subject to wide variations in flow, quality, and temperature, intake
structures must be designed so the required flow can be withdrawn
despite these natural fluctuations. Surface water intakes consist of
screened openings and conduit that conveys the flow to a sump from which
it may be pumped to the treatment works. Typical intakes are towers,
submerged ports, and shoreline structures. Intakes function primarily to
supply the highest quality water from the source and to protect
downstream piping, equipment, and unit processes from damage or clogging
as a result of floating and submerged debris, flooding, and wave action.
To facilitate this, intakes should be located to consider the effects of
anticipated variations in water level, navigation requirements, local
currents and patterns of sediment deposition and scour, spatial and
temporal variations in water quality, and the quantity of floating
debris. For lakes and impounding reservoirs where fluctuating water
levels and variations in water quality with depth are common, intake
structures that permit withdrawal over a wide range of elevations are
typically used. Towers (Figure 5.5) are commonly used for reservoirs and
lakes. A tower water intake provides ports located at several depths,
avoiding the problems of water quality that stem from locating a single
inlet at the bottom, as water quality varies with both time and depth.
With the exception of brief periods in spring and fall when overturns
may occur, water quality is usually best close to the surface, thus
intake ports located at several depths permit selection of the most
desirable water quality in any season of the year (see Figure 5.5).
Submerged ports also have the advantage of remaining free from ice and
floating debris. Selection of port levels must be related to
characteristics of the water body (Hammer and Hammer, 1996).
Considerations that affect lake intake location include (McGhee, 1991):

• Locate intakes as far as possible from any source of pollution. •
Factor in wind and current effects on the motion of contaminants.Provide
sufficient water depth (typically 20 to 30 feet) to prevent blocking of
the intake by ice jams that may fill shallower lake areas to the bottom.

River intakes are typically designed to withdraw water from slightly
below the surface to avoid both sediment in suspension at lower levels
and floating debris; if necessary, they are also located at levels low
enough to meet navigation requirements. Generally, river intakes are
submerged (see Figure 5.6) or screened shore intakes (see Figure 5.7).
Because of low costs, the submerged type is widely used for small river
and lake intakes.

Note: Submerged intakes are not readily accessible when they require
repair or maintenance. When they are used in lakes or reservoirs,
another distinct disadvantage is the lack of alternative withdrawal
levels available to provide the highest quality of water throughout the
year.

types of surface Water Distribution Along with providing potable water
to the household tap, water distribution systems are ordinarily designed
to adequately satisfy the water requirements for a combination of
domestic, commercial, industrial, and firefighting purposes. The system
should be capable of meeting the demands placed on it at all times and
at satisfactory pressures. Pipe systems, pumping stations, storage
facilities, fire hydrants, house service connections, meters, and other
appurtenances are the main elements of the system (Cesario, 1995). Water
is normally distributed by one of three different means: gravity
distribution, pumping without storage, or pumping with storage. Gravity
distribution is possible only when the source of supply is located
substantially above the level of the community. Pumping without storage
(the least desirable method because it provides no reserve flow and
pressures fluctuate substantially) use sophisticated control systems to
match an unpredictable demand. Pumping with storage is the most common
method of distribution (McGhee, 1991).

Distribution line netWork Distribution systems may be generally
classified as grid systems, branching systems, a combination of these,
or dead-end systems (see Figure 5.8). The branching system shown in
Figure 5.8A is not the preferred distribution network, because it does
not furnish supply to any point from at least two directions and because
it includes several terminals or dead ends. Normally, grid systems (see
Figure 5.8B,C) are the best arrangement for distributing water. All of
the arterials and secondary mains are looped and interconnected,
eliminating dead ends and permitting water circulation in such a way
that a heavy discharge from one main allows drawing water from other
pipes. Newly constructed distribution systems avoid the antiquated
dead-end system (see Figure 5.8D), and such systems are often
retrofitted later by incorporating proper looping.

serVice connection to householD tap A typical service connection
consists of a pipe from the distribution system to a turnoff valve
located near the property line (see Figure 5.9).

Distribution anD storage Distribution reservoirs and other storage
facilities or vessels are in place to provide a sufficient amount of
water to average or equalize daily demands on the water supply system
(see Figure 5.10). Storage also serves to increase operating
convenience, to level out pumping requirements, to decrease power costs,
to provide water during power source or pump failure, to provide large
quantities of water to meet fire demands, to provide surge relief, to
increase detention time, and to blend water sources. Generally, six
different types of storage are employed in storing potable water: clear
wells, elevated tanks, stand pipes, ground-level reservoirs,
hydropneumatic or pressure tanks, and surge tanks.

\begin{enumerate}
\item
  Clear wells are used to store filtered water from a treatment works
  and serve as chlorine contact tanks (Figure 5.11).
\item
  Elevated tanks are located above the service zone and are used
  primarily to maintain an adequate and fairly uniform pressure to the
  service zone (Figure 5.12).
\item
  Stand pipes are tanks that stand on the ground; they have a height
  greater than their diameter (Figure 5.13).
\item
  Ground-level reservoirs are located above service areas to maintain
  the required pressures (Figure 5.14).
\end{enumerate}

FIGURE 5.14 Ground-level reservoir.

\begin{enumerate}
\item
  Hydropneumatic or pressure tanks are generally used in small water
  systems, usually with a well or booster pump. The tank is used to
  maintain water pressures in the system and to control the operation of
  the well pump or booster pump (Figure 5.15).
\item
  Surge tanks are not necessarily storage facilities but are primarily
  used to control water hammer or to regulate water flow (Figure 5.16).
  Protective Coatings and Cathodic Protection Storing water in a storage
  tank that is not properly protected and preserved from corrosion makes
  little sense and can be highly dangerous. Tanks in improper physical
  condition actually degrade the water stored within them. With any
  storage tank, any coating or preservative that will be in contact with
  potable water must meet the National Sanitation Foundation (NSF)
  Standard 61. NSF Standard 61 lists the following types of coatings
  normally used in protecting tank surfaces:
\end{enumerate}

• Epoxy-based coatings • Powdered epoxy coatings • Vitreous coatings
such as glass fused to steel • Cement coatings • Polyurethane •
Polymer-modified asphaltic membrane • Galvanized steel • Lubricants •
Asphaltic coatings • Vinyl ester

The following actions or conditions can enhance the life of coating
systems:

• Proper surface preparation prior to coating application • High quality
of the coating (paint) • Skilled workmanship • Drying and aging of the
coating • Proper maintenance through periodic inspection; spot, partial,
or complete removal of old paint; and repainting as necessary

Cathodic protection (an electrical system) is often used in preserving
the integrity and material condition of potable water storage tanks by
preventing the corrosion and pitting of steel and iron surfaces in
contact with water. By passing a low-voltage current through a liquid or
soil in contact with the metal in such a manner that the external
electromotive force renders that metal structure cathodic, corrosion is
transferred to auxiliary sacrificial anodic parts. Intended to corrode
rather than the water storage facility corroding, sacrificial anodes are
typically made of magnesium or zinc and must be replaced periodically as
they are used up. Typically, the anodes are suspended from the tank roof
in warm climates or are submerged in cold climates. The life of the
sacrificial anode is about 10 years. Note that cathodic protection is
not a substitute for the use of a proper interior coating system.

Water Quality Monitoring at Storage Facilities Water stored in potable
water storage facilities must be routinely and properly monitored.
Monitoring includes determining chlorine residual levels, turbidity,
color, coliform analysis, decimal dilution, most probable number (MPN)
analysis, and taste and odor analysis.

Water Quality Problems in Storage/Distribution Systems The potable water
practitioner may be called upon to troubleshoot water quality problems
while water is in storage facilities and distribution systems. Typical
water quality problems that might be encountered include taste and odor,
turbidity, color, and presence of coliforms; see Table 5.1 for the types
of problems commonly encountered, their possible causes, and potential
solutions.

Inspection of Storage Facilities Drinking water practitioners must
incorporate an annual inspection protocol for storage facilities, to
verify the physical condition of the tank and to verify that the tank is
maintained in sanitary condition. Recommended items to be inspected
include the following:

\begin{enumerate}
\item
  Tank lot • Verify that access to tank location is locked, if possible.
  • Verify condition of fence. • Verify that surface water is diverted
  around tank. • Verify that there is no problem with erosion around the
  tank foundation, drain, and overflow. • Verify condition of the tank
  lot upkeep.
\item
  Water quality protection • Exterior and interior steel/concrete are
  watertight. • Vent is shielded and screened against animals and rain.
  • Drain is protected and screened to prevent access by animals and
  surface water. • Overflow is protected and screened to prevent access
  by animals and surface water. • Roof hatch is watertight. • Sidewall
  access is watertight. • Accesses are locked and/or bolted. • All other
  tank openings are curbed, sleeved, and covered to prevent access by
  animals, surface water, or rain.
\item
  Tank operational controls • Tank level indicator is operable and
  accurate. • Water level controls, such as altitude valve and other
  valves, are working properly. • Telemetry system are operational. •
  Tank water level recorder is operational.
\item
  Coating systems and corrosion control (interior and exterior) •
  Inspect and note cracks and peeling of coating systems. • Inspect and
  note location of rust on metal tanks. • Inspect and note pits in tank
  metal. • Inspect and note condition of cathodic protection system.
  Note: A detailed inspection of the coating system should be performed
  by qualified persons in accordance with American Water Works
  Association (AWWA) Standard D101-53. The annual inspection helps to
  determine when a detailed inspection of the coating system is needed.
\end{enumerate}

SUMMARY Whether a community water supply is taken from surface or
groundwater sources, individual community member needs are similar,
whether household or industrial. Consumers need consumable water, wash
water, irrigation water, and waste conveyance water. Consumers may give
little thought to how water arrives at their tap, but the systems
described in this chapter provide potable water to these consumers---an
essential and valuable service, as anyone who has had to transport
potable water during an emergency will tell you. Drinking water
distribution and conveyance are only half the job. The other half of the
job---making sure the water these systems provide is safe---is a matter
of biology (Chapter 6) and disinfection (Chapter 10).

Microbiological Drinking Water Parameters

INTRODUCTION Drinking water practitioners are concerned with water
supply and water purification through a treatment process. In treating
water, the primary concern, of course, is producing potable water that
is safe to drink (free of pathogens) and has no accompanying offensive
characteristics, such as a foul taste or odor. To accomplish this,
drinking water practitioners must possess a wide-ranging reservoir of
knowledge. They quickly learn that water by its very nature is often
very dirty. Microorganisms such as bacteria, algae, fungi, and
viruses---often 100 million per milliliter---can live in water that is
hot or cold, clear or muddy, rapidly flowing or stagnant
(Chapelle,1997). Consequently, to correctly examine raw water for
pathogenic microorganisms and to determine the type of treatment
necessary to ensure that the quality of the end product---potable
water---meets regulatory standards, in addition to accomplishing all the
other myriad requirements involved in drinking water processing, the
drinking water practitioner must be a combination specialist/generalist.
The next three chapters concentrate on the microbiological parameters
(this chapter), physical parameters (Chapter 7), and chemical parameters
(Chapter 8) that drinking water practitioners must know. As a
generalist, the water practitioner requires a great deal of knowledge
and skill to understand the ``big picture,'' so to speak. At the same
time, drinking water practitioners must fine-tune their ability to focus
on a single target within a broad field. A practitioner whose narrowly
focused specialty is not water microbiology must at least have enough
knowledge in biological science to enable full comprehension of the
fundamental factors concerning microorganisms and their relationships to
one another, their effect on the treatment process, and their impact on
the environment, human beings, and other organisms. The drinking water
practitioner as a generalist must understand the importance of
microbiological parameters and what they indicate---the potential of
waterborne disease. Microbiological contaminants are associated with
undesirable tastes and odors and are considered generators of treatment
problems in drinking water technology (algae and fungi, for example),
and they are important enough to the practitioner that knowledge of them
is essential. This chapter provides fundamental knowledge of water
biology for the water practitioner (primarily for the generalist).

MICROBIOLOGY: WHAT IS IT? Biology is generally defined as the study of
living organisms (i.e., the study of life). Microbiology is a branch of
biology that deals with the study of microorganisms so small in size
that they must be studied under a microscope. Microorganisms of interest
to the water and wastewater operator include bacteria, protozoa,
viruses, and algae, among others.

classification of organisms For centuries, scientists classified the
forms of life visible to the naked eye as either animal or plant. Much
of the current knowledge about living things was organized by the
Swedish naturalist Carolus Linnaeus in 1735. The importance of
classifying organisms cannot be overstated, for without a classification
scheme establishing criteria for identifying organisms and arranging
similar organisms into groups would be difficult. Probably the most
important reason for classifying organisms is to make things less
confusing (Wistreich and Lechtman, 1980). Linnaeus was innovative in the
classification of organisms. One of his innovations still with us today
is the binomial system of nomenclature. Under the binomial system, all
organisms are generally described by a two-word scientific name: genus
and species. Genus and species are groups that are part of a hierarchy
of groups of increasing size, based on their nomenclature (taxonomy):
Kingdom Phylum Class Order Family Genus Species

Using this hierarchy and Linnaeus' binomial system of nomenclature, the
scientific name of any organism indicates both the genus and the
species. The genus name is always capitalized, and the species name
begins with a lowercase letter. On occasion, when little chance for
confusion exists, the genus name is abbreviated with a single capital
letter. The names are always in Latin, so they are usually printed in
italics or underlined. Microbe names of interest to the drinking water
practitioner include the following: • Salmonella typhi, the typhoid
bacillus • Escherichia coli, a coliform bacteria • Giardia lamblia, a
protozoa Escherichia coli is commonly known as simply E. coli, and
Giardia lamblia is usually referred to by only its genus name, Giardia.
The water sciences use a simplified system of microorganism
classification that is broken down into the kingdoms of animal, plant,
and protista. As a general rule, the animal and plant kingdoms contain
all of the multicell organisms, and the protists contain all of the
single-cell organisms. Along with microorganism classifications based on
the animal, plant, and protist kingdoms, microorganisms can be further
classified as being eucaryotic or procaryotic (see Table 6.1). A
eucaryotic organism is characterized by a cellular organization that
includes a well-defined nuclear membrane. A procaryotic organism is
characterized by a nucleus that lacks a limiting membrane. To provide
the fundamental knowledge of microbiology that drinking water
practitioners require, this chapter takes a basic but far-reaching,
structured approach. Included is a lengthy and informative discussion of
waterborne protozoa such as Giardia, Cryptosporidium, and others, all of
which have received recent media attention.

WATERBORNE DISEASE Average citizens living in the United States or
Europe have heard of waterborne disease-causing microorganisms, but in
this modern age most people don't give them a second thought, even
though the World Health Organization (WHO) estimates that waterborne
diseases account for 5 million deaths annually, worldwide. Travelers to
various areas of the world often become more familiar with waterborne
diseases or illness-causing microorganisms than they would prefer.
Consider, for example, the traveler who drinks water here and there and
experiences what 17th-century visitors to Paris experienced after
drinking the local water. Martin Lister warned that the local water in
Paris during that time caused a certain ``looseness, and sometimes
dysenteries.'' Quite a few travelers have heard various euphemisms for
their upset stomachs, ranging from ``Montezuma's revenge'' to ``Delhi
belly.'' These people have learned to appreciate the value of clean and
safe drinking water. All the news about waterborne disease is not
necessarily grim, however. Modern sanitation practices have made
contracting most of the waterborne diseases (see Table 6.2) rare in the
United States

and Europe. It would be foolhardy and deadly for us to forget, however,
that in other areas of the world disease-causing organisms are still in
the environment---especially that part of the environment that is water
(Spellman, 1997). The bottom line is that waterborne diseases have not
been eliminated by treatment or even by modern, much improved sanitary
conditions, not even in the industrialized parts of the world. When we
refer to waterborne disease, the uninitiated may get the wrong
impression about water and waterborne disease. In the water environment,
water is not a medium for the growth of pathogenic microorganisms, but
instead is a means of transmission or conveyance (a conduit---hence, use
of the term waterborne) of the pathogen to the place where an individual
inadvertently consumes it, and thus the outbreak of disease begins
(Koren, 1991). The facts are summed up simply enough by the following
(Spellman, 1997): Waterborne pathogens are not at home in water. Nothing
could be further from the truth. A water-filled environment is not one
in which pathogenic organisms would choose to live---that is, if it had
such a choice. The point is that microorganisms do not normally grow,
reproduce, and thrive in watery surroundings. Pathogenic microorganisms
temporarily residing in water are simply biding their time, going with
the flow, waiting for their opportunity to meet up with their
unsuspecting host or hosts. To some degree, when the pathogenic
microorganism finds a host, it is finally home and may have found its
final resting place.

BACTERIA* Of all the microorganisms studied in this text, bacteria are
the most widely distributed, the smallest in size, the simplest in
morphology (structure), and the most difficult to identify and classify.
Because of considerable diversity, even providing a descriptive
definition of a bacterial organism is difficult. About the only
generalization that can be made is that bacteria are single-celled
plants, are procaryotic (the nucleus lacks a limiting membrane), are
seldom photosynthetic, and reproduce by binary fission. Bacteria are
found everywhere in our environment---they are present in the soil, in
the air, and in the water. Bacteria are also present in and on the
bodies of all living creatures, including humans. Most bacteria do not
cause disease; not all of them are pathogenic. Many bacteria perform
useful and necessary functions related to the life of larger organisms.
cause; in fact, ``the form of water pollution that poses the most direct
menace to human health is bacteriological contamination''
(Black-Covilli, 1992). This is partly the reason why bacteria are of
great importance to water specialists. For water treatment personnel
tasked with providing the public with safe, portable water, controlling
and eliminating disease-causing bacteria pose a constant challenge (see
Table 6.3). Attempts to eliminate disease have placed bacteria high on
the list of microorganisms of great interest to the scientific
community. This interest has spawned much work geared toward enhancing
our understanding of bacteria; however, we still have a great deal to
learn about bacteria. We are still principally engaged in making
observations and collecting facts, trying wherever possible to relate
one set of facts to another but still lacking much of a basis for grand
unifying theories. One of the important aspects of bacteria for which we
still lack complete understanding is the infecting dose. Determining,
for example, the number of viable pathogenic cells necessary to produce
infections is difficult. The National Academy of Sciences (NAS, 1977,
1982), for example, reported values varying from 103 to 109 pathogenic
cells per person, with subjects infected representing from 1 to 95\% of
the total subjects tested. Other factors such as age and general health,
as well as previous exposure, are important. Additional significant
influencing factors include the survival of an organism in water, water
temperature, and the presence of colloidal matter in water.

bacterial cells: shapes, forms, sizes, anD arrangements Since the 19th
century, scientists have known that all living things, whether animal or
plant, are made up of cells. The fundamental unit of all living matter,
no matter how complex, is the cell. A typical cell is a single entity,
isolated from other cells by a membrane or cell wall. The cell membrane
contains protoplasm, the living material found within it, and the
nucleus. In a typical mature plant cell, the cell wall is rigid and is
composed of nonliving material, whereas in the typical animal cell the
wall is an elastic living membrane. Cells exist in a very great variety
of sizes and shapes, and their functions also vary widely. The sizes of
cells range from bacteria too small to be seen with a light microscope
to the largest known single cell, the ostrich egg. Microbial cells also
have an extensive size range, some being larger than human cells
(Kordon, 1993). Bacteria come in three shapes: elongated rods called
bacilli, rounded or spherical cells called cocci, and spirals (helical
and curved) called spirilla (rigid cell wall) or spirochetes (flexible
cell wall). Elongated rod-shaped bacteria may vary considerably in
length; they may have square, round, or pointed ends and may be either
motile or nonmotile. The spherical-shaped bacteria may occur singly, in
pairs, in tetrads, in chains, or in irregular masses. The helical and
curved spiral-shaped bacteria exist as slender spirochetes, spirillum,
and bent rods (see Figure 6.1). Bacterial cells are usually measured in
microns (µ) or micrometers (µm); 1 µm = 0.001 or 1/1000 of a millimeter
(mm). A typical rod-shaped coliform bacterial cell is about 2 µm long
and about 0.7 µm wide. The size of a cell changes with time due to
growth and death. Viewed under a microscope, bacterial cells may be seen
as separate (individual) cells or as cells in groupings. Depending on
the species, cells may appear in pairs (diploids), chains, groups of
four (tetrads), cubes (sarcinae), or clumps. Long chains of cocci result
when cells adhere after repeated divisions in one plane; this pattern is
seen in the genera Enterococcus and Lactococcus. In the genus Sarcina,
however, cocci divide in three planes, producing cubical packets of
eight cells (tetrads). The exact shape of rod-shaped cells varies,
especially at the end of the rod. The end of the rod may be flat, cigar
shaped, rounded, or bifurcated. Although many rods do occur singly, they
may remain together after division to form pairs or chains (see Figure
6.1). Frequently, these characteristic arrangements are useful in
bacterial identification.

structure of the bacterial cell The structural form and various
components of the bacterial cell are probably best understood by
referring to the simplified diagram of a rod-form bacterium shown in
Figure 6.2. When studying the figure, keep in mind that cells of
different species may differ greatly, both in structure and chemical
composition; for this reason, no typical bacterium exists. Figure 6.2
shows a generalized bacterium used for the discussion that follows;
however, not all bacteria have all of the features shown in the figure,
and some bacteria have structures not shown in the figure.

Capsules Bacterial capsules (see Figure 6.2) are organized accumulations
of gelatinous material on cell walls, in contrast to slime layers (a
water secretion that adheres loosely to the cell wall and commonly
diffuses into the cell), which are unorganized accumulations of similar
material. Macrocapsules are usually thick enough to be seen under an
ordinary light microscope, but thinner capsules known as microcapsules
can be detected only by electron microscopy (Singleton and Sainsbury,
1994). The production of capsules is determined largely by genetics as
well as environmental conditions and depends on the presence or absence
of capsule-degrading enzymes and other growth factors. Varying in
composition, capsules are mainly composed of water; the organic contents
are made up of complex polysaccharides, nitrogencontaining substances,
and polypeptides. Capsules confer several advantages when bacteria grow
in their normal habitat; for example, they help to (1) prevent
desiccation, (2) resist phagocytosis by host phagocytic cells, (3)
prevent infection by bacteriophages, and (4) aid bacterial attachment to
tissue surfaces in plant and animal hosts or to surfaces of solid
objects in aquatic environments. Capsule formation often correlates with
pathogenicity (Spellman, 2008).

Flagella Many bacteria are motile, and this ability to move
independently is usually attributed to a special structure, the flagella
(singular: flagellum). Depending on species, a cell may have a single
flagellum (see Figure 6.2) (monotrichous bacteria; trichous means
``hair''), one flagellum at each end (amphitrichous bacteria; amphi
means ``on both sides''), a tuft of flagella at one or both ends
(lophotrichous bacteria; lopho means ``tuft''), or flagella that arise
all over the cell surface (peritrichous bacteria; peri means
``around''). A flagellum is a threadlike appendage extending outward
from the plasma membrane and cell wall. Flagella are slender, rigid
locomotor structures, about 20 µm across and up to 15 or 20 µm long.
Flagellation patterns are very useful in identifying bacteria and can be
seen by light microscopy, but only after the flagella have been stained
with special techniques designed to increase their thickness. The
detailed structure of flagella can be seen only in an electron
microscope. Bacterial cells benefit from flagella in several ways.
Flagella can increase the concentration of nutrients or decrease the
concentration of toxic materials near the bacterial surfaces by causing
a change in the flow rate of fluids. They can also disperse flagellated
organisms to areas where colony formation can take place. The main
benefit of flagella to organisms is the improved ability to flee from
areas that might be harmful.

Cell Wall The main structural component of most procaryotes is the rigid
cell wall. Functions of the cell wall include (1) providing protection
for the delicate protoplast from osmotic lysis (bursting); (2)
determining the shape of a cell; (3) acting as a permeability layer that
excludes large molecules and various antibiotics and plays an active
role in regulating the intake of ions by the cell; and (4) providing a
solid support for flagella. Cell walls of different species may differ
greatly in structure, thickness, and composition. The cell wall accounts
for about 20 to 40\% of the dry weight of a bacterium. Gram Stain
Microbial cells are nearly transparent when observed by light microscopy
and thus are difficult to see. The most common method for observing
cells is by the use of stained preparations. Dyes are used to stain
cells, which increases their contrast so they can be more easily
observed in a light microscope. Simple cell-staining techniques depend
on the fact that bacterial cells differ chemically from their
surroundings and thus can be stained to contrast with their environment.
Microbes also differ from one another chemically and physically and
therefore may react differently to a given staining procedure. This is
the basic principle of differential staining---so named because this
type of procedure does not stain all kinds of cells equally. The Gram
staining procedure was developed in the 1880s by Hans Christian Gram, a
Danish bacteriologist. Gram discovered that microbes could be
distinguished from surrounding tissue and observed that some bacterial
cells exhibit an unusual resistance to decolorization. He used this
observation as the basis for a differential staining technique. Gram
differentiation is based on the application of a series of four chemical
reagents: primary stain, mordant, decolorizer, and counterstain. The
purpose of the primary stain, crystal violet, is to impart a blue or
purple color to all organisms regardless of their Gram reaction. This is
followed by the application of Gram's iodine, which acts as a mordant
(fixer) that enhances the union between the crystal violet stain and its
substrate by forming a complex. The decolorizing solution of 95\%
ethanol extracts the complex from certain cells more readily than
others. In the final step, a counterstain (safranin) is applied to
reveal organisms previously decolorized by removal of the complex. Those
organisms retaining the complex are Gram positive (blue or purple),
whereas those losing the complex are Gram negative (red or pink).

Gram-Positive Cell Walls Normally, the thick, homogeneous cell walls of
Grampositive bacteria are composed primarily of a complex polymer, which
often contains linear heteropolysaccharide chains bridged by peptides to
form a three-dimensional netlike structure and envelop the protoplast.
Gram-positive cells usually also contain large amounts of teichoic
acids---typically, substituted polymers or ribitol phosphate and
glycerol phosphate. Amino acids or sugars such as glucose are attached
to the ribitol and glycerol groups. Teichoic acids are negatively
charged and help give the Gram-positive cell wall its negative charge.
Growth conditions can affect the composition of the cell wall; for
example, the availability of phosphates affects the amount of teichoic
acid in the cell wall of Bacillus. Teichoic acids are not present in
Gramnegative bacteria.

Gram-Negative Cell Walls Gram-negative cell walls are much more complex
than Gram-positive walls. The Gram-negative wall is about 20 to 30 µm
thick and has a distinctly layered appearance under the electron
microscope. The thin inner layer consists of peptidoglycan and
constitutes no more than 10\% of the wall weight. In Escherichia coli,
the Gram-negative walls are about 1 µm thick and contain only one or two
layers of peptidoglycan. The outer membrane lies outside the thin
peptidoglycan layer and is essentially a lipoprotein bilayer. The outer
membrane and peptidoglycan are so firmly linked by this lipoprotein that
they can be isolated as one unit.

Plasma Membrane (Cytoplasmic Membrane) Surrounded externally by the cell
wall and composed of a lipoprotein complex, the plasma membrane is the
critical barrier separating the inside from the outside of the cell.
About 7 to 8 nm thick and comprising 10 to 20\% of the dry weight of a
bacterium, the plasma membrane controls the passage of all material into
and out of the cell. The inner and outer faces of the plasma membrane
are embedded with water-loving (hydrophilic) lips, whereas the interior
is hydrophobic. Control of material into the cell is accomplished by
screening, as well as by electric charge. The plasma membrane is the
site of the surface charge of the bacteria. In addition to serving as an
osmotic barrier that passively regulates the passage of material into
and out of the cell, the plasma membrane participates in the active
transport of various substances into the bacterial cell. Inside the
membrane, many highly reactive chemical groups guide the incoming
material to the proper points for further reaction. This active
transport system provides bacteria with certain advantages, including
the ability to maintain a fairly constant intercellular ionic state in
the presence of varying external ionic concentrations. In addition to
participating in the uptake of nutrients, the cell membrane transport
system participates in waste excretion and protein secretions.

Cytoplasm Within a cell and bounded by the cell membrane is a
complicated mixture of substances and structures known as the cytoplasm.
The cytoplasm is a water-based fluid containing ribosomes, ions,
enzymes, nutrients, storage granules (under certain circumstances),
waste products, and various molecules involved in synthesis, energy
metabolism, and cell maintenance.

Mesosome An intracellular structure commonly found in the bacterial
cytoplasm is the mesosome. Mesosomes are invaginations of the plasma
membrane in the shape of tubules, vesicles, or lamellae. Their exact
function is unknown. Many bacteriologists believe that mesosomes are
artifacts generated during the fixation of bacteria for electron
microscopy (Singleton and Sainsbury, 1994).

Nucleoid (Nuclear Body or Region) The nuclear region of the procaryotic
cell is primitive and a striking contrast to that of the eucaryotic
cell. Procaryotic cells lack a distinct nucleus; instead, the function
of the nucleus is carried out by a single, long, double strand of
deoxyribonucleic acid (DNA) that is efficiently packaged to fit within
the nucleoid. The nucleoid is attached to the plasma membrane. A cell
can have more than one nucleoid when cell division occurs after the
genetic material has been duplicated.

Ribosomes The bacterial cytoplasm is often packed with ribosomes, which
are minute, rounded bodies made of ribonucleic acid (RNA) that are
loosely attached to the plasma membrane. Ribosomes are estimated to
account for about 40\% of the dry weight of a bacterium; a single cell
may have as many as 10,000 ribosomes. Ribosomes are the site of protein
synthesis and are part of the translation process; they are commonly
called the ``powerhouses of the cell.'' Inclusions Inclusions (or
storage granules) are often seen within bacterial cells. Some inclusion
bodies are not bound by a membrane and lie free in the cytoplasm. Other
inclusion bodies are enclosed by a single-layer membrane about 2 to 4 µm
thick. Many bacteria produce polymers that are stored as granules in the
cytoplasm.

chemical composition of a bacterial cell Bacteria, in general, are
composed primarily of water (about 80\%) and of dry matter (about 20\%).
The dry matter consists of both organic (90\%) and inorganic (10\%)
components. All basic elements from protoplasm must be derived from the
liquid environment, and if the environment is deficient in vital
elements the cell shows a characteristic lack of development. Note: The
normal growth of a bacterial cell in excess nutrients results in a cell
of definite chemical composition. The growth, however, involves a
coordinated increase in the mass of its constituent parts, not solely an
increase in total mass.

metabolism Metabolism refers to the bacteria's ability to grow in any
environment. The metabolic process refers to the chemical reactions that
occur in living cells. In this process, anabolism works to build up cell
components and catabolism breaks down or changes the cell components
from one form to another. Metabolic reactions require energy, as do
locomotion and the uptake of nutrients. Many bacteria obtain their
energy by processing chemicals from the environment through
chemosynthesis. Other bacteria obtain their energy from sunlight through
photosynthesis.

Chemosynthesis Chemosynthesis is the synthesis of organic substances
such as food nutrients using the energy of chemical reactions. Bacteria
that obtain carbon from carbon dioxide are called autotrophic. Bacteria
that obtain carbon through organic compounds are called heterotrophic
(see Figure 6.3).

Autotrophic Bacteria Organisms that can synthesize organic molecules
necessary for growth from inorganic compounds using light or another
source of energy are autotrophs. For their carbon requirements,
autotrophs are able to use (fix) carbon dioxide to form complex organic
compounds. Heterotrophic Bacteria Most bacteria are not autotrophic.
They cannot use carbon dioxide as a major source of carbon and therefore
must rely upon the presence of more reduced, complex molecules (mostly
derived from other organisms) for their carbon supply. Bacteria that
need complex carbon compounds are heterotrophs. Heterotrophs use a vast
range of carbon sources, including fatty acids, alcohols, sugars, and
other organic substances. Heterotrophic bacteria are widespread in
nature and include all those species that cause disease in humans, other
animals, and plants.

classification Classifying bacteria and other microbes is complicated
because of the enormous variety of microorganisms that differ widely in
their metabolic and structural properties. Some microorganisms are plant
like, others are animal like, and still others are totally different
from all other forms of life. As an example of the classification
process, let's consider microorganisms in terms of their activities.
Bacteria can be classified as aerobic, anaerobic, or facultative.
Aerobic bacteria must have oxygen to live. At the other extreme, that
same oxygen would be toxic to an anaerobe (bacteria that live without
oxygen). Facultative bacteria are capable of growth under aerobic or
anaerobic conditions.

Because bacteria have so many forms, their proper classification or
identification requires applying systematic procedures to grow, isolate,
and identify the individual varieties. These procedures are highly
specialized and technical. Ultimately, bacteria are characterized based
on observation and experience. Fortunately, certain classification
criteria have been established to help in the sorting process
(Singleton, 1992):

\begin{enumerate}
\item
  Shape
\item
  Size and structure
\item
  Chemical activities
\item
  Types of nutrients needed
\item
  Form of energy used
\item
  Physical conditions needed for growth
\item
  Ability to cause disease (pathogenic or nonpathogenic)
\item
  Staining behavior (Gram stain)
\end{enumerate}

Based on these criteria and on observation and experience, it is
possible to identify bacteria from descriptions published in the latest
Bergey's Manual of Determinative Bacteriology.

fecal coliform bacteria: inDicator organisms Fecal coliform are bacteria
that live in the digestive tract of warm-blooded animals. They are
excreted in the solid waste of humans and other mammals. Fecal coliform
generally enter the water in the following ways:

• Improperly treated wastewater from municipal systems, septic systems,
or combined sewer overflows • Runoff from animal stockyards, pastures,
and rangeland • Inadequately captured wastes from human activities such
as construction or camping

Fecal coliforms generally do not pose a danger to people or animals.
Where fecal coliforms are present, however, disease-causing bacteria are
usually also present. Unlike fecal coliforms, disease-causing bacteria
generally do not survive outside the body of animals long enough in the
water to be detected. This makes their direct monitoring difficult.
Drinking water practitioners and public health officials consider the
presence of fecal coliform an indicator of disease bacteria in the
water. The presence of fecal coliforms tends to affect humans more than
it does aquatic creatures, though not exclusively. Bacteria associated
with fecal coliforms can cause significant disease in humans, such as
diarrhea, dysentery, cholera, and typhoid fever. Some of these bacteria
can also cause infection in open wounds. Untreated fecal material that
contains fecal coliforms adds excess organic material to the water; the
decay of this material depletes oxygen in the water, which may kill fish
and other aquatic life. Reduction of fecal coliforms in wastewater may
require the use of chlorine and other disinfectant chemicals; such
materials may kill bacteria essential to the proper balance of the
aquatic environment, endangering the survival of species dependent on
those bacteria. Higher levels of fecal coliform require higher levels of
chlorine, further threatening those aquatic organisms.

Total Coliform Rule The Total Coliform Rule (40 CFR 141.21) is the part
of the Safe Drinking Water Act that addresses detection and removal of
bacterial contamination in drinking water. The Total Coliform Rule
applies to every public water system. Each public water system must take
at least one coliform sample every year and submit the results of that
sample to the state Drinking Water Program for compliance purposes.

Fecal Coliform Testing Procedures Extensive research has been conducted
and is ongoing in an attempt to compare the presence and the
significance of specific organisms in water to the traditional coliform
group and waterborne diseases, with the goal of pinpointing the best
indicator of contamination in water. The search continues for a quick,
economic, reliable determination for possible use in routine
examination, or at least during outbreaks of the waterborne diseases.
Federal regulations cite two approved methods for the determination of
fecal coliform in water: (1) the multiple-tube fermentation (MTF)
technique for members of the coliform group, and (2) the membrane
filtration (MF) technique for members of the coliform group. The
multiple-tube method is still used in many labs, because the membrane
filtration method is not applicable to turbid samples; however, the
membrane filtration method takes less time and provides a more direct
count of the coliforms than the multiple-tube method does. It also
requires less laboratory equipment. The bottom line? Drinking water
practitioners need to understand the essential differences between these
two tests, but before those differences are explained it is necessary to
lay some groundwork by explaining testing preparations. Note: Both
methods are still recognized as sufficiently reliable with relatively
simple techniques and economic equipment to be run as often as required
by the monitoring activity of water quality control. We discuss each of
these procedures briefly in the following sections; however, do not
attempt to perform the fecal coliform test using the summary information
provided within this text. Instead, refer to the appropriate references
cited in the Federal Register or the current Standard Methods and
Procedures for a complete discussion of these procedures. Note: Because
the MF procedure can yield low or highly variable results for
chlorinated water, the U.S. Environmental Protection Agency (USEPA)
requires verification of results using the most probable number (MPN)
procedure to resolve any controversies.

Testing Preparations Whenever microbiological testing of water samples
is performed, certain general considerations and techniques are
required. Because these considerations are similar for each test
procedure, we review them here prior to our specific discussion of the
two test methods:

• Reagents and media---All reagents and media used in performing
microbiological tests on water samples must meet the standards specified
in references cited in federal regulations. • Reagent-grade
water---Deionized water that is tested annually and found to be free of
dissolved metals and bactericidal or inhibitory compounds is preferred
for use in preparing culture media and test reagents, although distilled
water may be used. • Chemicals---All chemicals used in fecal coliform
monitoring must be American Chemical Society (ACS) reagent grade or
equivalent. • Media---To ensure uniformity in the test procedures the
use of dehydrated media is recommended. Sterilized prepared media in
sealed test tubes, ampules, or dehydrated media pads are also acceptable
for use in these tests. • Glassware and disposable supplies---All
glassware, equipment, and supplies used in microbiological testing
should meet the standards specified in references cited in federal
regulations.

All glassware used for bacteriological testing must be thoroughly
cleaned using a suitable detergent and hot water. The glassware should
be rinsed with hot water to remove all traces of detergent residue and
finally rinsed with distilled water. Laboratories should use a detergent
certified to meet bacteriological standards or at a minimum should rinse
all glassware after washing with two tap water rinses followed by five
distilled-water rinses. For sterilization of equipment, a hot-air
sterilizer or autoclave can be used. When using the hot-air sterilizer,
all equipment should be wrapped in high-quality (Kraft) paper or placed
in containers prior to hot-air sterilization. All glassware (except
those in metal containers) should be sterilized for a minimum of 60
minutes at 170°C. Sterilization of glassware in metal containers
requires a minimum of 2 hours. Hot-air sterilization cannot be used for
liquids. Sample bottles, dilution water, culture media, and glassware
may also be sterilized by autoclaving at 121°C for 15 minutes. Dilution
water used for making sample serial dilutions is prepared by adding 1.25
mL of stock buffer solution and 5.0 mL of magnesium chloride solution to
1000 mL of distilled or deionized water. The stock solutions of each
chemical should be prepared as outlined in the references cited in
federal regulations. The dilution water is then dispensed in sufficient
quantities to produce 9 or 99 mL in each dilution bottle following
sterilization. If the membrane filtration procedure is used, additional
60to 100-mL portions of dilution water should be prepared and sterilized
to provide the rinse water required by the procedure. At times, the
density of the organisms in a sample makes accurately determining the
actual number of organisms in the sample difficult. When this occurs,
the sample size may need to be reduced to as little as 1 millionth of a
milliliter. To obtain such small volumes, a technique known as serial
dilutions is used.

Multiple Tube Fermentation Technique The multiple-tube fermentation
(MTF) technique for fecal coliform testing of water, solid, or semisolid
samples is based on the fact that coliform organisms can use lactose
(the sugar occurring in milk) as food and produce gas in the process. In
waste testing, the procedure normally consists of presumptive,
confirming, and completed phases. It is recognized as the method of
choice for any samples that may be controversial (i.e., enforcement
related). When multiple tubes are used in the fermentation technique,
results of the examination of replicate tubes and dilutions are reported
in terms of the most probable number (MPN) of organisms present. This
number, based on certain probability formulas, is an estimate of the
mean density of coliforms in the sample. Coliform density provides the
best assessment of water treatment effectiveness and the sanitary
quality of untreated water. The production of gas in a single
fermentation tube may indicate the presence of coliforms, but it gives
no indication as to the concentration of bacteria in the sample; that
is, a coliform count cannot be obtained directly. The gas bubble created
could be the result of one bacterium or millions. The precision of each
test depends on the number of tubes used. The most satisfactory
information will be obtained when the largest sample inoculum shows no
gas at all in a majority of the tubes. Bacterial density can be
estimated mathematically or derived from a table using the number of
positive tubes in multiple dilutions. The number of sample portions
selected is governed by the desired precision of the result. MPN tables
are based on the assumption of a Poisson (random) distribution; however,
if the sample is not adequately shaken before the portions are removed
or if clumping of bacterial cells occurs, the MPN value will
underestimate the actual bacterial density. The technique utilizes a
two-step incubation procedure. The sample dilutions are first incubated
in lauryl (sulfonate) tryptose broth for 24 to 48 hours (presumptive
phase). Positive samples are then transferred to EC broth and incubated
for an additional 24 hours (confirming phase). Positive samples from
this second incubation are used to statistically determine the MPN from
the appropriate reference chart. A single-media, 24-hour procedure is
also acceptable. In this procedure, sample dilutions are inoculated in
A-1 media and incubated for 3 hours at 35°C, then incubated for 21 hours
at 44.5°C. Positive samples from these inoculations are then used to
statistically determine the MPN value from the appropriate chart.

Fecal Coliform MPN Presumptive Test Procedure 1. Prepare dilutions and
inoculate five fermentation tubes for each dilution. 2. Cap all tubes
and transfer to incubator. 3. Incubate 24 ± 2 hr at 35 ± 0.5°C. 4.
Examine tubes for gas: Gas present (positive test)---transfer. No
gas---continue incubation. Incubate for a total time of 48 ± 3 hr at 35
± 0.5°C. 5. Examine tubes for gas: Gas present (positive
test)---transfer. No gas (negative test). Note: Keep in mind that the
fecal coliform MPN confirming procedure and the fecal coliform procedure
using the A-1 broth test are used to determine the most probable number
per 100 mL. The MPN procedure for fecal coliform determination requires
a minimum of three dilutions with five tubes per dilution.

Calculation of Most Probable Number per 100 mL Most probable number test
calculations require the selection of a valid series of three
consecutive dilutions. The number of positive tubes in each of the three
selected dilution inoculations is used to determine the MPN/100 mL. When
selecting the dilution inoculations to be used in the calculation, each
dilution is expressed as a ratio of positive tubes per tubes inoculated
in the dilution (e.g., three positive per five inoculated, or 3/5).
Several rules must be followed in determining the most valid series of
dilutions. In the following examples, four dilutions were used for the
test.

\begin{enumerate}

\item
  Using the confirming test data, select the highest dilution showing
  all positive results (no lower dilution showing less than all
  positive) and the next two higher dilutions.
\item
  If a series shows all negative values (with the exception of one
  dilution), select the series that places the only positive dilution in
  the middle of the selected series.
\item
  If a series shows a positive result in a dilution higher than the
  selected series (using rule 1), it should be incorporated into the
  highest dilution of the selected series.
\end{enumerate}

After selecting the valid series, the MPN/100 mL is determined by
locating the selected series on the MPN reference chart. If the selected
dilution series matches the dilution series of the reference chart, the
MPN value from the chart is the reported value for the test. If the
dilution series used for the test does not match the dilution series of
the chart, the test result must be calculated. ? EXAMPLE 6.1

Solution: 1. Select the highest dilution (tube with the lowest amount of
sample) with all positive tubes (1.0-mL dilution). Select the next two
higher dilutions (0.1 mL and 0.01 mL). In this case, the selected series
will be 5--3--1. 2. Include any positive results in dilutions higher
than the selected series (0.001-mL dilution, 1/5). This changes the
selected series to 5--3--2. 3. Using the Sample Volume columns in Table
6.4, locate this series (5--3--2). 4. Read the MPN value from the fourth
column (140).

\begin{enumerate}

\item
  Note that, in Table 6.4, the dilution series begins with 10 mL. For
  this test, the series begins with 1.0 mL.
\end{enumerate}

MPN/100 mL ? 140 MPN/100 mL ? 10 mL ? 1400 MPN/100 mL 1 mL Membrane
Filtration Technique The membrane filtration (MF) technique is highly
reproducible, can be used to test relatively large volumes of sample,
and yields numerical results more rapidly than the multiple-tube
procedure. MF is extremely useful in monitoring drinking water and a
variety of natural waters; however, the technique does have limitations,
particularly when testing waters with high turbidity or non-coliform
(background) bacteria. For such waters, or when the MF technique has not
been used previously, conducting parallel tests with the MTF technique
to demonstrate applicability and comparability is desirable. Note: As
related to the MF technique, the coliform group may be defined as being
comprised of aerobic and many facultative anaerobic, Gram-negative,
non-spore-forming, rod-shaped bacteria that develop red colonies with a
metallic sheen within 24 hours at 35°C on an Endo-type medium. Some
members may produce dark red, mucoid, or nucleated colonies without a
metallic sheen. When verified these are classified as atypical coliform
colonies. When purified cultures of coliform bacteria are tested, they
produce a negative cytochrome oxidase (CO) and a positive ONPG reaction
for ?-galactosidase. Generally, all red, pink, blue, white, or colorless
colonies lacking sheen are considered noncoliforms by this technique.

The membrane filtration technique uses a specially designed filter pad
with uniformly sized pores (openings) small enough to prevent bacteria
from entering the filter. A measured volume of sample is drawn through
the filter pad by applying a partial vacuum. The special pad retains the
bacteria on its surface, while allowing the water to pass through. Note:
In the MF method, the number of coliforms is estimated by the number of
colonies grown. In order to use the MF procedure for chlorinated
effluents, you must be able to demonstrate that it gives information
comparable to that obtained by the MTF test. The MF procedure uses an
enriched lactose medium and incubation temperature of 44.5 ± 0.2°C for
selectivity and gives 93\% accuracy in differentiating between coliforms
found in the feces of warm-blooded animals and those from other
environmental sources. Because incubation temperature is critical,
submerge waterproofed (e.g., plastic bag enclosures) MF cultures in a
water bath for incubation at the elevated temperature, or use an
appropriate, accurate solid heat sink incubator. Alternatively, use an
equivalent incubator that will hold the 44.5°C temperature within 0.2°C
throughout the chamber over a 24-hour period while located in an
environment of ambient air temperatures ranging from 5 to 35°C.

Materials and Culture Medium 1. m-FC media---The need for uniformity
dictates the use of dehydrated media. Never prepare media from basic
ingredients when suitable dehydrated media are available. Follow
manufacturer's directions for rehydration. Commercially prepared media
in liquid form (sterile ampule or other) may also be used if known to
give equivalent results. 2. Culture dishes---Use tight-fitting plastic
dishes because the MF cultures are submerged in a water bath during
incubation. Enclose groups of fecal coliform cultures in plastic bags or
seal individual dishes with waterproof (freezer) tape to prevent leakage
during submersion. 3. Incubator---The specificity of the fecal coliform
test is related directly to the incubation temperature. Static air
incubation may be a problem in some types of incubators because of
potential heat layering within the chamber and the slow recovery of
temperature each time the incubator is opened during daily operations.
To meet the need for greater temperature control, use a water bath, a
heat-sink incubator, or a properly designed and constructed incubator
giving equivalent results. A temperature tolerance of 44.5 ± 0.2°C can
be obtained with most types of water baths equipped with a gable top for
the reduction of heat and water losses. A circulating water bath is
excellent but may not be essential to this test, if the maximum
permissible variation of 0.2°C in temperature can be maintained with
other equipment. Procedure 1. Sample filtration a. Select a filter and
aseptically separate it from the sterile package. b. Place the filter on
the support plate with the grid side up. c.~Place the funnel assembly on
the support; secure as needed. d.~Pour 100 mL of sample or serial
dilution onto the filter; apply vacuum. Note: The appropriate sample
size and serial dilution should result in the growth of 20 to 60 fecal
coliform colonies on at least one filter. The selected dilutions must
also be capable of showing permit exclusions. e. Allow all of the liquid
to pass through the filter. f.~Rinse the funnel and filter with three
portions (20 to 30 mL) of sterile, buffered dilution water. (Allow each
portion to pass through the filter before the next addition.) g. Remove
the filter funnel and aseptically transfer the filter, grid side up,
onto the prepared media. Note: Filtration units should be sterile at the
start of each filtration series and should be sterilized again if the
series is interrupted for 30 minutes or more. A rapid interim
sterilization can be accomplished by 2 minutes' exposure to ultraviolet
(UV) light, flowing steam, or boiling water. 2. Incubation a. Place
absorbent pad into culture dish using sterile forceps. b. Add 1.8 to 2.0
mL m-FC media to the absorbent pad.

\begin{enumerate}

\item
  Discard any media not absorbed by the pad.
\item
  Filter sample through sterile filter.
\item
  Remove filter from assembly and place on absorbent pad (grid up).
\item
  Cover culture dish.
\item
  Seal culture dishes in a weighted plastic bag.
\item
  Incubate filters in a water bath for 24 hours at 44.5 ± 0.2°C.
\end{enumerate}

Colony Counting Upon completion of the incubation period, the surface of
the filter will have growths of both fecal coliform and non-fecal
coliform bacteria colonies. The fecal coliform will appear blue in
color, whereas the non-fecal coliform colonies will appear gray or cream
colored. To count the colonies, the entire surface of the filter should
be scanned using a 10× to 15× binocular, wide-field dissecting
microscope. The desired range of colonies for the most valid fecal
coliform determination is 20 to 60 colonies per filter. If multiple
sample dilutions are used for the test, counts for each filter should be
recorded on the laboratory data sheet.

• Too many colonies---Filters that show a growth over the entire surface
of the filter with no individually identifiable colonies should be
recorded as ``confluent growth.'' Filters that show a very high number
of colonies (greater than 200) should be recorded as ``too numerous to
count'' (TNTC). • Not enough colonies---If no single filter meets the
desired minimum colony count (20 colonies), the sum of the individual
filter counts and the respective sample volumes can be used in the
formula to calculate the colonies per 100 mL.

Note: In each of these cases, adjustments in sample dilution volumes
should be made to ensure that future tests meet the criteria for
obtaining a valid test result.

Calculation The fecal coliform density can be calculated using the
following formula:

Colonies/100 mL ? Colonies counted ? 100 mL (6.2) Sample volume (mL)

Solution: 1. Influent sample---Select the influent sample filter that
has a colony count in the desired range (20 to 60). Because one filter
meets this criterion, the remaining influent filters that did not meet
the criterion are discarded. Colonies/100 mL ? 48 colonies ? 100 mL ?
48,000 colonies/100 mL 0.1 mL 2. Effluent sample---Because none of the
filters for the effluent sample meets the minimum test requirement, the
colonies per 100 mL must be determined by totaling the colonies on each
filter and the sample volumes used for each filter. Total colonies ? 10
? 5 ? 3 ? 18 colonies Total sample ? 10.0 mL ? 1.0 mL ? 0.1 mL ? 11.1 mL
Colonies/100 mL ? 18 colonies ? 100 ? 162 colonies/100 mL 11.1 mL

Note: The USEPA criterion for fecal coliform bacteria in bathing waters
is a logarithmic mean of 200 per 100 mL, based on a minimum of 5 samples
taken over a 30-day period, with not more than 10\% of the total samples
exceeding 400 per 100 mL. Because shellfish may be eaten without being
cooked, the strictest coliform criterion applies to shellfish
cultivation and harvesting. The USEPA criterion states that the mean
fecal coliform concentration should not exceed 14 per 100 mL, with not
more than 10\% of the samples exceeding 43 per 100 mL.

Interferences Large amounts of turbidity, algae, or suspended solids may
interfere with this technique by blocking the filtration of the sample
through the membrane filter. Dilution of these samples to prevent this
problem may make the test inappropriate for samples with low fecal
coliform densities, as the sample volumes after dilution may be too
small to give representative results. The presence of large amounts of
non-coliformgroup bacteria in the samples may also prohibit the use of
this method. Geometric Mean Calculation Many National Pollutant
Discharge Elimination System (NPDES) discharge permits require fecal
coliform testing. Results for fecal coliform testing must be reported as
a geometric mean (average) of all the test results obtained during a
reporting period. A geometric mean, unlike an arithmetic mean or
average, dampens the effect of very high or low values that otherwise
might cause a non-representative result. Note: Current regulatory
requirements prohibit the reporting of no MPN or colonies. If the test
result does not produce any positive results or colonies, it must be
reported as \textless1 (less than 1). In cases where test results are
reported as 0 or \textless1, a value of 1 should be used to calculate
the geometric mean. This substitution does not affect the result of the
calculation; it just ensures that the data are entered into the
calculation in a usable form.

Calculation of the geometric mean can be performed by one of two
methods. Both methods require a calculator capable of performing more
advanced calculations. The first method requires a calculator that is
capable of determining the nth root of a number (n = the number of
values used in the calculation). The general formula for this method of
calculating the geometric mean is

Geometric mean ? This equation states that the geometric mean can be
found by multiplying all of the data points for a given reporting period
together and taking the nth root of this product.

? EXAMPLE 6.3 Problem: Given the data in the chart below, determine the
geometric mean using the nth root method. Week 1 5 colonies/100 mL Week
2 7 colonies/100 mL Week 3 90 colonies/100 mL Week 4 1000 colonies/100
mL Solution:

Geometric mean ? ? ? 42 colonies/100 mL

The second method for calculation of the geometric mean requires a
calculator that can compute logarithms (log) and antilogarithms
(antilog): Procedure 1. If there are any reported values of 0, replace
them with \textless1. 2. Using the calculator, determine the logarithm
of each test result. 3. Add the logarithms of all of the test results.
4. Divide the sum by the number of test results (n). 5. Enter this
number into the calculator. 6. Press ``2nd'' or ``INV'' then ``LOG.'' 7.
See the geometric mean displayed on the calculator.

? EXAMPLE 6.4 Problem: Given the data below, determine the geometric
mean. Week 1 12 MPN per 100 mL Week 2 28 MPN per 100 mL

Week 3 37 MPN per 100 mL Week 4 25 MPN per 100 mL VIRUSES Viruses are
parasitic particles---the smallest living infectious agents known.
Because they are parasitic entities, they are not cellular (have no
nucleus, cell membrane, or cell wall), they are unable to multiply or
adapt to the hostile water environment, and they lack a living host.
They multiply only within living cells (hosts) and are totally inert
outside of living cells, but they still can survive in the environment.
Viruses differ from living cells in at least three ways: (1) they are
unable to reproduce independently of cells and cannot carry out cell
division; (2) they possess only one type of nucleic acid, either DNA or
RNA; and (3) they have a simple acellular organization (Prescott et al.,
1993). Viruses can infect humans (it only takes a single virus particle
to infect a host) through recently contaminated drinking water, the
relativity of time being connected with the survival ability of the
virus in natural and manmade hostile environments. At present, over 100
virus types are known to occur in human feces, and an infected person
may excrete as many as 106 infectious particles in 1 gram of feces;
thus, the potential for contamination is very great. Of those contacting
viruses, only a small percentage are infected, and of those infected
only about 2\% may become recognizably ill.~Assuming a 1\% infection
rate and a 2\% illness rate, this means that 1 in every 5000 persons
coming in contact with a virus becomes ill, a very high rate if water is
contaminated with fecal matter (Tchobanoglous and Schroeder, 1987).
Isolation of viruses has improved considerably over the past several
decades. They can be controlled by chlorination, but at much higher
levels than are necessary to kill bacteria. The viruses examined by the
drinking water practitioner are practically limited to enteric viruses
(infections of the intestinal tract). Some viruses that may be
transmitted by water include infectious hepatitis, adenovirus, polio,
coxsackie, echoviruses, and Norwalk agent. A virus that infects a
bacterium is called a bacteriophage

bacteriophage Lewis Thomas (1974) observed that when humans ``catch
diphtheria it is a virus infection, but not of us.'' In other words,
what humans have really caught is a bacterium that is infected by the
virus---humans simply ``blundered into someone else's accident.'' The
toxin of diphtheria bacilli is produced by organisms that have been
infected with a bacteriophage (phage), which is any viral organism whose
host is a bacterium. Most bacteriophage research has been carried out on
the bacterium Escherichia coli, which is one of the Gram-negative
bacteria that water specialists are concerned with because it is a
typical coliform.

inDicator Viruses Considerable research has been accomplished in the
last several decades in an attempt to determine certain viruses as
indicator viruses. In Volume 1 of Drinking Water and Health, the
National Academy of Sciences (NAS, 1977) reached the following
conclusions:

\begin{enumerate}

\item
  The presence of infecting viruses in drinking water is a potential
  hazard to public health, and no valid basis exists upon which a
  no-effect concentration of viral contamination in finished drinking
  water might be established. Note: This statement in no way should be
  interpreted to mean that viruses are not removed by conventional
  treatment and disinfection. On the contrary, drinking water produced
  by an effective conventional treatment and distributed after
  disinfection is expected to have significantly reduced concentrations
  of viruses inactivated by the treatment.
\item
  Continued testing for viral contamination of potable water should be
  carried out with the facilities and skills of a wide variety of
  research establishments, both inside and outside of government, and
  methodology for virus testing should be improved.
\item
  The bacteriological monitoring methods currently prescribed or
  recommended in this report (coliform count and standard plate count)
  are the best indicators available for routine use in evaluating the
  presence in water of intestinal pathogens, including viruses.
\end{enumerate}

In 1987, the USEPA concluded that measuring the level of enteric viruses
in drinking water is not economically or technologically feasible
because currently acceptable methods require levels of expertise that
utility personnel normally do not possess, and the methods would be too
expensive if analyzed by private laboratories.

Validation procedures have not yet been established. Continuous
monitoring would be required, but monitoring does not provide advance
notice to ensure the safety of drinking water at the consumer's tap
(DeZuane, 1997).

PROTOZOA The protozoa (``first animals'') are a large group (more than
50,000 known species) of eucaryotic organisms that have adapted a form
or cell to serve as the entire body. In fact, all protozoa are
single-celled organisms. Typically, they lack cell walls but have a
plasma membrane that is used to take in food and discharge waste. They
can exist as solitary or independent organisms (e.g., stalked ciliates
such as Vorticella), or they can colonize (e.g., the sedentary
Carchesium). Protozoa are microscopic and get their name because they
employ the same type of feeding strategy as animals. Most are harmless,
but some are parasitic. Some forms have two life stages: active
trophozoites (capable of feeding) and dormant cysts. As unicellular
eucaryotes, protozoa cannot be easily defined because they are diverse
and in most cases only distantly related to each other (Patterson and
Hedley, 1992). Each protozoa is a complete organism and contains the
facilities for performing the body functions for which vertebrates have
many organ systems. Certain types of protozoa can cause disease. Of
particular interest to the drinking water practitioner are Entamoeba
histolytica (ameobic dysentery and ameobic hepatitis), Giardia lamblia
(giardiasis), Cryptosporidium (cryptosporidiosis), and Cyclospora
(cyclosporosis). Sewage contamination transports eggs, cysts, and
oocysts of parasitic protozoa and helminths (tapeworms, hookworms, etc.)
into raw supplies, leaving water treatment and disinfection as the means
by which to diminish the risk of contaminated water for the consumer. To
prevent the occurrence of Giardia and Cryptosporidium in surface water
supplies and to address increasing problems with waterborne diseases,
the USEPA implemented its Surface Water Treatment Rule (SWTR) in 1989.
The rule requires both filtration and disinfection of all surface water
supplies as a means of primarily

controlling Giardia and enteric viruses. Since implementation of the
SWTR, the USEPA has also recognized that Cryptosporidium is an agent of
waterborne disease (Badenock, 1990). In its next series of surface water
regulations (in 1996), the USEPA included Cryptosporidium. To test the
need for and effectiveness of the Surface Water Treatment Rule, a study
was conducted on the occurrence and distribution of Giardia and
Cryptosporidium organisms in raw water supplies to 66 surface water
filter plants (LeChevallier et al., 1991). These plants were located in
14 states and a Canadian province. A combined immunofluorescence test
indicated that cysts and oocysts were widely dispersed in the aquatic
environment. Giardia was detected in more than 80\% of the samples.
Cryptosporidium was found in 85\% of the sample locations. Taking into
account several variables, Giardia or Cryptosporidium were detected in
97\% of the raw water samples. After evaluating their data, the
researchers came to the conclusion that the Surface Water Treatment Rule
may have to be upgraded (subsequently, it has been) to require
additional treatment (Spellman, 2008).

Giardia Giardia lamblia (also known as the hiker's/traveler's scourge or
disease) is a microscopic parasite that can infect warm-blooded animals
and humans. Although Giardia was discovered in the 19th century, not
until 1981 did the World Health Organization (WHO) classify Giardia as a
pathogen. An outer shell called a cyst allows Giardia to survive outside
the body for long periods of time. If viable cysts are ingested, Giardia
can cause the illness known as giardiasis, an intestinal illness that
can cause nausea, anorexia, fever, and severe diarrhea. The symptoms
last only for several days, and the body can naturally rid itself of the
parasite in 1 to 2 months; however, for individuals with weakened immune
systems, the body often cannot rid itself of the parasite without
medical treatment (CDC, 2015). In the United States, Giardia is the most
commonly identified pathogen in waterborne disease outbreaks.
Contamination of a water supply by Giardia can occur in two ways: (1) by
the activity of animals in the watershed area of the water supply, or
(2) by the introduction of sewage into the water supply. Wild and
domestic animals are major contributors to the contamination of water
supplies. Studies have also shown that, unlike many other pathogens,
Giardia is not host specific. In short, Giardia cysts excreted by
animals can infect and cause illness in humans. Additionally, in several
major outbreaks of waterborne diseases, the Giardia cyst source was
sewage-contaminated water supplies. Treating the water supply, however,
can effectively control waterborne Giardia. Chlorine and ozone are
examples of two disinfectants known to effectively kill Giardia cysts.
Filtration of the water can also effectively trap and remove the
parasite from the water supply. The combination of disinfection and
filtration is the most effective water treatment process available today
for prevention of Giardia contamination. In drinking water, Giardia is
regulated under the Surface Water Treatment Rule. Although the SWTR does
not establish a maximum contaminant level (MCL) for Giardia, it does
specify treatment requirements to achieve at least 99.9\% (3-log)
removal or inactivation of Giardia. This regulation requires that all
drinking water systems using surface water or groundwater under the
influence of surface water must disinfect and filter the water. The
Enhanced Surface Water Treatment Rule (ESWTR), which includes
Cryptosporidium and further regulates Giardia, was established in 1996.

Giardiasis Giardiasis is recognized as one of the most frequently
occurring waterborne diseases in the United States. Giardia lamblia
cysts have been discovered in places as far apart as Estes Park,
Colorado (near the Continental Divide); Missoula, Montana; Wilkes-Barre,
Scranton, and Hazleton, Pennsylvania; and Pittsfield and Lawrence,
Massachusetts, just to name a few. Giardiasis is characterized by
intestinal symptoms that usually last a week or more and may be
accompanied by one or more of the following: diarrhea, abdominal cramps,
bloating, flatulence, fatigue, and weight loss. Although vomiting and
fever are commonly listed as relatively frequent symptoms, people
involved in waterborne outbreaks in the United States have not commonly
reported them. Most Giardia infections persist only for 1 or 2 months,
but some people experience a more chronic phase that can follow the
acute phase or may become manifest without an antecedent acute illness.
Loose stools and increased abdominal gassiness with cramping,
flatulence, and burping characterize the chronic phase. Fever is not
common, but malaise, fatigue, and depression may ensue; for a small
number of people, the persistence of infection is associated with the
development of marked malabsorption and weight loss (Weller, 1985).
Similarly, lactose (milk) intolerance can be a problem for some people.
This can develop coincidentally with the infection or be aggravated by
it, causing an increase in intestinal symptoms after ingestion of milk
products. Some people may have several of these symptoms without
evidence of diarrhea or have only sporadic episodes of diarrhea every
three or four days. Still others may have no symptoms at all. The
problem, then, may not be one of determining whether or not someone is
infected with the parasite but how harmoniously the host and the
parasite can live together. When such harmony does not exist or is lost,
it then becomes a problem of how to get rid of the parasite, either
spontaneously or by treatment.

Three prescription drugs available in the United States to treat
giardiasis are quinacrine, metronidazole, and furazolidone. In a paper
comparing the effectiveness of these drugs in treating giardiasis,
quinacrine was reported to have an efficacy of 90\% or greater;
metronidazole has an efficacy rate of 60 to 100\%; and furazolidone has
an efficacy rate of 80 to 96\% (Gardner and Hill, 2001). Quinacrine is
generally the least expensive of the anti-Giardia medications, but it
often causes vomiting in children younger than 5 years old. Although the
treatment of giardiasis is not an FDA-approved indication for
metronidazole, the drug is commonly used for this purpose. Furazolidone
is considered to be the least effective of the three drugs but is the
only anti-Giardia medication that comes in liquid preparation, which
makes it easier to deliver the exact dose to small children who have
difficulty taking pills. Giardiasis occurs worldwide. In the United
States, Giardia is the parasite most commonly identified in stool
specimens submitted to state laboratories for parasitologic examination.
During a 3-year period, approximately 4\% of 1 million stool specimens
submitted to state laboratories tested positive for Giardia (CDC, 1979).
Other surveys have demonstrated Giardia prevalence rates ranging from 1
to 20\%, depending on the location and ages of persons studied.
Giardiasis ranks among the top 20 infectious diseases that cause the
greatest morbidity in Africa, Asia, and Latin America; about 200 million
people in these regions have symptomatic infections (Feng and Xiao,
2011). People who are at highest risk for acquiring Giardia infection in
the United States may be placed into five major categories:

\begin{enumerate}

\item
  People in cities whose drinking water originates from streams or
  rivers and whose water treatment process does not include filtration,
  or where filtration is ineffective because of malfunctioning equipment
\item
  Hikers, campers, and those who enjoy the outdoors
\item
  International travelers
\item
  Children who attend daycare centers, daycare center staff, and parents
  and siblings of children infected in daycare centers
\item
  Homosexual men
\end{enumerate}

People in categories 1, 2, and 3 have in common the same general source
of infection; that is, they acquire Giardia from fecally contaminated
drinking water. City residents usually become infected because the
municipal water treatment process does not include the filter necessary
to physically remove the parasite from the water. The number of people
in the United States at risk (i.e., the number who receive municipal
drinking water from unfiltered surface water) is estimated to be 20
million. International travelers may also acquire the parasite from
improperly treated municipal waters in cities or villages in other parts
of the world, particularly in developing countries. In Eurasia, only
travelers to Leningrad appear to be at increased risk. In prospective
studies, 88\% of U.S. and 35\% of Finnish travelers to Leningrad who had
negative stool tests for Giardia on departure to the Soviet Union
developed symptoms of giardiasis and had positive tests for Giardia
after they returned home (Brodsky et al., 1974). With the exception of
visitors to Leningrad, however, Giardia has not been implicated as a
major cause of traveler's diarrhea, as it has been detected in fewer
than 2\% of travelers who develop diarrhea. However, hikers and campers
risk infection every time they drink untreated raw water from a stream
or river. Persons in categories 4 and 5 become exposed through more
direct contact with feces or an infected person---exposure to the soiled
diapers of an infected child in cases associated with daycare centers or
through direct or indirect anal--oral sexual practices in the case of
homosexual men. Although community waterborne outbreaks of giardiasis
have received the greatest publicity in the United States during the
past decade, about half of the Giardia cases discussed with staff of the
Centers for Disease Control (CDC) over a 3-year period had a daycare
center exposure as the most likely source of infection. Numerous
outbreaks of Giardia in daycare centers have been reported. Infection
rates for children in daycare center outbreaks have ranged from 21 to
44\% in the United States and from 8 to 27\% in Canada (Black et al.,
1981; Keystone et al., 1978, 1984; Pickering et al., 1981, 1984; Sealy
and Schuman, 1983). The highest infection rates are usually observed in
children who wear diapers (1 to 3 years of age). In a CDC study of 18
randomly selected daycare centers in Atlanta, 10\% of diapered children
were found to be infected. Transmission from this age group to older
children, daycare staff, and household contacts is also common. About
20\% of parents caring for an infected child become infected. Local
health officials and managers of water utility companies need to realize
that sources of Giardia infection other than municipal drinking water
exist. Armed with this knowledge, they are less likely to make a quick
(and sometimes wrong) assumption that a cluster of recently diagnosed
cases in a city is related to municipal drinking water. Of course,
drinking water must not be ruled out as a source of infection when a
larger than expected number of cases is recognized in a community, but
the possibility that the cases are associated with a daycare center
outbreak, drinking untreated stream water, or international travel
should also be entertained. To understand the finer aspects of Giardia
transmission and strategies for control, drinking water practitioners
must become familiar with several aspects of the parasite's biology. Two
forms of the parasite exist: a trophozoite and a cyst, both of which are
much larger than bacteria (Figure 6.4). Trophozoites live in the upper
small intestine, where they attach to the intestinal wall by means of a
disc-shaped suction pad on their ventral surface. Trophozoites actively
feed and reproduce at this location. At some time during the
trophozoite's life, it releases its hold on the bowel wall and floats in
the fecal stream through the intestine. As it makes this journey, it
undergoes a morphologic transformation into an egg-like structure called
a cyst. The cyst (about 6 to 9 nm in diameter and 8 to 12 µm in length)
has a thick exterior wall that protects the parasite against the harsh
elements that it will encounter outside the body. This cyst form of
parasite is infectious to other people or animals. Most people become
infected either directly (by hand-to-mouth transfer of cysts from the
feces of an infected individual) or indirectly (by drinking
feces-contaminated water). Less common modes of transmission include
ingestion of fecally contaminated food and hand-to-mouth transfer of
cysts after touching a fecally contaminated surface. After the cyst is
swallowed, the trophozoite is liberated through the action of stomach
acid and digestive enzymes and becomes established in the small
intestine. Although infection after ingestion of only one Giardia cyst
is theoretically possible, the minimum number of cysts shown to infect a
human under experimental conditions is 10 (Rendroff, 1954). Trophozoites
divide by binary fission about every 12 hours. What this means in
practical terms is that if a person swallowed only a single cyst,
reproduction at this rate would result in more than 1 million parasites
10 days later and 1 billion parasites by day 15. The exact mechanism by
which Giardia causes illness is not well understood, but it is not
necessarily related to the number of organisms present. Nearly all of
the symptoms, however, are related to dysfunction of the
gastrointestinal tract. The parasite rarely invades other parts of the
body, such as the gall bladder or pancreatic ducts. Intestinal infection
does not result in permanent damage. Data reported by the CDC indicate
that Giardia is the most frequently identified cause of diarrheal
outbreaks associated with drinking water in the United States. The
remainder of this section is devoted specifically to waterborne
transmissions of Giardia. Giardia cysts have been detected in 16\% of
potable water supplies (lakes, reservoirs, rivers, springs, groundwater)
in the United States at an average concentration of 3 cysts per 100 L
(Rose et al., 1991). The CDC reported that 242 outbreaks occurred from
1972 to 2011 (Adam et al., 2016). In 1983, for example, Giardia was
identified as the cause of diarrhea in 68\% of waterborne outbreaks in
which the causal agent was identified. In 1984, about 250,000 people in
Pennsylvania were advised to boil their drinking water for 6 months
because of Giardia-contaminated water. In 2007, an outbreak occurred in
New Hampshire that caused illness in 31 people (Daly et al., 2010). Many
of the municipal waterborne outbreaks of Giardia have been subjected to
intense study to determine their cause. Several general conclusions can
be made from data obtained in those studies. Waterborne transmission of
Giardia in the United States usually occurs in mountainous regions where
community drinking water obtained from clear running streams is
chlorinated but not filtered before distribution. Although mountain
streams appear to be clean, fecal contamination upstream by human
residents or visitors, as well as by Giardia-infected animals such as
beavers, has been well documented. Water obtained from deep wells is an
unlikely source of Giardia because of the natural filtration of water as
it percolates through the soil to reach underground cisterns. Wells that
pose the greatest risk of fecal contamination are poorly constructed or
improperly located ones. A few outbreaks have occurred in towns that
include filtration in the water treatment process, but the filtration
was not effective in removing Giardia cysts because of defects in filter
construction, poor maintenance of the filter media, or inadequate
pretreatment of the water before filtration. Occasional outbreaks have
also occurred because of accidental cross-connections between water and
sewage systems. Two major ingredients are necessary for a waterborne
outbreak: (1) Giardia cysts must be present in untreated source water,
and (2) the water purification process must fail to either kill or
remove Giardia cysts from the water. Although beavers are often blamed
for contaminating water with Giardia cysts, the fact that they are
responsible for introducing the parasite into new areas seems unlikely.
Far more likely is that they are also victims: Giardia cysts may be
carried in untreated human sewage discharged into the water by
small-town sewage disposal plants or they may originate from cabin
toilets that drain directly into streams and rivers. Backpackers,
campers, and sports enthusiasts may also deposit Giardiacontaminated
feces in the environment, which are subsequently washed into streams by
rain. In support of this concept is a growing amount of data indicating
a higher Giardia infection rate in beavers living downstream from U.S.
national forest campgrounds when compared with beavers living in more
remote areas that have a near zero rate of infection. Although beavers
may be unwitting victims of the Giardia story, they still play an
important part in the contamination scheme, because they can (and
probably do) serve as amplifying hosts. An amplifying host is one that
is easy to infect, serves as a good habitat for the parasite to
reproduce, and, in the case of Giardia, returns millions of cysts to the
water for every one ingested. Beavers are especially important in this
regard, because they tend to defecate in or very near the water, which
ensures that most of the Giardia cysts excreted are returned to the
water. The contribution of other animals to waterborne outbreaks of
Giardia is less clear. Muskrats (another semiaquatic animal) have been
found in several parts of the United States to have high infection rates
(30 to 40\%) (Frost et al., 1980). Studies have shown that muskrats can
be infected with Giardia cysts from humans and beavers. Occasional
Giardia infections have been reported in coyotes, deer, elk, cattle,
dogs, and cats (but not in horses and sheep) encountered in mountainous
regions of the United States. Naturally occurring Giardia infections
have not been found in most other wild animals (e.g., bear, nutria,
rabbit, squirrel, badger, marmot, skunk, ferret, porcupine, mink,
raccoon, river otter, bobcat, lynx, moose, bighorn sheep). Knowledge
about what is required to kill or remove Giardia cysts from a
contaminated water supply has increased considerably. For example, we
know that cysts can survive in cold water (4°C) for at least 2 months,
and they are killed instantaneously by boiling water (100°C) (Bingham et
al., 1979). We do not know how long the cysts will remain viable at
other water temperatures (e.g., at 0°C or in a canteen at 15 to 20°C),
nor do we know how long the parasite will survive on various environment
surfaces, such as under a pine tree, in the sun, on a diaper-changing
table, or in carpets in a daycare center. Their survivability decreases
as the temperature increases (USEPA, 2000). The effect of chemical
disinfection (chlorine, for example) on the viability of Giardia cysts
is an even more complex issue. The number of waterborne outbreaks of
Giardia that have occurred in communities where chlorine was employed as
a disinfectant demonstrates that the amount of chlorine used routinely
for municipal water treatment is not effective against Giardia cysts.
These observations were confirmed in the laboratory under experimental
conditions (Jarroll et al., 1980a,b). This does not mean that chlorine
does not work at all. It does work under certain favorable conditions.
Without getting too technical, gaining some appreciation of the problem
can be achieved by understanding a few of the variables that influence
the efficacy of chlorine as a disinfectant:

• Water pH---At pH values above 7.5, the disinfectant capability of
chlorine is greatly reduced. • Water temperature---The warmer the water,
the higher the efficacy. Chlorine does not work in ice-cold water from
mountain streams. • Organic content of the water---Mud, decayed
vegetation, or other suspended organic debris in water chemically
combines with chlorine, making it unavailable as a disinfectant. •
Chlorine contact time---The longer that Giardia cysts are exposed to
chlorine, the more likely it is that the chemical will kill them. •
Chlorine concentration---The higher the chlorine concentration, the more
likely it is that chlorine will kill Giardia cysts. Most water treatment
facilities try to add enough chlorine to give a free (unbound) chlorine
residual at the customer tap of 0.5 mg per liter of water.

These variables are so closely interrelated that improving one can often
compensate for another; for example, if chlorine efficacy is expected to
be low for icy stream water, the chlorine contact time or chlorine
concentration, or both, could be increased. In the case of
Giardia-contaminated water, producing safe drinking water with a
chlorine concentration of 1 mg per liter and contact time as short as 10
minutes might be possible if all the other variables are optimal---a pH
of 7.0, water temperature of 25°C, and total organic content of the
water close to zero. On the other hand, if all of these variables are
unfavorable---pH of 7.9, water temperature of 5°C, and high organic
content---chlorine concentrations in excess of 8 mg/L with several hours
of contact time may not be consistently effective. Because water
conditions and water treatment plant operations (especially those
related to water retention time and, therefore, to chlorine contact
time) vary considerably in different parts of the United States, neither
the USEPA nor the CDC has been able to identify a chlorine concentration
that would be safe yet effective against Giardia cysts under all water
conditions. For this reason, the use of chlorine as a preventive measure
against waterborne giardiasis generally has been used under outbreak
conditions when the amount of chlorine and contact time have been
tailored to fit specific water conditions and the existing operational
design of the water utility. In an outbreak, for example, the local
health department and water utility may issue an advisory to boil water,
may increase the chlorine residual at the consumer's tap from 0.5 mg/L
to 1 or 2 mg/L, and, if the physical layout and operation of the water
treatment facility permit, increase the chlorine contact time. These are
emergency procedures intended to reduce the risk of transmission until a
filtration device can be installed or repaired or until an alternative
source of safe water (a well, for example) can be made operational. The
long-term solution to the problem of municipal waterborne outbreaks of
giardiasis involves improvements in and more widespread use of filters
in the municipal water treatment process. The sand filters most commonly
used in municipal water treatment today cost millions of dollars to
install, which makes them unattractive for many small communities. The
pore sizes in these filters are not sufficiently small to remove Giardia
(6 to 9 µm by 8 to 12 µm). For the sand filter to remove Giardia cysts
from the water effectively, the water must receive some additional
treatment before it reaches the filter. The flow of water through the
filter bed must also be carefully regulated. An ideal prefilter
treatment for muddy water would include sedimentation (a holding pond
where large suspended particles are allowed to settle out by the action
of gravity) followed by flocculation or coagulation (the addition of
chemicals such as alum or ammonium to cause microscopic particles to
clump together). The sand filter easily removes the large particles
resulting from the flocculation--coagulation process, including Giardia
cysts bound to other microparticulates. Chlorine is then added to kill
the bacteria and viruses that may escape the filtration process. If the
water comes from a relatively clear source, chlorine may be added to the
water before it reaches the filter. The successful operation of a
complete waterworks operation is a complex process that requires
considerable training. Troubleshooting breakdowns or recognizing the
potential problems in the system before they occur often requires the
skills of an engineer. Unfortunately, most small water utilities with
water treatment facilities that include filtration cannot afford the
services of a full-time engineer. Filter operation or maintenance
problems in such systems may not be detected until a Giardia outbreak is
recognized in the community. The bottom line is that, although
filtration is the best protection against waterborne giardiasis that
water treatment technology has to offer for municipal water systems, it
is not infallible. For municipal water filtration facilities to work
properly, they must be properly constructed, operated, and maintained.
Whenever possible, persons outdoors should carry drinking water of known
purity with them. When this is not practical and when water from
streams, lakes, ponds, or other outdoor sources must be used, time
should be taken to properly disinfect the water before drinking it.
Boiling water is one of the simplest and most effective ways to purify
water. Boiling for one minute is adequate to kill Giardia as well as
most other bacterial or viral pathogens likely to be acquired from
drinking polluted water. Disinfecting water with chlorine or iodine is
considered less reliable than boiling for killing Giardia; however, we
recognize that boiling drinking water is not practical under many
circumstances. When boiling drinking water is not possible, chemical
disinfectants such as iodine or chlorine should be used. They offer some
protection against Giardia and destroy most bacteria and viruses that
cause illness. Iodine or chlorine concentrations of 8 mg/L (8 ppm) with
a minimum contact time of 30 minutes are recommended. If the water is
cold (less that 10°C or 50°F), a minimum contact time of 60 minutes is
recommended. If a choice of disinfectants is available, use iodine. The
disinfectant activity of iodine is less likely than that of chlorine to
be reduced by unfavorable water conditions, such as dissolved organic
material in the water or a high pH. Table 6.5 gives instructions for
disinfecting water using household tincture of iodine, and Table 6.6
gives instructions for using chlorine bleach. If water is visibly dirty,
it should first be strained through a clean cloth into a container to
remove any sediment or floating matter. The water should then be treated
with chemicals as shown in the tables. Portable filter devices for field
or individual use as well as some household filters are available for
use against waterborne giardiasis. Manufacturer data accompanying these
filters indicate that some can remove particles the size of a Giardia
cyst or smaller and may be capable of providing a source of safe
drinking water for an individual or family during a waterborne outbreak.
If carefully selected, such devices might also be useful in preventing
giardiasis for international travelers, backpackers, campers, sportsmen,
or persons who live or work in areas where water is known to be
contaminated. Unfortunately, very few published reports in the
scientific literature detail both the methods used and the results of
tests employed to evaluate the efficacy of these filters against
Giardia. Until more published experimental data become available,
consumers should look for a few common-sense indications when selecting
a portable or household filter. The first thing to consider is the filer
media. Filters relying solely on ordinary or silver-impregnated carbon
or charcoal should be avoided, because they are not intended to prevent,
destroy, or repel microorganisms. Their principal use is to remove
undesirable chemicals, odors, and very large particles such as rust or
dirt. Some filters rely on chemicals such as iodide-impregnated resins
to kill Giardia. Although properly designed and manufactured
iodide-impregnated resin filters have been shown to kill many species of
bacteria and viruses present in human feces, their efficacy against
Giardia cysts is less well established. The principle under which these
filters operate is similar to that achieved by adding the chemical
disinfectant iodine to water, except that the microorganisms in the
water pass over the iodideimpregnated disinfectant as the water flows
through the filter. The disinfectant activity of iodide is not as
readily affected as chlorine by water pH or organic content, but iodide
disinfectant activity is markedly reduced by cold water temperatures.
Experiments on Giardia indicate that many of the cysts in cold water
(4°C) remain viable after passage through filters containing triiodide
or pentaiodide disinfectant (Marchin et al., 1983). Longer contact time
(compared to those required to kill bacteria) is required when using
chemical filters to process cold water for Giardia protection. Currently
available chemical filters are also not recommended for muddy or very
turbid water. Note that filters relying solely on chemical action
usually give no indication to the user when disinfectant activity has
been depleted. The so-called microstrainer types of filters are true
filters. Manufacturer data accompanying these filters indicate that some
have a sufficiently small pore size to physically restrict the passage
of some microorganisms through the filter. The types of filter media
employed in microstraining filters include acrylic, ceramic, and
proprietary materials. Theoretically, a filter having an absolute pore
size of less than 6 µm might be able to prevent Giardia cysts 8 to 10 µm
in diameter from passing through. When used as a water sampling device
during community outbreaks, portable filters in the 1to 3-µm range more
effectively removed Giardia cysts from raw water than filters with
larger pore sizes. For effective removal of bacterial or viral organisms
that cause disease in humans, microstraining filters with pore sizes of
less than 1 µm are advisable; however, the smaller the pores, the more
quickly the filters will tend to clog. To obtain maximum filter life,
and as a matter of reasonable precaution, the cleanest available water
source should always be used. Keep in mind, however, that even
sparkling, clear mountain streams can be heavily contaminated. Another
thing to consider when choosing a filter includes whether the filter
element can be cleaned or replaced without posing a significant health
hazard to the user, because infectious organisms can be concentrated on
the filter element or media. Properly engineered portable filters should
minimize the possibility of contaminating the clean water side of the
filter with contaminated water during replacement or cleaning of the
filter element. Because filters used in the field are often rinsed in a
stream or river that may be contaminated, this is an especially
important consideration for recreational outdoor use.

Cryptosporidium In 1907, when Ernest E. Tyzzer recognized, described,
and published an account of a parasite he frequently found in the
gastric glands of laboratory mice, he and his new discovery were
overlooked---just another scientist going quietly about his normal,
tedious, out-of-the-limelight research, buried in obscurity. Initially,
his studies focused on describing the asexual and sexual stages and
spores (oocysts), each with a specialized attachment organelle, and he
noted that the spores were excreted in the feces. Tyzzer identified the
parasite as a sporozoan, but of uncertain taxonomic status; he named it
Cryptosporidium muris (Tyzzer, 1907). Later, in 1910, after more
detailed study, he proposed Cryptosporidium as a new genus and C. muris
as the type species. Amazingly, except for developmental stages,
Tyzzer's original description of the life cycle (see Figure 6.5) was
later confirmed by electron microscopy. Then, in 1912, Tyzzer described
a new species, Cryptosporidium parvum. For almost 50 years, Tyzzer's
discovery of the genus Cryptosporidium remained (like himself)
relatively obscure because it appeared to be of no medical or economic
importance. Slight rumblings of the importance of the genus began to be
felt in the medical community when Slavin (1955) wrote about a new
species, Cryptosporidium melagridis, which was associated with illness
and death in turkeys. Interest remained slight even when Cryptosporidium
was found to be associated with bovine diarrhea (Panciera et al., 1971).
Not until 1982 did worldwide interest focus on the study of organisms in
the genus Cryptosporidium. At that time, the medical community and other
interested parties were beginning a full-scale, frantic effort to find
out as much as possible about acquired immune deficiency syndrome
(AIDS), and the CDC reported that 21 AIDSinfected males from six large
cities in the United States had severe protracted diarrhea caused by
Cryptosporidium. It was in 1993, though, that Cryptosporidium---the
``pernicious parasite''---made itself and Milwaukee famous (Mayo
Foundation, 1996). The massive waterborne outbreak in Milwaukee,
Wisconsin, where more than 400,000 persons developed acute and often
prolonged diarrhea or other gastrointestinal symptoms, increased
awareness of Cryptosporidium at an exponential level. The Milwaukee
incident spurred both public interest and the interest of public health
agencies, agricultural and environmental agencies and groups, and
suppliers of drinking water. This increase in interest level and concern
spurred new studies of Cryptosporidium with an emphasis on developing
methods for recovery, detection, prevention, and treatment (Fayer et
al., 1997).

The USEPA became particularly interested in this ``new'' pathogen; for
example, in their reexamination of water treatment and disinfection
regulations, the USEPA issued maximum contaminant level goals (MCLGs)
for Cryptosporidium and added it to their Contaminant Candidate Lists
(CCLs). Its similarity to Giardia lamblia and the necessity to provide
an efficient conventional water treatment capable of eliminating viruses
forced the USEPA to regulate surface water supplies in particular. The
Enhanced Surface Water Treatment Rule (ESWTR), promulgated in 1998,
includes regulations covering watershed protection, specialized
operation of treatment plants (certification of operators and state
overview), and effective chlorination. Protection against
Cryptosporidium includes control of waterborne pathogens such as Giardia
and viruses (DeZuane, 1997).

Cryptosporidium Basics Cryptosporidium is one of several single-celled
protozoan genera in the phylum Apircomplexa (all referred to as
coccidian). Cryptosporidium along with other genera in the phylum
Apircomplexa develop in the gastrointestinal tract of vertebrates
through all of their life cycle; in short, they live in the intestines
of animals and people. This microscopic pathogen causes a disease called
cryptosporidiosis. The dormant (inactive) form of Cryptosporidium is
called an oocyst and is excreted in the feces of infected humans and
animals. The tough-walled oocysts survive under a wide range of
environmental conditions (Kneen et al., 2004). Several species of
Cryptosporidium incorrectly named after the host in which they were
found have been invalidated, but today at least 26 species of
Cryptosporidium are recognized as valid (Ryan et al., 2014); eight of
these are listed in (Table 6.7). C. muris infects the gastric glands of
laboratory rodents and several other mammalian species but is not known
to infect humans (even though several texts state otherwise). C. parvum,
however, infects the small intestine of an unusually wide range of
mammals, including humans, and is the zoonotic species responsible for
human cryptosporidiosis. In most mammals, C. parvum is predominately a
parasite of neonate (newborn) animals. Even though exceptions occur,
older animals generally develop poor infections, even when unexposed
previously to the parasite (Upton, 1997). Humans are the one host that
can be seriously infected at any time in their lives, and only previous
exposure to the parasite results in either full or partial immunity to
challenge infections (Upton, 1997). Oocysts are present in most surface
bodies of water across the United States, many of which supply public
drinking water. Oocysts are more prevalent in surface waters when heavy
rains increase runoff of wild and domestic animal wastes from the land
or when sewage treatment plants are overloaded or break down. Only
laboratories with specialized capabilities can detect the presence of
Cryptosporidium oocysts in water. Unfortunately, current sampling and
detection methods are unreliable. Recovering oocysts trapped on the
material used to filter water samples is difficult. When a sample has
been obtained, however, determining whether the oocyst is alive and if
it is C. parvum and thus can infect humans can be easily accomplished by
looking at the sample under a microscope. The number of oocysts detected
in raw (untreated) water varies with location, sampling time, and
laboratory methods. Water treatment plants remove most, but not always
all, oocysts. Low numbers of oocysts are sufficient to cause
cryptosporidiosis, but the low numbers of oocysts sometimes present in
drinking water are not considered cause for alarm for the public.
Protecting water supplies from Cryptosporidium demands multiple
barriers. Why? Because Cryptosporidium oocysts have tough walls that can
withstand many environmental stresses and are resistant to chemical
disinfectants such as chlorine that are traditionally used in municipal
drinking water systems and swimming pools. Physical removal of
particles, including oocysts, from water by filtration is an important
step in the water treatment process. Typically, water pumped from rivers
or lakes into a treatment plant is mixed with coagulants (see Chapter
11), which help settle out particles suspended in the water. If sand
filtration is used, even more particles are removed. Finally, the
clarified water is disinfected and piped to customers. Filtration is the
only conventional method now in use in the United States for controlling
Cryptosporidium. Ozone is a strong disinfectant (see Chapter 11) that
kills protozoa if sufficient doses and contact times are used, but ozone
leaves no residual for killing microorganisms in the distribution
system, as does chlorine. The high costs of new filtration or ozone
treatment plants must be weighed against the benefits of additional
treatment. Even well-operated water treatment plants cannot ensure that
drinking water will be completely free of Cryptosporidium oocysts. Water
treatment methods alone cannot solve the problem; watershed protection
and monitoring of water quality are critical. Land use controls such as
septic system regulations and best management practices to control
runoff can help keep human and animal wastes out of water. Under the
Surface Water Treatment Rule of 1989, public water systems must filter
surface water sources unless water quality and disinfection requirements
are met and a watershed control program is maintained. This rule,
however, did not address Cryptosporidium. The USEPA has now set
standards for turbidity (cloudiness) and coliform bacteria (which
indicate that pathogens are probably present) in drinking water.
Frequent monitoring must occur to provide officials with early warning
of potential problems to enable them to take steps to protect public
health. Unfortunately, no water quality indicators can reliably predict
the occurrence of cryptosporidiosis. More accurate and rapid assays of
oocysts will make it possible to notify residents promptly if their
water supply is contaminated with Cryptosporidium and thus avert
outbreaks. The bottom line is that the collaborative efforts of water
utilities, government agencies, healthcare providers, and individuals
are needed to prevent outbreaks of cryptosporidiosis.

SIDEBAR: SYDNEY AUSTRALIA* From the end of July to the end of September
1998, upon three occasions, residents of the city of Sydney, Australia,
had to take the precaution of boiling their drinking water. Testing
found Giardia and Cryptosporidium in the public water supply. According
to the Sydney authorities, at these levels the Giardia and
Cryptosporidium cysts posed little, if any, health threat. No incidents
of illness were linked to the presence of Giardia and Cryptosporidium;
however, businesses that relied on large quantities of pure water could
not function on a boil-alert quality of water. Evidence seemed to
indicate that the plant itself was creating the problem and that the
results of water tests performed by the lab (Australian Water
Technologies) were either conducted improperly or were not read properly
and misinterpreted. The aftermath of the incidents left the Sydney Water
Corporation, the privately owned organization handling the treatment
systems since privatization in 1995, in shambles. Beginning on July 29,
1998, the three boil alerts led to a massive investigation into the
causes and sources of the contamination, as well as the resignation of
the Sydney Water Corporation's managing director and chairman, whose
blossoming political career was cut short. Sydney Water Corporation was
stripped of responsibility and its major assets; it lost all control of
its treatment plants, dams, and catchment to the government's new Sydney
Catchment Authority. Sydney Water Corporation was ordered to repay
residential water users for the expense and trouble of using bottled
water, and, of course, numerous lawsuits were initiated by businesses
and industries affected by the shutdown. On the positive side, Australia
has put an American-style Clean Water Act into place (which, if nothing
else, will establish guidelines to follow in such a case), and Sydney is
working to ensure good preventive maintenance measures for watershed
protection. Of special interest to water pollution control technologists
are the tests, their results, and the difficulty in pinpointing the
source (or sources) and cause of the contamination. Experts warned that
actually finding the direct source of the Sydney outbreaks was unlikely
(e.g., the cause of the 1995 outbreak in Milwaukee is uncertain), and
they recommended installing either an ozonation or microfiltration
system to ensure completely safe drinking water. Such expert advice,
though, presumed actual Giardia and Cryptosporidium
contamination---which at that point was in more than a little doubt.
Test results on the same water samples taken during the outbreaks varied
widely. Tests for the later shutdowns were less accurate than ones for
the initial shutdown---not surprising under the panic conditions at the
lab, which was under tremendous pressure to find the causes and was
reluctant to risk either consumer wrath over the inconvenience or
consumer illness and death due to contamination. One test sample was
read at 1000 Cryptosporidium oocysts initially, but a reexamination of
the sample found only 2 oocysts. Technicians may also have mistaken
harmless algae similar in appearance to Giardia and Cryptosporidium for
the dangerous cysts, raising false (and expensive) alarms. Even in
retesting, test results were shaky. The New South Wales Health
Department counted what they thought were more than 9000 oocysts per 100
liters of treated water for one sample---higher levels than testing
should find in raw sewage. An expert from the department of civil
engineering at the University of New South Wales who saw lots of
different algae and no Cryptosporidium in his own tests on the water
pointed out that the highest U.S. level ever reported was 1000---and
that those U.S. reports included the 1995 Milwaukee outbreak, where
hundreds of thousands of people fell ill.~He also pointed out that other
common fecal bacteria should be present in the sample as well, but were
not (Anon., 1998). The moral of the story? Know your stuff. Be sure of
your technique and do everything you can to ensure the accuracy of your
samples and test results. As a water control technologist, you may have
to turn in test results that open a similar can of worms at some
point---or announce unsafe water supplies to the press.Cryptosporidiosis
Cryptosporidium parvum is an important emerging pathogen in the U.S. and
a cause of severe, life-threatening disease in patients with AIDS. No
safe and effective form of specific treatment for cryptosporidiosis has
been identified to date. The parasite is transmitted by ingestion of
oocysts excreted in the feces of infected humans or animals. The
infection can therefore be transmitted from person-to-person, through
ingestion of contaminated water (drinking water and water used for
recreational purposes) or food, from animal to person, or by contact
with fecally contaminated environmental surfaces. Outbreaks associated
with all of these modes of transmission have been documented. Patients
with human immunodeficiency virus infection should be made more aware of
the many ways that Cryptosporidium species are transmitted, and they
should be given guidance on how to reduce their risk of exposure.
Juranek (1995) Since the Milwaukee outbreak, concern about the safety of
drinking water in the United States has increased, and new attention has
been focused on determining and reducing the risk of acquiring
cryptosporidiosis from community and municipal water supplies.
Cryptosporidiosis is spread by putting something in the mouth that has
been contaminated with the stool of an infected person or animal. In
this

way, people swallow the Cryptosporidium parasite. As mentioned earlier,
a person can become infected by drinking contaminated water or eating
raw or undercooked food contaminated with Cryptosporidium oocysts, by
direct contact with the droppings of infected animals or stools of
infected humans, or by hand-to-mouth transfer of oocysts from surfaces
that may have become contaminated with microscopic amounts of stool from
an infected person or animal. Symptoms may appear 2 to 10 days after
infection by the parasite. Although some persons may not have symptoms,
others have watery diarrhea, headache, abdominal cramps, nausea,
vomiting, and low-grade fever. These symptoms may lead to weight loss
and dehydration. In otherwise healthy persons, these symptoms usually
last 1 to 2 weeks, at which time the immune system is able to defeat the
infection. In persons with suppressed immune systems, such as persons
who have AIDS or who recently have had an organ or bone marrow
transplant, the infection may continue and become life threatening.
Currently, no safe and effective cure for cryptosporidiosis exists.
People with normal immune systems improve without taking antibiotic or
antiparasitic medications. The treatment recommended for this diarrheal
illness is to drink plenty of fluids and to get extra rest. Physicians
may prescribe medication to slow the diarrhea during recovery. The best
way to prevent cryptosporidiosis is to

• Avoid water or food that may be contaminated. • Wash hands after using
the toilet and before handling food. • Be sure, if you work in a daycare
center, to wash your hands thoroughly with plenty of soap and warm water
after every diaper change, even if you wear gloves when changing
diapers.

During community-wide outbreaks caused by contaminated drinking water,
drinking water practitioners should inform the public to boil drinking
water for 1 minute to kill the Cryptosporidium parasite.

CyClospora Cyclospora organisms, which until recently were considered
blue--green algae, were discovered at the turn of the century. The first
human cases of Cyclospora infection were reported in the 1970s. In the
early 1980s, Cyclospora was recognized as a pathogen in patients with
AIDS. We now know that Cyclospora is endemic in many parts of the world
and appears to be an important cause of traveler's diarrhea. Cyclospora
are two to three times larger than Cryptosporidium, but otherwise have
similar features. Cyclospora diarrheal illness in patients with healthy
immune systems can be cured with a week of therapy with
timethoprim--sulfamethoxazole (TMP--SMX). What is believed to be the
first known outbreak of diarrheal illness associated with Cyclospora in
the United States occurred in 1990 (Huang et al., 1995) and consisted of
21 cases of illness among physicians and others working at a Chicago
hospital. Contaminated tap water from a physicians' dormitory at the
hospital was the probable source of the organism. The tap water probably
picked up the organism while in a storage tank at the top of the
dormitory after the failure of a water pump.

Watery diarrhea, abdominal cramping, low-grade fever, and decreased
appetite are common features of the illness. The illness also is marked
by periods of remission and relapse that may continue for up to several
weeks. Microscopic examination of stool specimens from 11 infected
people showed many spherical bodies 8 to 10 µm in diameter that were
identified as a Cyclospora species. The only other outbreaks associated
with Cyclospora in the literature have been seasonal outbreaks in Nepal.
One outbreak in Nepal was associated with chlorinated drinking water.

Cyclospora Basics Cyclospora cayetanensis is a unicellular parasite
previously known as a cyanobacterium-like (blue--green algae-like) or
coccidia-like body (CLB). Since the first known cases of illness caused
by Cyclospora infection were reported in the medical journals in the
1970s, cases have been reported with increased frequency from around the
world, in part because of the availability of better techniques for
detecting the parasite in stool specimens. The transmission of
Cyclospora is not a straightforward process. When infected persons
excrete the oocyst state of Cyclospora in their feces, the oocysts are
not infectious and may require from days to weeks to become infectious
(i.e., to sporulate). Transmission of Cyclospora directly from an
infected person to someone else is unlikely; however, indirect
transmission can occur if an infected person contaminates the
environment and oocysts have sufficient time, under appropriate
conditions, to become infectious. Cyclospora, for example, may be
transmitted by ingestion of water or food contaminated with oocysts.
Outbreaks linked to contaminated water, as well as outbreaks linked to
various types of fresh produce, have been reported (CDC, 1997a,b, 2016a;
Herwaldt et al., 1997; Huang et al., 1995). How common the various modes
of transmission and sources of infection are is not yet known, nor is it
known whether animals can be infected and serve as sources of infection
for humans. Persons of all ages are at risk for infection. Persons
living or traveling in developing countries may be at increased risk,
but infection can be acquired worldwide, including in the United States.
In some countries of the world, infection appears to be seasonal. The
incubation period between acquisition of infection and the onset of
symptoms averages a week. Cyclospora infects the small intestine and
typically causes watery diarrhea, with frequent, sometimes explosive
stools. Other symptoms can include loss of appetite, substantial loss of
weight, bloating, increased flatus, stomach cramps, nausea, vomiting,
muscle aches, low-grade fever, and fatigue. If untreated, illness may
last for a few days to a month or longer and may follow a
remitting--relapsing course. Some infected persons are asymptomatic.
Identification of this parasite in stool requires special, not routine,
laboratory tests to be done (discussed later). A single negative stool
specimen does not rule out the diagnosis; three or more specimens may be
required. Stool specimens should also be checked for other microbes that
can cause a similar illness. TMP--SMX has been shown to be effective
treatment for Cyclospora infection (CDC, 2016b). No alternative
antibiotic regimen has been identified yet for patients who do not
respond to or are intolerant of TMP--SMX. Based on currently available
information, avoiding food or water that may be contaminated with stool
is the best way to prevent infection, and reinfection can occur.

Following are key points for the diagnosis of Cyclospora (Huang et al.,
1995):

\begin{enumerate}

\item
  To maximize recovery of Cyclospora oocysts, first concentrate the
  stool specimen by the Formalin-ethyl acetate technique (centrifuge for
  10 minutes at 500× g) and then examine a wet mount and/or stained
  slide of the sediment.
\item
  Cyclospora oocysts are 8 to 10 µm in diameter (in contrast,
  Cryptosporidium parvum oocysts are 4 to 6 µm in diameter).
\item
  Ultraviolet epifluorescence microscopy is a sensitive technique for
  rapidly examining stool sediments for Cyclospora oocysts, which
  autofluoresce (Cryptosporidium parvum oocysts do not). If suspect
  oocysts are found, bright-field microscopy can then be used to confirm
  that the structures have the characteristic morphologic features of
  Cyclospora oocysts (nonrefractile spheres that contain
  undifferentiated cytoplasm of refractile globules).
\item
  On a modified acid-fast-stained slide of stool (technique used by most
  laboratories), Cyclospora oocysts are variably acid fast (i.e., in the
  same field, oocysts may be unstained or stained from light pink to
  deep red). Unstained oocysts may have a wrinkled appearance; observers
  must distinguish oocysts from artifacts that may be acid fast but do
  not have the all-important wrinkled morphology of the oocyst wall.
\item
  Using a modified safranin technique, oocysts uniformly stain a
  brilliant reddish orange if fecal smears are heated in a microwave
  oven during staining (Visvesvara et al., 1997). If epifluorescence
  microscopy is available, the stained slide can first be examined with
  this technique and suspect oocysts reexamined with bright-field
  microscopy.
\item
  Although not recommended as an optimal technique for detection of
  Cyclospora, on a trichrome-stained slide of stool the oocysts appear
  as clear, round, and somewhat wrinkled.
\end{enumerate}

HELMINTHS Drinking water in the United States may transmit the following
intestinal worms (nematodes) (NAS, 1977):

• Ascaris lumbricoides (stomach worm) • Trichuris trichiura (whip worm)
• Ancylostoma duodenale (hookworm) • Necator americanus (hookworm) •
Strongyloides stercoralis (threadworm)

Along with inhabiting organic mud, worms also inhabit biological slimes
and are derived from sewage and wet soil. Nematodes multiply in
wastewater treatment plants; strict aerobes, they have been found in
activated sludge and particularly in trickling filters and therefore
appear in large concentrations in treated domestic liquid waste.
Nematodes range in length from 0.5 to 3 mm and in diameter from 0.01 to
0.05 mm. Most species have a similar appearance. They have a body that
is covered by cuticle, is cylindrical and nonsegmented, and tapers at
both ends.

Nematodes ingest bacterial pathogens that protect them from water supply
disinfectants and enhance their chances of reaching the consumer. Active
motile nematode larvae can penetrate sand filters and survive
chlorination, but they are not normally expected to cause parasitic
nematode infections (NAS, 1977). Free-living nematodes have a life cycle
consisting of egg, four larval stages, and one adult stage. Eggs are
easily recognizable in finished water, but raw water must have excessive
microfaunal forms to allow identification. Environmental conditions have
an impact on the growth of nematodes; for example, in anoxic conditions,
their swimming and growth are impaired. Temperature fluctuations
directly affect their growth and survival; nematode populations decrease
when temperatures increase. Aquatic flatworms (improperly named because
they are not all flat) feed primarily on algae. Because of their
aversion to light, they are found in the lower depths of pools.
Flatworms are very hardy and can survive in wide variations in humidity
and temperature. Surface waters that are grossly polluted with organic
matter (especially domestic sewage) have a fauna capable of thriving in
very low concentrations of oxygen. A few species of tubificid worms
dominate this environment. The bottoms of severely polluted streams can
be literally covered with a ``writhing mass'' of these tubificids
(Pennak, 1989). For the drinking water practitioner interested in
learning more about aquatic worms, the current Standard Methods and
Procedures has a section covering nematological examinations that
details sample collection and provides an illustrated key to freshwater
nematodes.

SUMMARY Pathogenic parasites are not easily removed or eliminated
completely by conventional treatment and disinfection unit processes
(DeZuane, 1997). This is particularly true for Giardia lamblia,
Cryptosporidium, and Cyclospora. Filtration facilities can be adjusted
in depth, prechlorination, filtration rate, and backwashing to become
more effective in the removal of cysts. The pretreatment of protected
watershed raw water is a major factor in the elimination of pathogenic
protozoa (see Chapter 11). Before discussing the physical
characteristics relevant to drinking water evaluation and the analysis
of water found in nature as potential or actual sources of water
supplies, some physical characteristics of water that are perhaps not so
obvious should be mentioned. The sections that follow address the
typical, traditional physical parameters of taste and odor, color,
temperature, turbidity, and solids, but, as shown in Figure 7.1, this
chapter also includes discussion about pH and solubility. Alkalinity and
hardness are compared, even though they can also be considered chemical
parameters. Finally, because of the importance of pH in water treatment,
this chapter reviews the basic concepts of water solubility. It is
important to point out that when this text refers to water quality the
definition is predicated on the intended use of the water (in this case,
potable use). Over the years, many parameters have evolved that
qualitatively reflect the impact that various contaminants (impurities)
have on water used for potable purposes; the following sections provide
a brief discussion of these parameters. Before we discuss the parameters
shown in Figure 7.1 let's talk about where many of the physical
contaminants come from. Think about hiking in a high mountain meadow and
later waking from the miles-deep sleep of earned exhaustion to the
awareness of peace \ldots{} inhaling deep draughts of cool, clean
mountain air; breathing through nostrils tickled with the pungency of
pure, sweet pine \ldots{} eardrums soothed by the light tattoo of fresh
rain pattering against taut nylon \ldots{} watching darkness lifted,
then suddenly replaced with cloud-filtered daylight \ldots{} being
spellbound by the sudden, ordinary miracle of a new morning \ldots{}
anticipating expected adventures and appreciating the pure,
unadulterated treasure of pristine wilderness momentarily owned, with
minds not weighed down by mundane, everyday existence. Glorious account,
isn't it? Makes us want to pack up and head for the hills, so to speak.
But there is one short phrase in the account that must be repeated here
for emphasis: ``eardrums soothed by the light tattoo of fresh rain
pattering against taut nylon.'' Rainfall is a key element of the natural
water cycle, which is essential to maintaining life on Earth as we know
it. But, there is more to rainfall than the sound of it striking against
the taut nylon of a tent in the woods, or anywhere else for that matter.
Rainfall events are double-edged swords. They are one of the most
important natural occurrences that life on this planet depends on, but
they are also one of the most significant contributors to the
degradation of drinking water source water quality affecting surface
waters and groundwaters. Rainfall drives the movement of materials,
including pathogens, into and through water bodies. It can move soil,
resuspend sediments, cause overflow of combined and poorly maintained
sewers, and degrade groundwater by infiltrating subsurface aquifers.

TASTE AND ODOR Under the Safe Drinking Water Act (SDWA), the U.S.
Environmental Protection Agency (USEPA) issued guidelines to the states
regarding secondary drinking water regulations. These guidelines apply
to drinking water contaminants that may adversely affect the aesthetic
qualities of water such as odor and appearance. Because these aesthetic
qualities have no known adverse health effects, secondary regulations
are not mandatory, but most drinking water systems comply with the
limits. They have learned through experience that, although the odor and
appearance of drinking water are not problems until a customer
complains, one thing is certain---they will complain, and complain quite
often. Disinfection itself often becomes a major source of complaint.
Probably the most often expressed complaint among drinking water
consumers is a ``chlorine taste,'' although the odor threshold can be as
low as 0.2 to 0.4 mg/L at the typical pH level (DeZuane, 1997). Taste
and odor are used jointly in the vernacular of water science. Even
though drinking water taste and odor are not normally a problem until a
consumer complains, drinking water practitioners have learned through
experience that such problems may be an early indication of a potential
health hazard. Taste and odor are important for aesthetic reasons (as a
measure of the acceptability of water) and rarely have an impact on how
safe water is to drink, but they should not be ignored. Objectionable
taste and odor are more likely found at the raw water source than at the
consumer's tap. In general, water contaminants are attributable to
contact with nature or human use. Taste and odor in water are caused by
a variety of substances, including minerals, metals and salts from the
soil, constituents of wastewater, and end products produced in
biological reactions. Earthy or musty odors common in some water
supplies are normally derived from natural biological processes. More
offensive odors, such as those caused by hydrogen sulfide gas (H2S), are
not unusual in water supplies. The rotten-egg smell of hydrogen sulfide
gas may be encountered in water that has been in contact with naturally
occurring deposits of decaying organic matter. Groundwater supplies
sometimes have this problem; such wells are commonly referred to as
sulfur wells. Problems with tastes and odors are usually associated with
surface rather than groundwater. Surface water taste and odor problems
are normally caused by algae and other microorganisms, whereas
groundwater taste and odor problems are generally the result of human
interference or influence (e.g., landfill leachate). Note: The ability
of humans to detect odor thresholds of various substances in water
ranges from a low of about 1 µg/L for methylisoborneol to a high of
about 20 mg/L for chloroform. For testing performed in laboratories,
consult Standard Methods for the Examination of Water and Wastewater
(APHA-AWWA-WEF, 2017). The qualitative terms used to describe taste and
odor are often classified as grassy, swampy, septic, musty, fishy,
phenolic, and sweet. In water treatment, one of the common methods used
to remove taste and odor is to use oxidants, including potassium
permanganate and chlorine, to oxidize the materials causing the problem.
Another common treatment method is to feed powdered activated carbon
prior to the filter. The activated carbon has numerous small openings
that adsorb the components that cause the odor and tastes. Taste and
odor problems are also often controlled by watershed management,
algicides, aeration, and pretreatment. Properly functioning water
filtration systems help to minimize taste and odor problems, as well.
Odor is typically measured and expressed in terms of a threshold odor
number (TON), which represents the dilution required to make the odor
become virtually unnoticeable. In 1989, the USEPA issued a secondary
maximum contaminant level (SMCL) of 3 TON for odor. (Remember that
secondary standards apply to parameters not related to health.) When a
dilution is used, a number can be derived to describe clarifying odor:

\begin{verbatim}
              Threshold odor number = (VT + VD)/VT  (7.1)
\end{verbatim}

where VT = volume tested, and VD = volume of dilution with odor-free
distilled water. For VD = 0, TON = 1 (lowest value possible); for VD =
VT, TON = 2; for VD = 2VT, TON = 3, etc. Note: Although taste and odor
(along with color) are seldom connected to toxicological effects, the
drinking water practitioner should never be fooled into assuming that a
water supply with a ``bit'' of taste and odor will not offend the
consumer. It will.

COLOR The quality of water can also be judged by its color, and the
consumer does so, at least from a psychological point of view. Imagine
going to the kitchen tap and drawing a glass of water that is rust
colored, maybe to the point where you cannot even

see your fingers on the other side of the glass. The rust-colored water
may be safe to drink, but do you honestly think that the average
consumer---who is used to drinking a colorless glass of water---is going
to happily consume the rust-colored contents of that glass? Doubtful, at
best. And let us not forget the folks in Flint, Michigan, who were
provided tap water containing lead and other corrosion products sloughed
off from aging pipes. Pure water is colorless, but water in nature is
often colored by foreign substances, including organic matter from
soils, vegetation, minerals, and aquatic organisms that are often
present in natural waters. Color can also be contributed by municipal
and industrial wastes. Color in water is classified as either true color
or apparent color. Color partly due to dissolved solids that remain
after removal of suspended matter is known as true color; color
contributed by suspended matter is the apparent color. In water
treatment, true color is the most difficult to remove. Color in water,
although not usually considered unsafe or unsanitary, does present a
treatment problem because it exerts a chlorine demand, which reduces the
effectiveness of chlorine as a disinfectant. Color is measured by
comparing the water sample with standard color solutions or colored
glass disks. One color unit (CU) is equivalent to the color produced by
a 1-mg/L solution of platinum. In 1989, the USEPA issued a secondary MCL
of 15 color units for color. At 10 to 15 color units, color may not be
visually detectable; at 100 color units, water may have the appearance
of tea. Note: In practice, the process of isolating and identifying
specific chemicals that cause the color is not practical. As we have
pointed out, color in water is a matter of aesthetics; consumers do not
find it acceptable. No matter how safe the water may be to drink, most
people object strongly to water that offends their sense of sight. Given
a choice, the public obviously would prefer clear, uncolored water. The
effects of color in water, though, extend beyond psychological
implications; for example, colored water affects laundering,
papermaking, manufacturing, textiles, and food processing. The color of
water has a profound effect on its marketability for both domestic and
industrial use.

TEMPERATURE Water possesses many important thermal qualities; for
example, water has a high specific heat. Water is not subject to rapid
temperature fluctuations, because it can absorb or lose large amounts of
heat with relatively small changes in temperature. Water temperature
changes gradually in response to seasonal changes. Small water bodies
will be influenced by air temperature more quickly that larger water
bodies. The ideal water supply has, at all times, an almost constant
temperature or one with minimum variation. Real-world conditions,
however, do not always provide this condition, especially in surface
water supplies. Thermal pollution is often cited as

a cause of wild variations in surface water supplies (see Chapter 10).
Many thermal pollution problems are a result of anthropogenic (human)
activities; however, some water quality problems occur because of
natural temperature fluctuations. Whatever the cause of temperature
fluctuations, to live and reproduce fish and other aquatic organisms
require certain conditions of temperature. The optimum temperature for
trout (a coldwater fish) is 15°C, whereas carp require a temperature of
about 32°C, more than twice the preferred temperature for trout. The
problem with varying temperatures in surface waters (besides the impact
on the health of the fish population) is that temperature affects the
solubility of oxygen in water, the rate of bacterial activity, and the
rate at which gases are transferred to and from the water, all of which
are concerns for the drinking water practitioner. Water temperature also
affects how efficiently certain water treatment processes operate; for
example, temperature affects the rate at which chemicals dissolve and
react. Cold water requires more chemicals for efficient coagulation and
flocculation to take place. When water temperature is high, chlorine
demand may rise due to the increased reactivity, and warmer temperatures
often result in an increased level of algae and other organic matter in
raw water. Temperature values are not normally standardized by public
health criteria because of the insignificant health effects of water
temperature. Water temperature, however, does have an influence on the
treatment of water supplies, on the aquatic life of water reservoirs
(biochemical reactions may double the reaction rate for a 10°C increase
in temperature), on the taste of drinking water, on the level of
dissolved oxygen, on the activity of organisms producing bad taste and
odor, on the solubility of solids in water, and on the rate of corrosion
of the distribution system (DeZuane, 1997). Note: When sampling,
temperature readings must be done immediately because of changes caused
by air temperature and manipulation of the sample.

TURBIDITY Turbidity is a unit of measurement quantifying the degree to
which light traveling through a water column is scattered by the
suspended organic (including algae) and inorganic particles. The
scattering of light increases with a greater suspended load. Turbidity
is commonly measured in nephelometric turbidity units (NTUs) but may
also be measured in Jackson turbidity units (JTUs). Note: To obtain
nephelometric turbidity units, observers compare the light scattered by
the sample and the light scattered by a reference solution: Detection
limits---Should be able to detect turbidity differences of 0.02 NTU with
a range of 0 to 40 NTU Interferences---Rapidly settling coarse debris,
dirty glassware, presence of air bubbles, and surface vibrations The
velocity of the water resource largely determines the composition of the
suspended load. Suspended loads are carried in both the gentle currents
of lentic (lake) waters and the fast currents of lotic (flowing) waters.
Even in flowing water, the suspended load usually consists of grains
less than 0.5 mm in diameter. Suspended loads in lentic waters usually
consist of the smallest sediment fractions---silt and clay.

Turbidity plays an important role in drinking water quality, for,
without a doubt, one of the first things consumers notice about water is
its clarity. Turbidity may be caused by organic or inorganic
constituents, but the organic particulates may harbor microorganisms;
thus, turbid conditions may represent a threat of waterborne disease
(see Chapter 6). Turbidity may be classified as both a physical
parameter, because it can raise aesthetic and psychological objections
by the consumer, and a microbiological parameter, because it may harbor
pathogens and impede the effectiveness of disinfection. Note: Inorganic
constituents have no notable health effects. In surface water supplies,
most turbidity results from the erosion of very small colloidal
material, including rock fragments, silt, clay, and metal oxides from
the soil. Microorganisms and vegetable material may also contribute to
turbidity. Wastewaters from industry and households usually contain a
wide variety of turbidity-producing material. Detergents, soaps, and
various emulsifying agents contribute to turbidity. Turbidity
measurements are normally made on ``clean'' waters as opposed to
wastewaters. In water treatment, turbidity is useful in defining
drinking water quality and is relatively easy to measure. Given that the
total coliform test is a very reliable routine test of drinking water
quality but not an actual determination of pathogens in water, its use
in combination with a turbidity reading and their joint evaluation can
provide an extra safety factor for judging water quality changes either
at the source or during distribution system sampling (DeZuane, 1997).
Note: In the preliminary evaluation of raw water, when turbidity at the
source of supply is under 10 units, generally only disinfection is
required---with biochemical oxygen demand (BOD) at less than 1.0,
coliform under 50 MPN/100 mL monthly average, and acceptable chemical
parameters. When turbidity at the source exceeds 40 units, conventional
treatment is considered necessary.

Turbidity is regulated by the 1996 Safe Drinking Water Act amendments
and by Interim Enhanced Surface Water Treatment Rule (IESWTR) treatment
optimization rules, which apply to large public water systems (those
serving more than 10,000 people) that use surface water or groundwater
directly influenced by surface water. The rules require continuous
turbidity monitoring of individual filters and tighten allowable
turbidity limits for combined filter effluent, cutting the maximum from
5 NTU to 1 NTU and the average monthly limit from 0.5 NTU (for
conventional or direct filtration) to 0.3 NTU in at least 95\% of the
daily samples for any two consecutive months.

SOLIDS Water always contains a certain amount of particulate matter
ranging from colloidal organic or inorganic matter to silts, algae,
plankton or debris of all kinds. All water contaminants other than gases
contribute to the solids content. Solids can be dispersed in water in
both suspended and dissolved forms. Some dissolved solids may be
perceived by the physical senses but fall more appropriately under the
category of chemical parameters (see Chapter 8). Classified by size and
state, by chemical characteristics, and by size distribution, solids in
drinking water may consist of inorganic particles (salts) with small
concentrations of inorganic matter or of immiscible liquids.
Contributory ions are mainly carbonate, bicarbonate, chloride, sulfate,
nitrate, potassium, sodium, magnesium, and calcium. Organic material
such as plant fibers and biological solids (e.g., bacteria) are also
common constituents of surface waters. Inorganics include clay, silt,
and other soil constituents common in surface waters. These materials
are often natural contaminants resulting from the erosive action of
water flowing over surfaces. The filtering properties of soil generally
mean that suspended solids are seldom a constituent of groundwater.
Other suspended material may result from human use of the water; for
example, domestic wastewater usually contains large quantities of
suspended solids that are mostly organic in nature. Industrial use of
water may result in a wide variety of organic or inorganic suspended
impurities. Immiscible liquids such as oils and greases are often
constituents of wastewater. The solids parameter is used to evaluate and
measure all suspended and dissolved matters in water. Solids are
classified (in spite of their chemical composition) among the physical
parameters of water quality. In water, suspended material is
objectionable because it provides adsorption sites for biological and
chemical agents. These adsorption sites provide a protective barrier for
attached microorganisms against the chemical action of chlorine
disinfectants. Suspended solids in water may be degraded biologically,
resulting in objectionable byproducts. These factors make the removal of
these solids of great concern in the production of clean, safe drinking
water and wastewater effluent. In water treatment, the most effective
means of removing solids from water is by filtration; however, some
solids (including colloids and other dissolved solids) cannot be removed
by filtration. Several different tests may be performed on raw and
treated waters with regard to solids:

• Total dissolved solids (TDS) in water samples are limited to the
solids in solution. The recommended TDS upper limit is 500 mg/L. •
Settleable solids are solids in suspension that can be expected to
settle by gravity only in a quiescent state, such as is found in an
oversized settling tank. The period of time must be defined. Commonly
used in the analysis of sewage, this test may provide data useful to
evaluate the sedimentation process, but only when dealing with very high
turbidity. • Suspended solids (SS or TSS), also called suspended matter,
are solids that are not dissolved. They have little or no significance
for domestic water consumption where turbidity provides a proportional
if not equivalent value but with easier determination. • Total solids
are all of the solids contained in the water sampled, as determined by
evaporation and drying. • Volatile solids are made up of organic
chemicals. • Conductance (specific conductance) is a measure of the
electric current in the water sample carried by the ionized substances;
therefore, dissolved solids are basically related to this measure, which
is also influenced by the good conductivity of inorganic acids, bases,
and salts, as well as the poor conductivity of organic compounds. The
methods prescribed by Standard Methods (APHA-AWWA-WEF, 2017) for the
determination of solids include the following: • Total solids dried at
103°C to 105°C • Total dissolved solids dried at 180°C • Total suspended
solids dried at 103°C to 105°C • Fixed and volatile solids ignited at
550°C • Settleable solids (Imhoff cone, volumetric, gravimetric) •
Total, fixed, and volatile solids in solid or semisolid samples

pH Raw water examined for potential use as drinking water has an
expected pH value between 4 and 9, but it is more than likely that
encountered values will be between 5.5 and 8.6. What does this mean? pH
is defined as the negative log base 10 of the hydrogen ion
concentration: pH = --log10{[}H+{]} (7.2)

The pH changes one unit for every power of ten change in {[}H+{]}; for
example, water with a pH of 3 has 100 times the amount of {[}H+{]} as
that found in water with a pH of 5. Remember that, because pH =
--log10{[}H+{]}, the pH will decrease as {[}H+{]} increases. The pH of
water is controlled by the equilibrium achieved by dissolved compounds
in the system. In natural waters, the pH is primarily a function of the
carbonate system, which is composed of carbon dioxide, carbonic acid,
bicarbonate, and carbonate. Acid inputs to a water system may
substantially alter the pH. The main sources of acid include acid mine
drainage and atmospheric acid deposition.

Low-pH water may corrode distribution pipes in potable water plants. The
pipes may be costly to replace, and the corrosion may release metal ions
such as copper, lead, zinc, and cadmium into the treated drinking water.
Ingestion of heavy metals may pose substantial health risks to humans.
According to the Safe Drinking Water Act, the minimum and maximum
allowable pH range for potability is 6.5 to 8.5. Note: The role of pH in
water is also associated with corrosivity, hardness, acidity,
chlorination, coagulation, carbon dioxide stability, and alkalinity. The
pH of water affects treatment unit processes. Any change in the pH of
source water should be investigated, as pH is a relatively stable
parameter over the short term and any unusual change may indicate a
major event.

ALKALINITY Alkalinity is a measure of the ability of water to absorb
hydrogen ions without significant pH change. Simply stated, alkalinity
is a measure of the buffering capacity of water and is thus a measure of
the ability or capacity of water to neutralize acids. The major chemical
constituents of alkalinity in natural water supplies are bicarbonate,
carbonate, and hydroxyl ions. These compounds are mostly the carbonates
and bicarbonates of sodium, potassium, magnesium, and calcium. These
constituents originate from carbon dioxide (from the atmosphere and
occurring as a byproduct of microbial decomposition of organic material)
or minerals (primarily from chemical compounds dissolved from rocks and
soil). Highly alkaline water is unpalatable; however, this condition has
little known significance on human health. The principal problem with
alkaline water is the reactions that occur between alkalinity and
certain substances in the water. The resultant precipitate can foul
water system equipment. Alkalinity levels also affect the efficiency of
certain water treatment processes, especially the coagulation process.
Note: Total alkalinity is determined by titration with sulfuric acid or
other strong acids of known strength to the end point of indicators
(APHA-AWWA-WEF, 2017). The result is expressed in mg/L of equivalent
calcium carbonate.

HARDNESS Water hardness is commonly defined as the sum of the polyvalent
cations dissolved in the water. The most common cations are calcium and
magnesium, although iron, strontium, and manganese may contribute.
Hardness is usually reported as an equivalent quantity of calcium
carbonate. Generally, waters are classified according to degree of
hardness, as shown in Table 7.1. Hardness is primarily a function of the
geology of the area with which the surface water is associated. Waters
underlain

by limestone are prone to hard water because rainfall, which is
naturally acidic because it contains carbon dioxide gas, continually
dissolves the rock and carries the dissolved cations into the water
system. Standard Methods (APHA-AWWA-WEF, 2017) recommends measuring
hardness as follows:

Hardness = 2497(Ca) + 4.118(Mg) (7.3)

where total calcium (Ca) and total magnesium (Mg) are expressed in mg/L.
Hardness can also be measured using the EDTA titration method
(APHA-AWWA-WEF, 2017).

SOLUBILITY Solubility is a term often used in connection with water
treatment, laboratory analyses, and chemical and physical studies of
water, even though solubility is not a general, physical, or chemical
parameter. To understand solubility, you must also understand the
concept of a water solution, which is a homogeneous liquid comprised of
a solvent (the substance that dissolves another substance) and a solute
(the substance that dissolves in the solvent). Simply stated, solubility
is defined as the mass of substance contained in a solution that is in
equilibrium with an excess of the substance.

Chemical Drinking Water Parameters INTRODUCTION When one considers all
of the chemicals that can be in water, it is almost overwhelming to try
to comprehend the possible number of them---acrylamide, antimony,
arsenic benzene, boron bromate, lead, mercury, nickel nitrate, nitrite,
selenium, vinyl chloride. The list it is long and mind-boggling. This
chapter does not look at each organic or inorganic chemical
individually; instead, we look at general chemical parameter categories
such as dissolved oxygen (DO) organics (i.e., biochemical oxygen demand
and chemical oxygen demand), synthetic organic chemicals (SOCs),
volatile organic chemicals (VOCs), total dissolved solids (TDS),
fluoride, metals, and nutrients---the major chemical parameters of
concern.

ORGANIC CHEMICALS Natural organics contain carbon and consist of
biodegradable organic matter such as wastes from biological material
processing, human sewage, and animal feces. Microbes aerobically break
down the complex organic molecules into simpler, more stable end
products. Microbial degradation yields end products such as carbon
dioxide, water, phosphate, and nitrate. Organic particles in water may
harbor harmful bacteria and pathogens. Infection by microorganisms may
occur if the water is used for primary contact or as a raw drinking
water source. Treated drinking water will not present the same health
risks. In a potable drinking water plant, all organics should be removed
from the water before disinfection (see Chapter 11). Organic chemicals
also contain carbon; they are substances that come directly from, or are
manufactured from, plant or animal matter. Plastics provide a good
example of organic chemicals obtained from petroleum, which is derived
from plant and animal matter. Some organic chemicals released by
decaying vegetation occur naturally and by themselves tend not to pose
health problems when they get into our drinking water; however, more
serious problems are caused by the more than 100,000 different
manufactured or synthetic organic chemicals in commercial use today.
These include paints, herbicides, synthetic fertilizers, pesticides,
fuels, plastics, dyes, preservatives, flavorings, and pharmaceuticals,
to name a few. Many organic materials are soluble in water and are
toxic, and many can be found in public water supplies. The presence of
organic matter in water is troublesome. Organic matter causes: ``(1)
color formation, (2) taste and odor problems, (3) oxygen depletion in
streams, (4) interference with water treatment process, and (5) the
formation of halogenated compounds when chlorine is added to disinfect
water'' (Tchobanoglous and Schroeder, 1987). Remember that organics in
natural water systems may come from natural sources or may result from
human activities. Generally, organic matter in water comes primarily
from natural sources, including decaying leaves, weeds, and trees; the
amount of these materials present in natural waters is usually low.
Anthropogenic (manmade) sources of organic substances include pesticides
and other synthetic organic compounds. Again, many organic compounds are
soluble in water, and surface waters are more prone to contamination by
natural organic compounds than are groundwaters. In water, dissolved
organics are usually divided into two categories: biodegradable and
nonbiodegradable. Biodegradable (able to break down) material consists
of organics that can be used for food by naturally occurring
microorganisms within a reasonable length of time. Alcohols, acids,
starches, fats, proteins, esters, and aldehydes are the main
constituents of biodegradable materials. They may result from domestic
or industrial wastewater discharges, or they may be end products of the
initial microbial decomposition of plant or animal tissue. Biodegradable
organics in surface waters cause problems mainly associated with the
effects that result from the action of microorganisms. As the microbes
metabolize organic material, they consume oxygen. When this process
occurs in water, the oxygen consumed is dissolved oxygen (DO). If the
oxygen is not continually replaced in the water by artificial means, the
DO level will decrease as the organics are decomposed by the microbes.
This need for oxygen is known as the biochemical oxygen demand (BOD),
which is the amount of dissolved oxygen demanded by bacteria to break
down the organic materials during the stabilization action of the
decomposable organic matter under aerobic conditions over a 5-day
incubation period at 20°C (68°F). This bioassay test measures the oxygen
consumed by living organisms using the organic matter contained in the
sample and dissolved oxygen in the liquid. The organics are broken down
into simpler compounds, and the microbes use the energy released for
growth and reproduction. A BOD test is not required for monitoring
drinking water. Note: The more organic material in the water, the higher
the BOD exerted by the microbes will be. Note also that some
biodegradable organics can cause color, taste, and odor problems.
Nonbiodegradable organics are resistant to biological degradation. Good
examples are the constituents of woody plants, including tannic and
lignic acids, phenols, and cellulose, which are found in natural water
systems and are considered refractory (resistant to biodegradation).
Some polysaccharides with exceptionally strong bonds and benzene (for
example, associated with the refining of petroleum) with its ringed
structure are essentially nonbiodegradable. Certain nonbiodegradable
chemicals can react with oxygen dissolved in water. The chemical oxygen
demand (COD) is a more complete and accurate measurement of the total
depletion of dissolved oxygen in water. Standard Methods for the
Examination of Water and Wastewater (APHA-AWWA-WEF, 2017) defines COD as
a test that provides a measure of the oxygen equivalent of that portion
of the organic matter in a sample that is susceptible to oxidation by a
strong chemical oxidant. Note: Chemical oxygen demand (COD) is not
normally used to monitor water supplies but is often used to evaluate
contaminated raw water.

synthetic organic chemicals Synthetic organic chemicals (SOCs) are
manmade and are often toxic to humans. More than 50,000 SOCs are in
commercial production today, including common pesticides, carbon
tetrachloride, chloride, dioxin, xylene, phenols, and aldicarb.
Unfortunately, even though they are so prevalent, little information has
been collected on these toxic substances. Determining definitively just
how dangerous many of these SOCs are is rather difficult. Volatile
organic chemicals Volatile organic chemicals (VOCs) are a particularly
dangerous type of organic chemical. VOCs are absorbed through the skin
during contact with water---as in the shower or bath. Hot water exposure
allows these chemicals to evaporate rapidly, and they are harmful if
inhaled. VOCs can be found in any tap water, regardless of where one
lives and the water supply source.

TOTAL DISSOLVED SOLIDS Solids in water occur either in solution or in
suspension, and they are distinguished by passing the water sample
through a glass-fiber filter. By definition, suspended solids are
retained on top of the filter, and dissolved solids pass through the
filter with the water. When the filtered portion of the water sample is
placed in a small dish and then evaporated, the solids in the water
remain as residue in the evaporating dish. This material is referred to
as total dissolved solids (TDS). Dissolved solids may be organic or
inorganic. Water may come into contact with these substances within the
soil, on surfaces, and in the atmosphere. The organic dissolved
constituents of water come from the decay products of vegetation, from
organic chemicals, and from organic gases. Removing these dissolved
minerals, gases, and organic constituents is desirable, because they may
cause physiological effects and produce an aesthetically displeasing
color, taste, or odor. Note: In water distribution systems, high levels
of TDS indicate high conductivity with consequently higher ionization in
corrosion control; however, high TDS levels also indicate the greater
likelihood of a protective coating, a positive factor in corrosion
control.

FLUORIDE Water fluoridation prevents tooth decay primarily through
frequent, daily contact with low levels of fluoride (CDC, 2011). Even
today, with other available sources of fluoride, studies show that water
fluoridation reduces tooth decay by about 25\% over a person's lifetime;
in fact, drinking water containing a proper amount of fluoride can
reduce tooth decay as much as 65\% in children between the ages 12 and
15. It should be pointed out, however, that water fluoridation is not
necessarily the safe public health measure we have been led to believe
it is (Mullenix, 1997). Concerns about uncontrolled dosage, accumulation
in the body over time, and effects beyond the teeth (brain as well as
bones) have not been resolved, although most authorities believe that a
moderate amount of fluoride ions (F--) in drinking water contributes to
good dental health. Fluoride is seldom found in appreciable quantities
of surface waters and appears in groundwater in only a few geographical
regions, although it is sometimes found in a few types of igneous or
sedimentary rocks. Fluoride is toxic to humans in large quantities (the
key words here being ``large quantities'') and is also toxic to some
animals. Few would argue that small concentrations of fluoride (about
1.0 mg/L in drinking water) can be beneficial; however, when the
concentration of fluoride in untreated natural water supplies is
excessive, either alternative

water supplies must be used or treatment must be applied to reduce the
fluoride concentration, because excessive amounts of fluoride cause
mottled or discolored teeth, a condition called dental fluorosis. The
bottom line on fluoridation is that the widespread availability of
fluoride through water fluoridation, toothpaste, and other sources has
resulted in the steady decline of dental caries throughout the United
States.

HEAVY METALS Heavy metals are elements with atomic weights between 63.5
and 200.5 and specific gravity greater than 4.0. Living organisms
require trace amounts of some heavy metals, including cobalt, copper,
iron, manganese, molybdenum, vanadium, strontium, and zinc. Excessive
levels of essential metals, however, can be detrimental to the organism.
Non-essential heavy metals of particular concern to surface water
systems are cadmium, chromium, mercury, lead, arsenic, and antimony.
Heavy metals in water are classified as either nontoxic or toxic. Only
those metals that are harmful in relatively small amounts are labeled
toxic; other metals fall into the nontoxic group. In natural waters
(other than in groundwaters), sources of metals include dissolution from
natural deposits and discharges of domestic, agricultural, or industrial
wastes. All heavy metals exist in surface waters in colloidal,
particulate, and dissolved phases, although dissolved concentrations are
generally low. Colloidal and particulate metals may be found in
hydroxides, oxides, silicates, or sulfides or they may be adsorbed to
clay, silica, or organic matter. The soluble forms are generally ions or
unionized organometallic chelates or complexes. The solubility of trace
metals in surface waters is predominately controlled by water pH, the
type and concentration of liquids on which the metal could adsorb, and
the oxidation state of the mineral components and the redox environment
of the system. The behavior of metals in natural waters is a function of
the substrate sediment composition, the suspended sediment composition,
and the water chemistry. Sediment composed of fine sand and silt will
generally have higher levels of adsorbed metal than will quartz,
feldspar, and detrital carbonate-rich sediment. The water chemistry of
the system controls the rate of adsorption and desorption of metals to
and from sediment. Adsorption removes the metal from the water column
and stores the metal in the substrate. Desorption returns the metal to
the water column, where recirculation and bioassimilation may take
place. Metals may be desorbed from the sediment if the water experiences
increases in salinity, decreases in redox potential, or decreases in pH.
Although heavy metals such as iron and manganese do not cause health
problems, they do impart a noticeable bitter taste to drinking water,
even at very low concentrations. These metals usually occur in
groundwater in solution, and these and others may cause brown or black
stains on laundry and on plumbing fixtures.

NUTRIENTS Nitrogen, an extremely stable gas, is the primary component
(78\%) of the Earth's atmosphere. The nitrogen cycle is composed of four
processes. Three of the processes---fixation, ammonification, and
nitrification---convert gaseous nitrogen into usable chemical forms. The
fourth process, denitrification, converts fixed nitrogen back to the
unusable gaseous nitrogen state. Nitrogen occurs in many forms in the
environment and takes part in many biochemical reactions. Major sources
of nitrogen include runoff from animal feedlots, fertilizer runoff from
agricultural fields, municipal wastewater discharges, and certain
bacteria and blue--green algae that obtain nitrogen directly from the
atmosphere. Certain forms of acid rain can also contribute nitrogen to
surface waters. Nitrogen in water in the form of nitrate (NO3) indicates
that the water may be contaminated with sewage. Nitrates can also enter
the groundwater from chemical fertilizers used in agricultural areas.
Excessive nitrate concentrations in drinking water pose an immediate
health threat to infants, both human and animal, and can cause death.
The bacteria commonly found in the intestinal tract of infants can
convert nitrate to highly toxic nitrites (NO2), which can replace oxygen
in the bloodstream and result in oxygen starvation, causing a bluish
discoloration of the infant (``blue baby'' syndrome). Note: Lakes and
reservoirs usually have less than 2 mg/L of nitrate measured as
nitrogen. Higher nitrate levels are found in groundwater ranging up to
20 mg/L, but much higher values are detected in shallow aquifers
polluted by sewage or excessive use of fertilizers. Phosphorus is an
essential nutrient that contributes to the growth of algae and
eutrophication of lakes, although its presence in drinking water has
little effect on health. In aquatic environments, phosphorus is found in
the form of phosphate and is a limiting nutrient. If all phosphorus is
used up, plant growth ceases, no matter the amount of nitrogen
available. Many bodies of freshwater currently experience influxes of
nitrogen and phosphorus from outside sources. The increasing
concentration of available phosphorus allows plants to assimilate more
nitrogen before the phosphorus is depleted. If sufficient phosphorus is
available, high concentrations of nitrates will lead to phytoplankton
(algae) and macrophyte (aquatic plant) production. Major sources of
phosphorus include phosphates in detergents, fertilizer and feedlot
runoff, and municipal wastewater discharges. The USEPA's 1976
publication Quality Criteria for Water recommended a phosphorus
criterion of 0.10 µg/L (elemental) phosphorus for marine and estuarine
waters but established no freshwater criterion.

SUMMARY The biological, physical, and chemical condition of our water is
of enormous concern to us all, because we must live in such intimate
contact with water. When these parameters shift and change, these
changes affect us, often in ways science cannot yet define for us. Water
pollution is an external element that can and does significantly affect
our water, but what exactly is water pollution? We quickly learn that
the sources of water pollution do not always travel a direct path to
water. Controlling what enters our water is difficult, because the
hydrologic cycle carries water (and whatever it picks up along the way)
through all of our environment's media, thus affecting the biological,
physical, and chemical condition of the water we must drink to live.
Water pollution is discussed further in Chapter 9.

INTRODUCTION Is drinking water contamination really a problem---a
serious problem? The answer to the first part of the question depends on
where your water comes from. As to the second part of the question, the
reader is referred to a book (or the film based on the book) that
concerns a case of toxic contamination that might be familiar---A Civil
Action, by Jonathan Harr. The book and film portray the legal
repercussions connected with polluted water supplies in Woburn,
Massachusetts. Two wells became polluted with industrial solvents,
apparently causing 24 of the town's children who lived in neighborhoods
supplied by those wells to contract leukemia and die over a span of 12
years---a rate several times the national average for a community of its
size. The families involved sued two companies for dumping toxic waste.
Their attorney, Jan Schlichtmann, emphasized that Woburn was only one
example of an underlying pathology that threatens many other
communities. Many who have read the book or have seen the movie may
mistakenly get the notion that Woburn, a toxic ``hot spot,'' is a rare
occurrence. Nothing could be further from the truth. Toxic ``hot spots''
abound. Most striking are areas of cancer clusters, a short list of
which includes not only Woburn but also Storrs, Connecticut, where wells
polluted by a landfill are suspected of sickening and killing residents
in nearby homes. In Bellingham, Washington, pesticide-contaminated
drinking water is thought to be linked to a sixfold increase in
childhood cancers. Cancer is now the primary cause of childhood death
from disease. Drinking water contamination is a problem---a very serious
problem. In this chapter, we discuss a wide range of water contaminants,
the sources of these contaminants, and their impact on drinking water
supplies from both surface water and groundwater sources. Moreover,
before moving on to an in-depth discussion of drinking water pollution
and many of the contaminants involved, it is appropriate to highlight
two drinking water crises that have occurred in the United States: the
Gold King Mine spill (USEPA, 2015a,b) and the lead-contaminated drinking
water in Flint, Michigan (Hanna-Attisha et al., 2015; United Way, 2016).
GOLD KING MINE SPILL On August 5, 2014, the U.S. Environmental
Protection Agency (USEPA) conducted a mine site investigation of the
abandoned Gold King Mine above the old adit (a mine tunnel) to

• Assess the ongoing water releases from the mine. • Treat mine water. •
Assess the feasibility of further mine remediation.

During the excavation required for the investigation the heavy equipment
disturbed loose material around a soil ``plug'' at the mine entrance,
spilling about 3 million gallons of pressurized water stored behind the
collapsed material into Cement Creek, a tributary of the Animas River.
The spill volume associated with the release on August 5 was calculated
to be approximately 3 million gallons based on flow rates. Discharge
rates from the mine as of November 5, 2015, averaged around 600 gallons
per minute. It is important to point out, for context, that multiple
mines are located along the upper Animas, and historically there have
been considerable discharges at each mine site. The Red and Bonita
mines, just below the Gold King Mine, currently discharge about 300
gallons per minute. One of the most striking effects of the Gold King
Mine spill was the color change clearly visible in Cement Creek and
Animas River and to a lesser degree downstream almost to the San Juan
River. The iron from the acid mine drainage precipitated out into the
water as a result of the rise in pH, turning it yellow. Old-time
goldpanners and other sluice miners refer to the resulting red, orange,
and yellow solids as ``yellow boy.'' Typically, as more water is mixed
in (dilution is the solution to pollution, according that mythical hero
Hercules, who arguably might have been the world's first environmental
engineer), the iron and other metals become even more dilute or get
attached to sediments, causing them to drop out of the water, sink, and
settle into river bottom sediments. The water color then returns to
normal. But, and this is the gist of this text, what appears normal in
surface water bodies may not actually be normal because, as in the case
of the Animas River and thousands of other polluted streams, what we are
able to see at the surface does not in any way certify the quality of
the water that is contained within the water body. ``Momma, this water
is dirty!'' ``Momma, this water is smelly!'' ``Momma, this water tastes
yucky!'' ``Momma, I drank the water and now I feel icky!'' Does this
refrain sound at all familiar to you? Is it something you've ever heard
a child utter? In modern countries, such as the United States, has this
refrain ever even been spoken? Well, let's have a conversation about
this issue. Most likely, this refrain does not sound familiar to you.
It's possible that you've heard it, but you may not have heard it spoken
in English. Such complaints may be spoken in any of many different
languages and too often are heard in underdeveloped nations.
Underdeveloped nation? What exactly is an underdeveloped nation? In
keeping with Voltaire and his famous saying, ``If you wish to converse
with me, please define your terms,'' so, for our purposes here, let's
define the term as a nation that, because of several possible
conditions, is lacking access to job opportunities, food, healthcare,
education, housing, and---most importantly in the real world and in this
book---safe drinking water (the key word being safe). It has been
estimated that 780 million people do not have access to an improved
drinking water source and that over 800,000 children younger than 5
years of age perish each year from diarrhea, mostly in underdeveloped
countries. But, what is an improved drinking water source? The

FIGURE 9.1 Rainwater collection. (Illustration by F.R. Spellman and K.
Welsh.)

World Health Organization/UNICEF Joint Monitoring Programme for Water
Supply and Sanitation has defined ``improved'' water sources as follows
(WHO/ UNICEF, 2017):

• Piped water into a dwelling; water can be drawn from a common
household tap • Piped water into a yard/plot • Public taps/standpipes •
Tubewells/boreholes • Protected dug wells • Protected springs •
Rainwater collection (see Figure 9.1) • Bottled water, if the secondary
source used by the household for cooking and personal hygiene is
improved

Water sources that are not considered ``improved'' include the
following:

• Unprotected dug wells • Unprotected springs • Vendor-provided water •
Cart with a small tank/drum • Bottled water, if the secondary source
used by the household for cooking and personal hygiene is unimproved •
Tanker-truck • Surface water

It should be clear now as to what an improved drinking water source is
and what an unimproved source is. So, getting back to our earlier
question, are complaints about drinking water heard only in
underdeveloped countries? Unfortunately, no. They can be heard even in
this country, depending on geographical location and personal
circumstance. Your experience with the tap water from your own kitchen
sink is probably okay, but consider the residents of Flint, Michigan,
for example. Many readers are no doubt familiar with the Flint,
Michigan, fiasco, but for those of you who are not, let's summarize. In
April 2014, Flint changed its water source from treated Detroit Water
and Sewerage Department water, which was sourced from the Detroit River
and Lake Huron, to water from the Flint River, a very corrosive water
source. The city experienced a series of problems with the new drinking
water source that culminated in the discovery of lead contamination, a
grave public health hazard. Flint River water that was treated
improperly (or not treated at all) caused lead, scale, and who knows
what else that had built up in the aging pipes (see Figure 9.2) to leach
into the water supply, resulting in extremely elevated levels of lead, a
heavy metal neurotoxin. It is estimated that, in Flint, somewhere
between 6000 and 12,000 children have been exposed to tap water, a
supposedly improved water source, with high levels of lead. These lead
levels were high enough to cause a range of serious health issues
(United Way, 2016). Due to the change in water source, the percentage of
Flint children with elevated blood lead levels may have raised from
about 2.5\% in 2013 to as much as 5\% in 2015 (Hanna-Attisha et al.,
2015). Some speculate that the water change may have caused an outbreak
of Legionnaires' disease in the county that killed 10 people and made
more than 70 people ill. The experience of Flint's lead-contaminated
water supply suggests that children's complaints about the water they
are drinking could actually be heard here in the United States. This, of
course, raises other questions. Are there other, as yet undiscovered
cities in the United States distributing contaminated water? Without a
doubt, due to this country's aging infrastructure. Many water
distribution systems are more than 100 years old and in dire need of
upgrading. The main problem is money. Replacing old and deteriorating
distribution systems is not cheap. When city managers, city councils,
and utility directors are advised that they need to upgrade their water
system to the tune of millions of dollars, they immediately ask, ``Where
would we get the money to do that?'' If they contemplate raising funds
by making rate payers pay the bill, their political careers or appointed
positions will quickly be over. Well, how about the states and federal
government? Can't they come up with the funds needed to upgrade such
critical infrastructure? Again, though, their answer is, ``Where would
we get the money to do that?'' Unfortunately, no action can be expected
until a Flint-type event occurs, one that has a highly emotional impact
because of potential damage to children and others. The press would have
to be all over the situation, and the politicians would have to point
fingers here, there, and everywhere (except toward themselves). Funding
that is difficult or impossible to find to retrofit aging municipal
water distribution systems is one thing, but allowing what occurred in
Flint is an entirely different matter. After my years of teaching
undergraduateand graduate-level environmental health courses, including
waterworks operation, as well as short courses in water and wastewater
treatment at Virginia Tech for operators and for those seeking licensure
as operators, I was totally surprised by what happened in Flint,
Michigan. Water operators, water administrators, waterworks managers,
waterworks environmental engineers, and other waterworks personnel know
that incoming water must be sampled and tested for quality. Moreover,
during the treatment of water as it flows from unit process to unit
process, it must be sampled and tested. At the completion of treatment
and prior to discharge to the distribution system the treated water must
be sampled and treated again. This sequence of sampling and treating is
not just nice to do; it is not just best practices---it is the absolute
law of the land, and there is a distinct moral obligation to provide the
best and safest drinking water possible. What exactly occurred at Flint
may never be fully revealed; however, one thing is absolutely
certain---people did not do what they were trained and obligated to do.
They did not deliver safe drinking water to the consumer. And, because
of their incompetence, violation of law, or just plan stupidity or
immorality, we are now likely to hear, ``Get the lead out of the water,
please!''

SOURCES OF CONTAMINANTS If one were to list all of the sources of
contaminants and the contaminants themselves (the ones that can and do
foul our water supply systems), along with a brief description of each
contaminant, that list would be long enough to fill a book. To give the
reader some idea of the magnitude of the problem, following is a very
condensed list of selected sources and contaminants (the ``short
list''). Note: Keep in mind that when we specify ``water pollutants'' we
are in most cases speaking about pollutants that somehow get into the
water (by whatever means) due to the interactions of the other two
environmental mediums: air and soil. Probably the best example of this
is the acid rain phenomenon. Pollutants originally emitted only into the
atmosphere land on Earth and affect both soil and water. Consider that
69\% of the anthropogenic lead and 73\% of the mercury in Lake Superior
have reached it by atmospheric deposition (Hill, 1997).

• Subsurface percolation---Hydrocarbons, metals, nitrates, phosphates,
microorganisms, cleaning agents (e.g., trichloroethylene, or TCE) •
Injection wells---Hydrocarbons, metals, non-metal inorganics, organic
and inorganic acids, organics, microorganisms, radionuclides • Land
application---Nitrogen, phosphorus, heavy metals, hydrocarbons,
microorganisms, radionuclides • Landfills---Organics, inorganics,
microorganisms, radionuclides • Open dumps---Organics, inorganics,
microorganisms • Residential (local) disposal---Organic chemicals,
metals, non-metal inorganics, inorganic acids, microorganisms • Surface
impoundments---Organic chemicals, metals, non-metal inorganics,
inorganic acids, microorganisms, radionuclides • Waste mine
tailings---Arsenic, sulfuric acid, copper, selenium, molybdenum,
uranium, thorium, radium, lead, manganese, vanadium • Waste
piles---Arsenic, sulfuric acid, copper, selenium, molybdenum, uranium,
thorium, radium, lead, manganese, vanadium • Materials
stockpiles---Aluminum, iron, calcium, manganese, sulfur, and traces of
arsenic, cadmium, mercury, lead, zinc, uranium, and copper (coal piles);
metals and non-metals; microorganisms (other materials piles) •
Graveyards---Metals, non-metals, microorganisms • Animal
burial---Site-specific contamination, depending on disposal practices
(e.g., surface or subsurface), hydrology, proximity of the site to water
sources, type and amount of disposed material, cause of death •
Aboveground storage tanks---Organics, metal and non-metal inorganics,
inorganic acids, microorganisms, radionuclides • Underground storage
tanks---Organics, metal, inorganic acids, microorganisms, radionuclides
• Containers---Organics, metal and non-metal inorganics, inorganic
acids, microorganisms, radionuclides • Open burning and detonating
sites---Inorganics (including heavy metals), organics (including TNT) •
Radioactive waste disposal sites---Radioactive cesium, plutonium,
strontium, cobalt, radium, thorium, uranium • Pipelines---Organics,
metals, inorganic acids, microorganisms • Material transport and
transfer operations---Organics, metals, inorganic acids, microorganisms,
radionuclides • Irrigation practices---Fertilizers, pesticides,
naturally occurring contamination, sediments • Pesticide
applications---Estimated at 1200 to 1400 active ingredients, including
alachlor, aldicarb, atrazine, bromacil, carbofuran, cyanazine,
dibromochloropropane (DBCP), dimethyl tetrachloroterephthalate (DCPA),
1,2-dichloropropane, dyfonate, ethylene dibromide (EDB), metolachlor,

metribyzen, oxalyl, siazine, 1,2,3-trichloropropane (the extent of
groundwater contamination cannot be determined with current data) •
Animal feeding operations---Nitrogen, bacteria, viruses, phosphates •
Deicing salts applications---Chromate, phosphate, ferric ferocyanide,
Na-ferrocyan, chlorine • Urban runoff---Suspended solids and toxic
substances, especially heavy metals and hydrocarbons, bacteria,
nutrients, petroleum residues • Percolation of atmospheric
pollutants---Sulfur and nitrogen compounds, asbestos, heavy metals •
Mining and mine drainage---Acids, toxic inorganics (heavy metals),
nutrients (coal); radium, uranium, fluorides (phosphate); sulfuric acid,
lead, cadmium, arsenic, sulfur, cyan (metallic ores) • Production
wells---Oil wells (1.2 million abandoned production wells); farm
irrigation wells; installation, operation, and plugging of all wells •
Construction excavation---Pesticides, diesel fuel, oil, salt, various
others • Pharmaceuticals and personal care products (PPCPs)---Any
product used by individual for personal health or cosmetic reason or
used by agribusiness to enhance growth or health of livestockNote:
Before we discuss specific water pollutants, we must examine several
terms important to the understanding of water pollution. One of these is
point source. The USEPA defines a point source as ``any single
identifiable source of pollution from which pollutants are discharged,
e.g., a pipe, ditch, ship, or factory smokestack.'' For example, the
outlet pipes of an industrial facility or a municipal wastewater
treatment plant are point sources. In contrast, non-point sources are
widely dispersed sources and are a major cause of stream pollution. An
example of a

non-point source of pollution is rainwater carrying topsoil and chemical
contaminants into a river or stream. Some of the major sources of
non-point source pollution include water runoff from farming, urban
areas, forestry, and construction activities. The word runoff signals a
non-point source that originated on land. Runoff may carry a variety of
toxic substances and nutrients, as well as bacteria and viruses with it.
Non-point sources now comprise the largest source of water pollution,
contributing approximately 65\% of the contamination in qualityimpaired
streams and lakes.

RADIONUCLIDES When radioactive elements decay, they emit alpha, beta, or
gamma radiations caused by transformation of the nuclei to lower energy
states. In drinking water, radioactivity can be from natural or
artificial radionuclides (the radioactive metals and minerals that cause
contamination). These radioactive substances in water are of two types:
radioactive minerals and radioactive gas. The U.S. Environmental
Protection Agency (USEPA) reports that some 50 million Americans face
increased cancer risk because of radioactive contamination of their
drinking water. Because of their occurrence in drinking water and their
effects on human health, the natural radionuclides of chief concern are
radium-226, radium-228, radon-222, and uranium. The source of some of
these naturally occurring radioactive minerals is typically associated
with certain regions of the country where mining is active or was active
in the past. Mining activities expose rock strata, most of which contain
some amount of radioactive ore. Uranium mining, for example, produces
runoff. Radioactive contamination also occurs when underground streams
flow through various rockbed and geologic formations containing
radioactive materials. Other sources of radioactive minerals that may
enter water supplies are smelters and coalfired electrical generating
plants. Also contributing to radioactive contamination of water are
nuclear power plants, nuclear weapons facilities, radioactive materials
disposal sites, and mooring sites for nuclear-powered ships. Hospitals
contribute radioactive pollution when they dump low-level radioactive
wastes into sewers; some of these radioactive wastes eventually find
their way into water supply systems. Although radioactive minerals such
as uranium and radium in water may present a health hazard in particular
areas, a far more dangerous threat exists in the form of radon. Radon is
a colorless, odorless gas created by the natural decay of minerals in
the soil. Normally present in all water in minute amounts, radon is
especially concentrated in water that has passed through rock strata of
granite, uranium, or shale. Radon enters homes from the soil beneath
through cracks in the foundation or through crawl spaces and unfinished
basements, as well as in tainted water. Radon is considered to be the
second leading cause of lung cancer in the United States (about 20,000
cases each year), second only to cigarette smoking. Contrary to popular
belief, radon is not a threat from surface water (lakes, rivers, or
above-ground reservoirs), because radon dissipates rapidly when water is
exposed to air. Even if the water source is groundwater, radon is still
not a threat if the water is exposed to air (aerated) or if it is
processed through an open tank during treatment. Studies have shown that
where high concentrations of radon are detected within the air in a
house most of that radon has come through the foundation and from the
water; however, hot water used for showers, baths, or cooking (hot
water) can release high concentrations of radon into the air. Still,
radon is primarily a threat from groundwater taken directly from an
underground source---either a private well or from a public water supply
whose treatment of the water does not include exposure to air. Because
radon in water evaporates quickly into the air, the primary danger is
from inhaling it, not from drinking it.

THE CHEMICAL COCKTAIL If we were to take the time to hold a full glass
of water and inspect the contents, we might find that the contents
appear cloudy or colored, making us think that the water is not fit to
drink. Or, the contents might look fine but an odor of chlorine is
prevalent. Most often, though, we simply draw water from the tap and
either drink it or use it to cook dinner. The fact is that typically a
glass of treated water is a chemical cocktail (Kay, 1996). Water
utilities in communities seek to protect the public health by treating
raw water with certain chemicals; what they are in essence doing is
providing a drinking water product that is a mixture of various
treatment chemicals and their byproducts. Water treatment facilities
typically add chlorine to disinfect, but chlorine can produce
contaminants. Another concoction is formed when ammonia is added for
disinfection. Alum and polymers are added to the water to settle out
various contaminants. The water distribution system and appurtenances
must be protected from pipe corrosion, so the water treatment facility
adds caustic soda, ferric chloride, and lime, which in turn increase the
aluminum, sulfates, and salts in the water. Thus, when we hold that
glass of water before us and we perceive a full glass of crystal clear,
refreshing water, what we are really seeing is a concoction of many
chemicals mixed with water, forming the chemical cocktail. The most
common chemical additives used in water treatment are fluorides,
chlorine, and flocculants. Because fluorides have already been
discussed, the discussion in the following sections focuses on the
byproducts of chlorine and flocculant additives.

byproDucts of chlorine To lessen the potential impact of that water
cocktail, the biggest challenge today is to make sure that the old
standby, chlorine, will not produce as many new contaminants as it
destroys. At the present time, arguing against the use of chlorine is
difficult. Since 1908, chlorine has been used in the United States to
kill off microorganisms that spread cholera, typhoid fever, and other
waterborne diseases. In the 1970s, however, scientists discovered that,
although chlorine does not seem to cause cancer in lab animals, when
used in the water treatment process it can create a long list of
byproducts that do. The byproducts of chlorine that present the biggest
health concern are organic hydrocarbons, known as trihalomethanes, which
are usually discussed as total trihalomethanes (TTHMs). The USEPA
classifies three of these trihalomethane byproducts---chloroform,
bromoform, and bromodichloromethane---as probable human carcinogens. The
fourth, dibromochloromethane, is classified as a possible human
carcinogen. The USEPA set the first trihalomethane limits in 1979. Most
water companies met these standards initially, but the standards were
tightened after passage of the 1996 Safe

Drinking Water Act (SDWA) amendments. The USEPA is continuously studying
the need to regulate other cancer-causing contaminants, including
haloacetic acids (HAAs), also produced by chlorination. Most people
concerned with protecting public health applaud the USEPA's efforts in
regulating water additives and disinfection byproducts; however, some
people involved in the water treatment and supply business have
expressed concern. A common concern often heard from water utilities
having a tough time balancing the use of chlorine without going over the
regulated limits revolves around the necessity of meeting regulatory
requirements by lowering chlorine amounts to meet byproducts standards
and at the same ensuring that all the pathogenic microorganisms are
killed off. Many make the strong argument that, although no proven case
exists that disinfection byproducts cause cancer in humans, many
cases---an extensive history of cases---show that if we do not
chlorinate water, then people get sick and sometimes die from waterborne
disease. Because chlorination is now prompting regulatory pressure and
compliance with new, demanding regulations, many water treatment
facilities are looking for other options. Choosing an alternative
disinfection chemical process is feeding a growing enterprise. One
alternative that is currently being given widespread consideration in
the United States is ozonation, which uses ozone gas to kill
microorganisms. Ozonation is Europe's preferred method, and it does not
produce trihalomethanes, but the USEPA does not yet recommend a
wholesale switchover to ozone to replace chlorine or chlorination
systems utilizing sodium hypochlorite or calcium hypochlorite. The USEPA
points out that ozone also has problems, as it does not produce a
residual disinfectant in the water distribution system, it is much more
expensive, and in salty water it can produce another carcinogen,
bromate. We discuss disinfection alternatives in greater detail in
Chapter 11. At the present, what drinking water practitioners are doing
(in the real world) is attempting to fine-tune water treatment. What it
all boils down to is a delicate balancing act. Drinking water
professionals do not want to cut back on disinfection; if anything, they
would prefer to strengthen it. So, we have to ask how we can bring into
parity the microbial risks versus the chemical risks. How can both risks
be reduced to an acceptable level? Unfortunately, no one is quite sure
how to do this. The problem really revolves around the enigma associated
with a ``we don't know what we don't know'' scenario. The disinfection
byproducts problem stems from the fact that most U.S. water systems
produce the unwanted byproducts when the chlorine reacts to decayed
organics: vegetation and other carbon-containing materials in water.
Communities that take drinking water from lakes and rivers have a
tougher time keeping the chlorine byproducts out of the tap than those
that use clean groundwater. When a lot of debris is in the reservoir, a
water utility may switch to alternative sources, such as wells. In other
facilities, chlorine is combined with ammonia in a disinfection method
called chloramination. This method is not as potent as pure
chlorination, but it does prevent the production of unwanted
trihalomethanes. In communities where rains wash leaves, grasses, and
trees into the local water source (such as a lake or river), hot summer
days can trigger algae blooms, upping the organic matter that can
produce trihalomethanes. Spring runoff in many communities

exacerbates the problem. With increased runoff comes agricultural waste,
pesticides, and quantities of growth falling into the water that must be
dealt with. Nature's conditions in summer diminish some precursors for
trihalomethanes---the bromides in salty water. Under such conditions,
usually nothing unusual is visible in the drinking water; however, water
that is cloudy due to silt (dissolved organics from decayed plants)
could harbor trihalomethanes. Most cities today strain out the organics
from their water supplies before chlorinating to prevent the formation
of trihalomethanes and haloacetic acids. In other communities, the move
is on to switch from chlorine to ozone and other disinfectant methods.
The National Resources Defense Council (NRDC, 2003) suggested that
eventually most U.S. systems will catch up with Europe in using ozone to
kill resistant microbes such as Cryptosporidium. When this method is
employed, the finishing touch is usually accomplished by filtering the
water through granular activated carbon, which increases the cost for
consumers (estimated at about \$100 or more per year per hookup).

Disinfection byproDuct regulations* A major challenge for drinking water
practitioners is how to balance the risks from microbial pathogens and
disinfection byproducts. Providing protection from these microbial
pathogens while simultaneously ensuring decreasing health risks to the
population from disinfection byproducts (DBPs) is important. The Safe
Drinking Water Act (SDWA) amendments, signed by President Clinton in
1996, required the USEPA to develop rules to achieve these goals. The
Stage 1 Disinfectants and Disinfection Byproducts Rule and the Interim
Enhanced Surface Water Treatment Rule are the first of a set of rules
known as the Microbial and Disinfection Byproduct (MDBP) Rules. These
new rules were the product of 6 years of collaboration among the water
industry, environmental and public health groups, and local, state, and
federal government. The schedule for implementing the MDBP Rules was
established as follows:

• 1998---Final Rule, Interim Enhanced Surface Water Treatment Rule and
Stage 1 Disinfectants and Disinfection Byproducts Rule (DBPR) •
2001---Final Rule, Filter Backwash Recycling Rule and Final Rule,
LongTerm 1 Enhanced Surface Water Treatment Rule • 2002---Final Rule,
Ground Water Rule and Final Rule, Long-Term 2 Enhanced Surface Water
Treatment Rule and Stage 2 Disinfectants and Disinfection Byproduct Rule
(DBPR) • 2006---Under Stage 2 DBPR, community water systems serving at
least 100,000 must submit initial distribution system evaluation (IDSE)
monitoring plans, system-specific study plans, or 40/30 certification •
2009---Under Stage 2 DBPR, community water systems serving 50,000 to
99,000 must submit initial distribution system evaluation (IDSE)
monitoring plans, system-specific study plans, or 40/30 certification

\begin{itemize}
\item
  Adapted from USEPA, Drinking Water Priority Rulemaking: Microbial and
  Disinfection Byproducts Rules, EPA 816-F-01-012, U.S. Environmental
  Protection Agency, Washington, DC, 2001.
\end{itemize}

Other pertinent Stage 1 and 2 DBPR rules can be found in applicable
USEPA regulations; see
http://water.epa.gov/lawsregs/rulesregs/sdwa/stage2/regs\_factsheet.cfm.

Public Health Concerns Most Americans drink tap water that meets all
existing health standards all the time. These rules were designed to
further strengthen existing drinking water standards and thus increase
protection for many water systems. The USEPA's Science Advisory Board
concluded in 1990 that exposure to microbial contaminants such as
bacteria, viruses, and protozoa (e.g., Giardia lamblia, Cryptosporidium)
was likely the greatest remaining health risk management challenge for
drinking water suppliers. Acute health effects from exposure to
microbial pathogens are documented, and associated illness can range
from mild, to moderate cases lasting only a few days, to more severe
infections that can last several weeks and may result in death for those
with weakened immune systems. Disinfectants are effective in controlling
many microorganisms, but they react with natural organic and inorganic
matter in source water and distribution systems to form potential DBPs,
many of which have been shown to cause cancer and reproductive and
developmental effects in laboratory animals. More than 200 million
people consume water that has been disinfected. Because of the large
population exposed, health risks associated with DBPs, even if small,
need to be taken seriously.

Existing Regulations • Microbial contaminants---The 1989 Surface Water
Treatment Rule applies to all public water systems using surface water
sources or groundwater sources under the direct influence of surface
water. It establishes maximum contaminant level goals (MCLGs) for
viruses, bacteria, and Giardia lamblia. It also addresses treatment
technique requirements for filtered and unfiltered systems specifically
designed to protect against the adverse health effects of exposure to
these microbial pathogens. The Total Coliform Rule, revised in 1989,
applies to all public water systems and establishes a maximum
contaminant level (MCL) for total coliforms. • Disinfection
byproducts---In 1979, the USEPA set an interim MCL for total
trihalomethanes of 0.10 mg/L as an annual average. This applies to any
community water system serving at least 10,000 people that adds a
disinfectant to the drinking water during any part of the treatment
process. In 1998, the USEPA established the Stage 1 Disinfectants and
Disinfection Byproducts Rule, which required public water systems to use
treatment measures to reduce the formation of disinfection byproducts
and to meet the following specific standards: • Information Collection
Rule---To support the MDBP rulemaking process, the Information
Collection Rule establishes monitoring and data reporting requirements
for large public water systems serving at least 100,000 people. This
rule was intended to provide the USEPA with information on the
occurrence in drinking water of microbial pathogens and DBPs. The USEPA
has collected engineering data on how public water systems currently
control such contaminants as part of the Information Collection Rule.

Interim Enhanced Surface Water Treatment Rule and Stage 1 Disinfectants
and Disinfection Byproducts Rule The USEPA finalized the Interim
Enhanced Surface Water Treatment Rule and Stage 1 Disinfectants and
Disinfection Byproducts Rule in 1998, as required by the 1996 amendments
to the Safe Drinking Water Act, Section 1412(b)(2)(C). The final rules
resulted from formal regulatory negotiations with a wide range of
stakeholders that took place from 1992 to 1993 and in 1997.

Interim Enhanced Surface Water Treatment Rule The Interim Enhanced
Surface Water Treatment Rule applies to systems using surface water or
groundwater under the direct influence of surface water that serve
10,000 or more persons. The rule also includes provisions for states to
conduct sanitary surveys for surface water systems regardless of system
size. The rule built upon the treatment technique requirements of the
Surface Water Treatment Rule with the following key additions and
modifications:

• Maximum contaminant level goal (MCLG) of zero for Cryptosporidium •
2-log Cryptosporidium removal requirements for systems that filter •
Strengthened combined filter effluent turbidity performance standards •
Individual filter turbidity monitoring provisions • Disinfection
profiling and benchmarking provisions • Systems using groundwater under
the direct influence of surface water now subject to the new rules
dealing with Cryptosporidium • Inclusion of Cryptosporidium in the
watershed control requirements for unfiltered public water systems •
Requirements for covers on new finished water reservoirs • Sanitary
surveys, conducted by states, for all surface water systems regardless
of size

The Interim Enhanced Surface Water Treatment Rule, with tightened
turbidity performance criteria and required individual filter
monitoring, was designed to optimize treatment reliability and to
enhance physical removal efficiencies to minimize the Cryptosporidium
levels in finished water. The rule also includes disinfection benchmark
provisions to ensure continued levels of microbial protection while
facilities take the necessary steps to comply with DBP standards. Stage
1 Disinfectants and Disinfection Byproducts Rule The final Stage 1
Disinfectants and Disinfection Byproducts Rule applies to community
water systems and nontransient, noncommunity systems (including those
serving fewer than 10,000 people) that add a disinfectant to the
drinking water during any part of the treatment process. The final Stage
1 Disinfectants and Disinfection Byproducts Rule includes the following
key provisions:

• Maximum residual disinfectant levels (MRDLs) for chlorine (4 mg/L),
chloramines (4 mg/L), and chlorine dioxide (0.8 mg/L) • Maximum
contaminant level goals (MCLGs) for four trihalomethanes (chloroform, 0;
bromodichloromethane, 0; dibromochloromethane, 0.06 mg/L; bromoform, 0);
two haloacetic acids (dichloroacetic acid, 0; trichloroacetic acid, 0.3
mg/L); bromate (0); and chlorite (0.8 mg/L) • Maximum residual
disinfectant levels (MRDLs) for three disinfectants (chlorine, 4.0 mg/L;
chloramines, 4.0 mg/L; chlorine dioxide, 0.8 mg/L) • Maximum contaminant
levels (MCLs) for total trihalomethanes, a sum of the four listed above
(0.080 mg/L); haloacetic acids (HAA5) (0.060 mg/L), a sum of the two
listed above plus monochloroacteric acid and monoand dibromoacetic
acids; and two inorganic disinfection byproducts (chlorite, 1.0 mg/L;
bromate, 0.010 mg/L) • A treatment technique for removal of DBP
precursor material

The term maximum residual disinfectant level (MRDL) (not included in the
SDWA) was created during the negotiations to distinguish disinfectants
(because of their beneficial use) from contaminants. The final rule
includes monitoring, reporting, and public notification requirements for
these compounds. This final rule also describes the best available
technology (BAT) upon which the MRDLs and MCLs are based.

Subsequent Rules Long-Term 1 Enhanced Surface Water Treatment Rule
Whereas the Stage 1 Disinfectants and Disinfection Byproducts Rule
applied to systems of all sizes, the Interim Enhanced Surface Water
Treatment Rule applied only to systems serving 10,000 or more people.
The Long-Term 1 Enhanced Surface Water Treatment Rule strengthened
microbial controls for small systems (i.e., those systems serving fewer
than 10,000 people). The rule also prevents significant increases in
microbial risk where small systems take steps to implement the Stage 1
Disinfectants and Disinfection Byproducts Rule. The USEPA believes that
the rule will generally track the approaches in the Interim Enhanced
Surface Water Treatment Rule for improved turbidity control, including
individual filter monitoring and reporting. The rule also addresses
disinfection profiling and benchmarking. The USEPA is considering what
modifications of some large system requirements may be appropriate for
small systems. Long-Term 2 Enhanced Surface Water Treatment Rule The
1996 SDWA amendments required the USEPA to finalize a Stage 2
Disinfectants and Disinfection Byproducts Rule by 2002. Although the
amendments did not require the USEPA to finalize a Long-Term 2 Enhanced
Surface Water Treatment Rule along with the Stage 2 Disinfectants and
Disinfection Byproducts Rule, the USEPA believed that finalizing these
rules together to ensure a proper balance between microbial and DBP
risks was important. The intent of the rules is to provide additional
public health protection (if needed) from DBPs and microbial pathogens.
Ground Water Rule The USEPA published the Ground Water Rule in the
Federal Register on November 8, 2006. The purpose of the rule is to
provide for increased protection against microbial pathogens in public
water systems that use groundwater sources. The USEPA is particularly
concerned about groundwater systems that are susceptible to fecal
contamination because disease-causing pathogens may be found in fecal
contamination. The Ground Water Rule specifies the appropriate use of
disinfection and, equally importantly, addresses other components of
groundwater systems to ensure public health protection. More than
158,000 public or community water systems serve almost 89 million people
through groundwater systems. Of these, 99\% serve fewer than 10,000
people; however, systems serving more than 10,000 people serve 55\%
(more than 60 million) of all people who get their drinking water from
public groundwater systems. Filter Backwash Recycling Rule The 1996 SDWA
amendments required the USEPA to establish a standard on recycling
filter backwash within the treatment process of public water systems by
August 2000. The actual Filter Backwash Recycling Rule (FBRR) was
implemented in 2001. The purpose of the FBRR is to improve public health
protection by assessing and changing, where needed, recycle practices
for improved contaminant control, particularly microbial contaminants.
Generally, the FBRR requires systems that recycle to return specific
recycle flows though all processes of the system's existing conventional
or direct filtration system or at an alternate location approved by the
state. The FBRR applies to public water systems that use surface water
or groundwater under the direct influence of surface water, practice
conventional or direct filtration, and recycle spent filter backwash,
thickener supernatant, or liquids from dewatering processes. Note: This
section has provided only a summary of federal drinking water
requirements; to ensure full compliance, consult the federal regulations
at 40 CFR 141 and any approved state requirements.

Opportunities for Public Involvement The USEPA encourages public input
into regulation development. Public meetings and opportunities for
public comment on MDBP rules are announced in the Federal Register. The
USEPA's Office of Groundwater and Drinking Water also provides
information regarding the MDBP rule and other programs online
(www.epa.gov/ safewater/standards.html). flocculants In addition to
chlorine and sometimes fluoride, water treatment plants often add
several other chemicals, including flocculants, to improve the
efficiency of the treatment process---and they all add to the cocktail
mix. Flocculants are chemical substances added to water to make
particles clump together, which improves the effectiveness of
filtration. Some of the most common flocculants are polyelectrolytes
(polymers)--- chemicals with constituents that cause cancer and birth
defects and are banned for use by several countries. Although the USEPA
classifies them as ``probable human carcinogens,'' it still allows their
continued use. Acrylamide and epichlorohydrin are two flocculants used
in the United States that are known to be associated with probable
cancer risk (Lewis, 1996).

GROUNDWATER CONTAMINATION Groundwater under the direct influence of
surface water comes under the same monitoring regulations as does
surface water (i.e., all water open to the atmosphere and subject to
surface runoff). The legal definition of groundwater under the direct
influence of surface water is any water beneath the surface of the
ground with (1) significant occurrence of insects or microorganisms,
algae, or large-diameter pathogens such as Giardia lamblia; or (2)
significant and relatively rapid shifts in water characteristics such as
turbidity, temperature, conductivity, or pH, which closely correlate to
climatological or surface water conditions. Direct influence must be
determined for individual sources in accordance with criteria
established by the state. The state determines for individual sources in
accordance with criteria established by the state, and that
determination may be based on site-specific measurements of water
quality and/or documentation of well construction characteristics and
geology with field evaluation. Generally, most groundwater supplies in
the United States are of good quality and produce essential quantities.
The full magnitude of groundwater contamination in the United States is,
however, not fully documented, and federal, state, and local efforts
continue to assess and address the problems. Groundwater supplies about
25\% of the freshwater used for all purposes in the United States,
including irrigation, industrial uses, and drinking water (about 50\% of
the U.S. population relies on groundwater for drinking water).
Elsewhere, groundwater aquifers beneath or close to Mexico City have
provided the area with as much as 3 billion liters of water per day
(Chilton, 1998). But, as groundwater pumping increases to meet water
demand, it can exceed the aquifer rate of replenishment, and in many
urban aquifers water levels are showing a long-term decline. Mexico
City, for example, is sinking (Kimmelman, 2017). With excessive
extraction comes a variety of other undesirable effects, including

• Increased pumping costs • Changes in hydraulic pressure and
underground flow directions (in coastal areas, this results in seawater
intrusion) • Saline water drawn up from deeper geological formations •
Poor-quality water from polluted shallow aquifers leaking downward
Severe depletion of groundwater resources is often compounded by a
serious deterioration in its quality. Without a doubt, then,
contamination of a groundwater supply should be a concern of those
drinking water practitioners responsible for supplying a community with
potable water provided by groundwater. Despite our strong reliance on
groundwater, groundwater has for many years been one of the most
neglected natural resources. Why? Good question. Groundwater has been
ignored because it is less visible than other environmental
resources---rivers or lakes, for example. What the public cannot see or
observe, the public doesn't worry about, or even think about; however,
recent publicity about events concerning groundwater contamination is
making the public more aware of the problem, and the regulators have
also taken notice. Are natural contaminants a threat to human
health---harbingers of serious groundwater pollution events? No, not
really. The main problem with respect to serious groundwater pollution
has been human activities. When we improperly dispose of wastes or spill
hazardous substances onto the ground, we threaten our groundwater and in
turn public health. Let's take a closer look at a few sources of
groundwater contamination. Several sources of groundwater contamination
are cause for concern for the drinking water practitioner. Consider for
a moment the importance of groundwater. People depend on groundwater in
every state, and its usage accounts for approximately one-fourth of all
water used. This consumption includes about 35\% of water withdrawn for
municipal water supplies.

unDergrounD storage tanks If we could look at a map of the United States
indicating the exact location of every underground storage tank (UST),
most of us would be surprised at the large number of tanks buried
underground. With so many buried tanks, it should come as no surprise
that structural failures arising from a wide variety of causes have
occurred over the years. Subsequent leaking has become a huge source of
contamination that affects the quality of local groundwaters.

Note: A UST is a tank and any underground piping connected to the tank
that has at least 10\% of its combined volume below ground (USEPA,
2017).

The fact is, leakage of petroleum and its products from USTs occurs more
often than we generally realize. This widespread problem has been and
continues to be a major concern and priority in the United States. In
1987, the USEPA promulgated regulations for many of the nation's USTs,
and much progress has been made in mitigating this problem to date. When
a UST leak or past leak is discovered and contaminants have been or are
being released to the soil and thus to groundwater, it would seem to be
a rather straightforward process to identify the contaminant, which in
many cases would be fuel oil, diesel, or gasoline. Other contaminants,
however, also present problems; for example, in the following section,
we discuss one such contaminate, a byproduct of gasoline, to help
illustrate the magnitude of leaking USTs. mtbe anD ethanol In 1997, the
USEPA issued a drinking water advisory, Consumer Acceptability Advice
and Health Effects Analysis on Methyl Tertiary-Butyl Ether (MtBE), for
communities exposed to drinking water contaminated with MtBE, a volatile
organic chemical. MtBE was used as an octane enhancer in gasoline
because it can promote more complete burning of gasoline (thus reducing
carbon monoxide and ozone levels). Note: A USEPA Advisory is usually
initiated to provide information and guidance to individuals or agencies
concerned with potential risk from drinking water contaminants for which
no national regulations currently exist. Advisories are not mandatory
standards for action and are used only for guidance. They are not
legally enforceable, and are subject to revision as new information
becomes available. The USEPA's Health Advisory program is recognized in
the 1996 amendments to the Safe Drinking Water Act, which state in
Section 102(b)(1)(F): The Administrator may publish health advisories
(which are not regulations) or take other appropriate actions for
contaminants not subject to any national primary drinking water
regulation. As its title indicates, this Advisory includes consumer
acceptability advice as ``appropriate'' under this statutory provision,
as well as a health effects analysis. The Clean Air Act of 1990 mandated
the use of reformulated gasoline (RFG) in areas of the country with the
worst ozone or smog problems. After passage of the Clean Air Act, RFG
was required to meet certain technical specifications set forth in the
Act, including a specific oxygen content. Ethanol and MtBE were the
primary oxygenates used to meet the oxygen content requirement. In 2005,
the Energy Policy Act removed the oxygenate requirement for RFG, and
Congress instituted a renewable fuel standard. In response, refiners
made a wholesale switch, removing MtBE and blending fuel instead with
ethanol. According to USEPA survey data, MtBE has not been used in
significant quantities in RFG areas since 2005. A similar decrease in
MtBE use has also been observed in conventional gasoline areas (USEPA,
2016). Studies identified significant air quality and public health
benefits that were a direct result of the use of fuels oxygenated with
MtBE, ethanol, or other chemicals. Refiners' fuel data submitted to the
USEPA for 1995/1996, for example, indicated that the national emissions
benefits exceeded those required. The 1996 Air Quality Trends Report
showed that toxic air pollutants declined significantly between 1994 and
1995, and analysis indicated that at least some of this progress could
be attributed to the use of RFG. Beginning in the year 2000, required
emission reductions became substantially greater, at about 27\% for
volatile organic chemicals, 22\% for toxic air pollutants, and 7\% for
nitrogen oxides. Note: When gasoline that has been oxygenated with MtBE
comes in contact with water, large amounts of MtBE dissolve. At 25°C,
the water solubility of MtBE is about 5000 mg/L for a gasoline that is
10\% MtBE by weight. In contrast, for a nonoxygenated gasoline, the
total hydrocarbon solubility in water is typically about 120 mg/L. MtBE
sorbs only weakly to soil and aquifer material; therefore, sorption will
not significantly retard MtBE transport by groundwater. In addition, the
compound generally resists degradation in groundwater (Squillace et al.,
1997). A limited number of instances of significant contamination of
drinking water with MtBE have occurred because of leaks from underground
and aboveground petroleum storage tank systems and pipelines. Due to its
small molecular size and solubility in water, MtBE moves rapidly into
groundwater, faster than do other constituents of gasoline. Public and
private wells have been contaminated in this manner. Non-point sources
(such as recreational watercraft) are most likely to be the cause of
small amounts of contamination in a large number of shallow aquifers and
surface waters. Air deposition through precipitation of industrial or
vehicular emissions may also contribute to surface water contamination.
The extent of any potential for build-up in the environment from such
deposition is uncertain. Based on the limited sampling data available,
most concentrations at which MtBE has been found in drinking water
sources are unlikely to cause adverse health effects; however, the USEPA
is continuing to evaluate the available information and is doing
additional research to seek more definitive estimates of potential risks
to humans from drinking water. There are no data on the effects on
humans of drinking MtBE-contaminated water. In laboratory tests on
animals, cancer and noncancer effects have occurred at high levels of
exposure. These tests are conducted by inhalation exposure or by
introducing the chemical in oil directly to the stomach. The tests
support a concern for potential human hazard; however, because the
animals were not exposed through drinking water, significant
uncertainties exist concerning the degree of risk associated with human
exposure to low concentrations typically found in drinking water. The
very unpleasant taste and odor of MtBE make contaminated drinking water
unacceptable to the general public. Studies conducted on the
concentrations of MtBE in drinking water determined the level at which
individuals can detect the odor or taste of the chemical. Humans vary
widely in the concentrations they are able to detect. Some who are
sensitive can detect very low concentrations. Others do not taste or
smell the chemical, even at much higher concentrations. The presence or
absence of other natural or water treatment chemicals sometimes masks or
reveals the taste or odor effects. Studies to date have not been
extensive enough to completely describe the extent of this variability
or to establish a population response threshold. Nevertheless, it can be
concluded from the available studies that keeping concentrations in the
range of 20 to 40 µg/L of water or below will likely avert unpleasant
taste and odor effects, recognizing that some people may detect the
chemical below this. The original USEPA Advisory recommended control
levels for taste and odor acceptability that would also protect against
potential health effects. Concentrations in the range of 20 to 40 µg/L
are about 20,000 to 100,000 (or more) times lower than the range of
exposure levels in which cancer or noncancer effects were observed in
rodent tests. This margin of exposure lies within the range of margins
of exposure typically provided to protect against cancer effects by the
National Primary Drinking Water Regulations under the federal Safe
Drinking Water Act---a margin greater than such standards typically
provided to protect against noncancer effects. Protection of the water
source from unpleasant taste and odor as recommended also protects
consumers from potential health effects. The USEPA observed that
occurrences of groundwater contamination at or above this 20to 40-µg/L
taste and odor threshold (that is, contamination at levels that may
create consumer acceptability problems for water supplies) have, to
date, resulted from leaks in petroleum storage tanks or pipelines, not
from any other sources. Public water systems that conduct routine
monitoring for volatile organic chemicals can test for MtBE at little
additional cost, and some states are already moving in this direction.
Public water systems detecting MtBE in their source water at problematic
concentrations can remove MtBE from water using the same conventional
treatment techniques that are used to clean up other contaminants
originating from gasoline releases---air stripping and granular
activated carbon (GAC), for example. Because MtBE is more soluble in
water and more resistant to biodegradation than other chemical
constituents in gasoline, air stripping and GAC treatment require
additional optimization and must often be used together to effectively
remove MtBE from water. The costs of removing MtBE are higher than when
treating for gasoline releases that do not contain MtBE. Oxidation of
MtBE using ultraviolet/peroxide/ ozone treatment may also be feasible
but typically has higher capital and operating costs than air stripping
and GAC. Note: The U.S. Geological Survey analyzed 3500 water samples
collected from various types of wells from 1985 to 2001 to assess the
presence of 55 volatile organic chemicals in groundwater.
Trihalomethanes, which may originate as chlorination byproducts, and
solvents were the most frequently detected VOCs, but MtBE demonstrated
regional patterns of occurrence (Zogorski et al., 2006).

inDustrial Waste Because industrial waste represents a significant
source of groundwater contamination, drinking water practitioners and
others expend an increasing amount of time in abating or mitigating
pollution events that damage groundwater supplies. Groundwater
contamination from industrial waste usually begins with the practice of
disposing of industrial chemical wastes in surface
impoundments---unlined landfills or lagoons, for example. Fortunately,
these practices, for the most part, are part of our past. Today, we know
better; for example, we now know that what is most expedient or least
expensive does not work for industrial waste disposal practices. We have
found through actual experience that the long run has proven just the
opposite---for society as a whole (with respect to health hazards and
the costs of cleanup activities) to ensure clean or unpolluted
groundwater supplies is very expensive and utterly necessary.

septic tanks Seepage from septic tanks is a biodegradable waste capable
of affecting the environment through water and air pollution. The
potential environmental problems associated with use of septic tanks are
magnified when you consider that subsurface sewage disposal systems
(septic tanks) are used by almost one-third of the U.S. population.
Briefly, a septic tank and leaching field system traps and stores solids
while the liquid effluent flows from the tank into a leaching or
absorption field, where it slowly seeps into the soil and degrades
naturally. The problem with subsurface sewage disposal systems such as
septic tanks is that most of the billions of gallons of sewage that
enter the ground each year are not properly treated. Because of faulty
construction or lack of maintenance, not all of these systems work
properly. Experience has shown that septic disposal systems are
frequently sources of fecal bacteria and virus contamination of water
supplies taken from private wells. Many septic tank owners dispose of
detergents, nitrates, chlorides, and solvents in their septic systems or
use solvents to treat their sewage waste. A septic tank cleaning fluid
that is commonly used contains organic solvents (trichloroethylene, or
TCE) that are potential human carcinogens that pollute the groundwater
in areas served by septic systems.

lanDfills Humans have been disposing of waste by burying it in the
ground since time immemorial. In the past, this practice was largely
uncontrolled, and the disposal sites (i.e., garbage dumps) were places
where municipal solid wastes were simply dumped on and into the ground
without much thought or concern. Even in this modern age, landfills have
been used to dispose of trash and waste products at controlled locations
that are then sealed and buried under the ground. Now such practices are
increasingly seen as a less than satisfactory disposal method, because
of the long-term environmental impact of waste materials in the ground
and groundwater. Unfortunately, many of the older (and even some of the
newer) sites have been located in low-lying areas with high groundwater
tables. Leachate (seepage of liquid through the waste) high in
biochemical oxygen demand (BOD), chloride, organics, heavy metals,
nitrate, and other contaminants has little difficulty reaching the
groundwater in such disposal sites. In the United States, literally
thousands of inactive or abandoned dumps like this exist.

agriculture Fertilizers and pesticides are the two most significant
groundwater contaminants that result from agricultural activities. The
impact of agricultural practices wherein fertilizers and pesticides are
normally used is dependent on local soil conditions. If, for example,
the soil is sandy, nitrates from fertilizers are easily carried through
the porous soil into the groundwater, contaminating private wells.
Pesticide contamination of groundwater is a subject of national
importance because groundwater is used for drinking water by about 50\%
of the nation's population. This especially concerns people living in
the agricultural areas where pesticides are most often used, as about
95\% of that population relies upon groundwater for drinking water.
Before the mid1970s, the common thought was that soil acted as a
protective filter, one that stopped pesticides from reaching
groundwater. Studies have now shown that this is not the case.
Pesticides can reach water-bearing aquifers below ground from
applications onto crop fields, seepage of contaminated surface water,
accidental spills and leaks, improper disposal, and even through
injection of waste material into wells. Pesticides are mostly modern
chemicals. Many hundreds of these compounds are used, and extensive
tests and studies of their effect on humans have not been completed.
This leads us to ask, ``Just how concerned should we be about their
presence in our drinking water?'' Certainly, considering pesticides to
be potentially dangerous and handling them with appropriate care would
be wise. We can say they pose a potential danger if they are consumed in
large quantities, but as any experienced scientist knows we cannot draw
factual conclusions unless scientific tests have been done. Some
pesticides have had a designated maximum contaminant level (MCL) in
drinking water set by the USEPA, but many have not. Another serious
point to consider is the potential effect of combining more than one
pesticide in drinking water, the effects of which might be different
than those of each individual pesticide alone. This is another situation
where we do not have sufficient scientific data to draw reliable
conclusions---in other words, we don't know what we don't know.

saltWater intrusion In many coastal cities and towns as well as in
island locations, the intrusion of salty seawater presents a serious
water quality problem. Because freshwater is lighter than saltwater (the
specific gravity of seawater is about 1.025), it will usually float
above a layer of saltwater. When an aquifer in a coastal area is pumped,
the original equilibrium is disturbed and saltwater replaces the
freshwater (Viessman and Hammer, 1998). The problem is compounded by
increasing population, urbanization, and industrialization, all of which
increase the use of groundwater supplies. In such areas, while
groundwater is heavily drawn upon, the quantity of natural groundwater
recharge is decreased because of the construction of roads, tarmac, and
parking lots, which prevent rainwater from infiltrating, reducing the
groundwater table elevation. In coastal areas, the natural interface
between the fresh groundwater flowing from upland areas and the saline
water from the sea is constantly under attack by human activities.
Because seawater is approximately 2.5 times more dense than freshwater,
a high-pressure head of seawater occurs (in relation to freshwater),
which results in a significant rise in the seawater boundary. Potable
water wells close to this rise in sea level may have to be abandoned
because of saltwater intrusion.

other sources of grounDWater contamination To this point, we have
discussed only a few of the many sources of groundwater contamination;
for example, we have not discussed mining and petroleum activities that
lead to contamination of groundwater or contamination caused by
activities in urban areas, both of which are important. Urban activities
(including spreading

salt on roads to keep them free of ice during the winter) eventually
contribute to contamination of groundwater supplies, as can underground
injection wells used to dispose of hazardous materials. As discussed
earlier, underground storage tanks are also significant contributors to
groundwater pollution. Other sources of groundwater contamination
include the following items on our short list:

• Waste tailings • Residential disposal • Urban runoff • Hog wastes •
Biosolids • Land-applied wastewater • Graveyards • Deicing salts •
Surface impoundments • Waste piles • Animal feeding operations • Natural
leaching • Animal burial • Mine drainage • Pipelines • Open dumps • Open
burning • Atmospheric pollutants • Hydraulic fracturing

Raw sewage is not listed, because for the most part raw sewage is no
longer routinely dumped into our nation's wells or into our soil. Sewage
treatment plants effectively treat wastewater so that it can be safely
discharged to local water bodies. In fact, the amount of pollution being
discharged from these plants has been cut by over one-third during the
past several decades, even as the number of people served has doubled.
Yet, in some areas raw sewage spills still occur, sometimes because a
underground sewer line is blocked, broken, or too small or because
periods of heavy rainfall overload the capacity of the sewer line or
sewage treatment plant so overflows into city streets or streams occur.
Some of this sewage finds its way to groundwater supplies.

SUMMARY The best way to prevent groundwater pollution is to stop it from
occurring in the first place. Unfortunately, a perception held by many
is that natural purification of chemically contaminated ground can take
place on its own---without the aid of human intervention. To a degree
this is true; however, natural purification functions on its own time,
not on human time. Natural purification could take decades, perhaps
centuries. The alternative? Remediation. But remediation and mitigation
don't come cheap. When groundwater is contaminated, the cleanup efforts
are sometimes much too expensive to be practical. The USEPA established
the Groundwater Guardian program in 1994, which is a voluntary approach
to improving drinking water safety. Established and managed by a
nonprofit organization in the Midwest and strongly promoted by the
USEPA, this program focuses on communities that rely on groundwater for
their drinking water. It provides special recognition and technical
assistance to help communities protect their groundwater from
contamination. Groundwater Guardian programs have been established in
over 150 communities in 34 states (http://www.groundwater.org).

Drinking Water Monitoring INTRODUCTION Drinking water monitoring refers
to water quality monitoring based on three criteria: 1. Ensure to the
greatest extent possible that the water is not a danger to public
health. 2. Ensure that water provided at the tap is as aesthetically
pleasing as possible. 3. Ensure compliance with applicable regulations.
To meet these goals, all public water systems must monitor water quality
to some extent. Before the Ground Water Rule of 2006 was implemented,
the degree of monitoring employed was dependent on local needs,
requirements, and the type of water system. Small water systems using
good-quality water from deep wells may only have to provide occasional
monitoring, but systems using surface water sources must test water
quality frequently (AWWA, 2003). The Ground Water Rule modified sampling
and monitoring requirements for all public water systems that use
groundwater sources at risk of microbial contamination; they are now
required to take corrective action to protect consumers from harmful
bacteria and viruses. Monitoring is a key element of this risk-targeted
approach.

Note: Compliance monitoring ensures that systems already providing
99.99\% (4-log) inactivation, removal, or a state-approved combination
of inactivation and removal of viruses are achieving this level of
treatment. Water quality monitoring can be defined as the sampling and
analysis of water constituents and conditions. When we monitor, we
collect data. As a monitoring program is developed, deciding the reason
for collecting the information is important. The reasons for gathering
it are defined by establishing a set of objectives and providing a
description of who will collect the information. It may come as a
surprise to learn that today the majority of people collecting data are
volunteers, not necessarily professional drinking water practitioners.
These volunteers have a vested interest in their local stream, lake, or
other body of water and in many cases are proving they can successfully
carry out a water quality monitoring program.

IS THE WATER GOOD OR BAD?* To answer the question of whether the water
is good or bad we must consider two factors. First, let's return to the
basic principles of water quality monitoring---sampling and analyzing
water constituents and conditions. These constituents include the
following:

\begin{enumerate}

\item
  Introduced pollutants, such as pesticides, metals, and oil
\item
  Constituents found naturally in water that can nevertheless be
  affected by human sources, such as dissolved oxygen, bacteria, and
  nutrients
\end{enumerate}

The magnitude of their effects is influenced by properties such as pH
and temperature; for example, temperature influences the quantity of
dissolved oxygen that water is able to contain, and pH affects the
toxicity of ammonia. The second factor to be considered is that the only
valid way to answer this question is to conduct tests, the results of
which must then be compared to some form of water quality standards. If
simply assigning a ``good'' and ``bad'' value to each test factor were
possible, the meters and measuring devices in water quality test kits
would be much easier to make. Instead of fine graduations, they could
simply have a ``good'' and a ``bad'' zone. Water quality---the
difference between good and bad water---must be interpreted according to
the intended use of the water; for example, the perfect balance of water
chemistry that provides a sparkling clear, sanitary swimming pool would
not be acceptable for drinking water and would be a deadly environment
for many biota (Table 10.1). In another example, widely different levels
of fecal coliform bacteria are considered acceptable, depending on the
intended use of the water. State and local water quality practitioners
as well as volunteers have been monitoring water quality conditions for
many years. In fact, until the past decade or so (until biological
monitoring protocols were developed and began to take hold), water
quality monitoring was generally considered the primary way to identify
water pollution problems. Today, professional water quality
practitioners and volunteer program coordinators alike are moving toward
approaches that combine chemical, physical, and biological monitoring
methods to achieve the best picture of water quality conditions. Water
quality monitoring can be used for many purposes:

\begin{enumerate}
\item
  To identify whether waters are meeting designated uses. All states
  have established specific criteria (limits on pollutants) identifying
  what concentrations of chemical pollutants are allowable in their
  waters. When chemical pollutants exceed maximum or minimum allowable
  concentrations, waters may no longer be able to support the beneficial
  uses---such as fishing, swimming, and drinking---for which they have
  been designated (see Table 10.2). Designated or intended uses and the
  specific criteria that protect them (along with antidegredation
  statements prohibiting waters from deteriorating below existing or
  anticipated uses) together form water quality standards. State water
  quality professionals assess water quality by comparing the
  concentrations of chemical pollutants found in streams to the criteria
  in the state's standards and so judge whether or not streams are
  meeting their designated uses. Water quality monitoring, however,
  might be inadequate for determining whether aquatic life needs are
  being met in a stream. Although some constituents (such as dissolved
  oxygen and temperature) are important to maintaining healthy fish and
  aquatic insect populations, other factors (such as the physical
  structure of the stream and the condition of the habitat) play an
  equal or greater role. Biological monitoring methods are generally
  better suited to determining whether aquatic life is supported.
\item
  To identify specific pollutants and sources of pollution. Water
  quality monitoring helps link sources of pollution to water body
  quality problems because it identifies specific problem pollutants.
  Because certain activities tend to generate certain pollutants
  (bacteria and nutrients are more likely to come from an animal feedlot
  than an automotive repair shop), a tentative link to what would
  warrant further investigation or monitoring can be formed.
\item
  To determine trends. Chemical constituents that are properly monitored
  (i.e., using consistent time of day and on a regular basis using
  consistent methods) can be analyzed for trends over time.
\item
  To screen for impairment. Finding excessive levels of one or more
  chemical constituents can serve as an early warning for potential
  pollution problems.
\end{enumerate}

state Water Quality stanDarDs programs Each state has a program to set
standards for the protection of each body of water within its
boundaries. Standards for each body of water are developed that 1.
Depend on the water's designated use 2. Are based on USEPA national
water quality criteria and other scientific research into the effects of
specific pollutants on different types of aquatic life and on human
health 3. May include limits based on the biological diversity of the
body of water (the presence of food and prey species) State water
quality standards set limits on pollutants and establish water quality
levels that must be maintained for each type of water body, based on its
designated use. Resources for this type of information include the
following: 1. USEPA water quality criteria program 2. U.S. Fish and
Wildlife Service habitat suitability index models (for specific species
of local interest) Individual monitoring test results can be plotted
against these standards to provide a focused, relevant, required
assessment of water quality.

DESIGNING A WATER QUALITY MONITORING PROGRAM The first step in designing
a water quality monitoring program is to determine the purpose for the
monitoring. This aids in selection of parameters to monitor. This
decision should be based on such factors as Types of water quality
problems and pollution sources that are likely to be encountered (see
Table 10.3) 1. Cost of available monitoring equipment 2. Precision and
accuracy of available monitoring equipment 3. Capabilities of monitors

This discussion in this chapter focuses on the parameters most commonly
monitored by drinking water practitioners in streams (i.e., it is
assumed, for illustration and discussion purposes, that the water source
is a surface water stream). These parameters include dissolved oxygen
(DO), biochemical oxygen demand (BOD), temperature, hardness, pH,
turbidity, orthophosphates, nitrates, total solids, conductivity, total
alkalinity, fecal bacteria, apparent color, and odor (see Figure 10.1).
Note: When monitoring water supplies under the Safe Drinking Water Act
(SDWA) or the National Pollutant Discharge Elimination System (NPDES),
utilities must follow USEPA-approved test procedures. Additional testing
requirements for these and other federal programs are published as
amendments in the Federal Register. Except when monitoring discharges
for specific compliance purposes, a large number of approximate
measurements can provide more useful information than one or two
accurate analyses. Because water quality and chemistry continually
change, making periodic, representative measurements and observations
that indicate the range of water quality is necessary, rather than
testing the quality at any single moment. The more complex a water
system, the more time is required to observe, understand, and draw
conclusions regarding the cause and effect of changes in the particular
system.

GENERAL PREPARATION AND SAMPLING CONSIDERATIONS The sections that follow
detail specific and equipment considerations and analytical procedures
for each of the most common water quality parameters, and general tasks
that should be accomplished any time water samples are taken are
discussed below. preparation of sampling containers Sampling devices
should be corrosion resistant, easily cleaned, and capable of collecting
desired samples safely and in accordance with test requirements.
Whenever possible, assign a sampling device to each sampling point.
Sampling equipment must be cleaned on a regular schedule to avoid
contamination. Note: Some tests require special equipment to ensure that
the sample is representative. Dissolved oxygen and fecal bacteria
sampling requires special equipment and procedures to prevent collection
of nonrepresentative samples.

Reused sample containers and glassware must be cleaned and rinsed before
the first sampling run and after each run by following Method A or
Method B described below. The more suitable method depends on the
parameter being measured.

Method A: General Preparation of Sampling Containers Use the following
method when preparing all sample containers and glassware for monitoring
conductivity, total solids, turbidity, pH, and total alkalinity. Wearing
latex gloves,

\begin{enumerate}
\item
  Wash each sample bottle or piece of glassware with a brush and
  phosphatefree detergent.
\item
  Rinse three times with cold tap water.
\item
  Rinse three times with distilled or deionized water.
\end{enumerate}

Method B: Acid Wash Procedures Use this method when preparing all sample
containers and glassware for monitoring nitrates and phosphorus. Wearing
latex gloves,

\begin{enumerate}
\item
  Wash each sample bottle or piece of glassware with a brush and
  phosphatefree detergent.
\item
  Rinse three times with cold tap water.
\item
  Rinse with 10\% hydrochloric acid.
\item
  Rinse three times with deionized water.
\end{enumerate}

Sample Types Two basic types of samples are commonly used for water
quality testing: grab samples and composite samples. The type of sample
used depends on the specific test, the reason why the sample is being
collected, and the applicable regulatory requirements. Grab samples are
collected at one time and one location. They are representative only of
the conditions at the time of collection. Grab samples must be used to
determine pH, total residual chlorine (TRC), dissolved oxygen, and fecal
coliform concentrations. Grab samples may also be used for any test that
does not specifically prohibit their use. Composite samples consist of a
series of individual grab samples collected over a specified period of
time in proportion to flow. The individual grab samples are mixed
together in proportion to the flow rate at the time the sample was
collected to form the composite sample. Note: Before collecting samples
for any test procedure, it is best to review the sampling requirements
of the test.

collecting samples from a stream In general, sample away from the stream
bank in the main current. Never sample stagnant water. The outside curve
of the stream is often a good place to sample, because the main current
tends to hug this bank. In shallow stretches, carefully wade into the
center current to collect the sample. A boat is required for deep sites.
Try to maneuver the boat into the center of the main current to collect
the water sample. When collecting a water sample for analysis in the
field or at the lab, follow the procedures provided below.

Whirl-Pak® Bags 1. Label the bag with the site number, date, and time.
2. Tear off the top of the bag along the perforation above the wire tab
just before sampling. Avoid touching the inside of the bag. If you
accidentally touch the inside of the bag, use another one. 3. When
wading, try to disturb as little bottom sediment as possible. In any
case, be careful not to collect water that contains bottom sediment.
Stand facing upstream. Collect the water samples in front of you. By
boat, carefully reach over the side and collect the water sample on the
upstream side of the boat. 4. Hold the two white pull-tabs in each hand
and lower the bag into the water on your upstream side with the opening
facing upstream. Open the bag midway, between the surface and the bottom
by pulling the white pull-tabs. The bag should begin to fill with water.
You may need to scoop water into the bag by drawing it through the water
upstream and away from you. Fill the bag no more than three-quarters
full. 5. Lift the bag out of the water. Pour out excess water. Pull on
the wire tabs to close the bag. Continue holding the wire tabs and flip
the bag over at least four to five times quickly to seal the bag. Do not
try to squeeze the air out of the top of the bag. Fold the ends of the
bag, being careful not to puncture the bag. Twist them together, forming
a loop. 6. Fill in the bag number and site number on the appropriate
field data sheet. This is important. It is the only way the lab
specialist will know which bag goes with which site. 7. If samples are
to be analyzed in a lab, place the sample in the cooler with ice or cold
packs. Take all samples to the lab.

Screw-Cap Bottles To collect water samples using screw-cap bottles, use
the following procedures (see Figure 10.2):

\begin{enumerate}
\item
  Label the bottle with the site number, date, and time.
\item
  Remove the cap from the bottle just before sampling. Avoid touching
  the inside of the bottle or the cap. If you accidentally touch the
  inside of the bottle, use another one.
\item
  When wading, try to disturb as little bottom sediment as possible. In
  any case, be careful not to collect water that has sediment from
  bottom disturbance. Stand facing upstream. Collect the water sample on
  your upstream side, in front of you. You may also tape the bottle to
  an extension pole to sample from deeper water. By boat, carefully
  reach over the side and collect the water sample on the upstream side
  of the boat.
\item
  Hold the bottle near its base and plunge it (opening downward) below
  the water surface. If you are using an extension pole, remove the cap,
  turn the bottle upside down, and plunge it into the water, facing
  upstream. Collect a water sample 8 to 12 inches beneath the surface,
  or midway between the surface and the bottom if the stream reach is
  shallow.
\item
  Turn the bottle underwater into the current and away from you. In
  slowmoving stream reaches, push the bottle underneath the surface and
  away from you in the upstream direction.
\item
  Leave a 1-inch air space (except for DO and BOD samples). Do not fill
  the bottle completely (so the sample can be shaken just before
  analysis). Recap the bottle carefully, remembering not to touch the
  inside.
\item
  Fill in the bottle number and site number on the appropriate field
  data sheet. This is important because it tells the lab specialist
  which bottle goes with which site.
\item
  If the samples are to be analyzed in the lab, place them in the cooler
  for transport to the lab.
\end{enumerate}

sample preserVation anD storage Samples can change very rapidly, and no
single preservation method will serve for all samples and constituents.
If analysis must be delayed, follow the instructions for sample
preservation and storage listed in Standard Methods for the Examination
of Water and Wastewater (APHA-AWWA-WEF, 2017), or those specified by the
laboratory that will eventually process the samples (see Table 10.4). In
general, handle the sample in a way that does not cause changes in
biological activity, physical alterations, or chemical reactions. Cool
the sample to reduce biological and chemical reactions. Store in
darkness to suspend photosynthesis. Fill the sample container completely
to prevent the loss of dissolved gases. Be aware that metal cations such
as iron and lead and suspended particles may adsorb onto container
surfaces during storage. References used for sampling and testing must
correspond to those listed in the most current federal regulations. For
the majority of tests, to compare the results of either different water
quality monitors or the same monitors over the course of time requires
some form of standardization of the methods. The American Public Health
Association (APHA) recognized this requirement when, in 1899, it
appointed a committee to draw up standard procedures for the analysis of
water. The report (published in 1905) constituted the first edition of
what is now known as Standard Methods for the Examination of Water and
Wastewater, or Standard Methods. This book serves as the primary
reference for water testing methods and as the basis for most
USEPA-approved methods. This book is now in its 23rd edition and serves
as the primary reference for water testing methods, and as the basis for
most USEPAapproved methods. TEST METHODS* Descriptions of general
methods to help you understand how each works in specific test kits
follow. Always follow the specific instructions included with the
equipment and individual test kits. Most water analyses are conducted
either by titrimetric analyses or colorimetric analyses. Both methods
are easy to use and provide accurate results.

titrimetric Titrimetric analyses are based on adding a solution of known
strength (the titrant) to a specific volume of a treated sample in the
presence of an indicator. The indicator produces a color change
indicating that the reaction is complete. Titrants are generally added
by a titrator (microburette) or a precise glass pipette.

colorimetric Two basic types of colorimetric tests are commonly used: 1.
The pH is a measure of the concentration of hydrogen ions (the acidity
of a solution) determined by the reaction of an indicator that varies in
color, depending on the hydrogen ion levels in the water. 2. Tests that
determine a concentration of an element or compound are based on Beer's
law. Simply, this law states that the higher the concentration of a
substance, the darker the color produced in the test reaction and
therefore the more light absorbed. Assuming a constant viewpath, the
absorption increases exponentially with concentration.

Visual methoDs The Octet Comparator uses standards that are mounted in a
plastic comparator block. It employs eight permanent translucent color
standards and built-in filters to eliminate optical distortion. The
sample is compared using either of two viewing windows. Two devices that
can be used with the comparator are a color reader, which neutralizes
color or turbidity in water samples, and an axial mirror, which
intensifies faint colors of low concentrations for easy distinction.

electronic methoDs Although the human eye is capable of differentiating
color intensity, interpretation is quite subjective. Electronic
colorimeters consist of a light source that passes through a sample and
is measured on a photodetector with an analog or digital readout.
Besides electronic colorimeters, specific electronic instruments are
manufactured for lab and field determination of many water quality
factors, including pH, total dissolved solids, conductivity, dissolved
oxygen, temperature, and turbidity.

DISSOLVED OXYGEN AND BIOCHEMICAL OXYGEN DEMAND* A stream system (in this
case, the hypothetical one providing the source of water used for this
discussion) produces and consumes oxygen. It gains oxygen from the
atmosphere and from plants as a result of photosynthesis. Because of
running water's churning, it dissolves more oxygen than still water---in
a reservoir behind a dam, for example. Respiration by aquatic animals,
decomposition, and various chemical reactions consume oxygen. Oxygen is
actually poorly soluble in water. Its solubility is related to pressure
and temperature. In water supply systems, dissolved oxygen (DO) in raw
water is considered the necessary element to support life of many
aquatic organisms. From the drinking water practitioner's point of view,
DO is an important indicator of the water treatment process and an
important factor in corrosivity. Wastewater from sewage treatment plants
often contains organic materials that are decomposed by microorganisms,
which use oxygen in the process. The amount of oxygen consumed by these
organisms in breaking down the waste is known as the biochemical oxygen
demand (BOD). We include a discussion of BOD and how to monitor it
later. Other sources of oxygen-consuming waste include stormwater runoff
from farmland or urban streets, feedlots, and failing septic systems.
Oxygen is measured in its dissolved form as dissolved oxygen. If more
oxygen is consumed than produced, DO levels decline and some sensitive
animals may move away, weaken, or die. DO levels fluctuate over a
24-hour period and seasonally. They vary with water temperature and
altitude. Cold water holds more oxygen than warm water (Table 10.5), and
water holds less oxygen at higher altitudes. Thermal discharges (such as
water used to cool machinery in a manufacturing plant or a power plant)
raise the temperature of water and lower its oxygen content. Aquatic
animals are most vulnerable to lowered DO levels in the early morning on
hot summer days when stream flows are low, water temperatures are high,
and aquatic plants have not been producing oxygen since sunset.

sampling anD eQuipment consiDerations In contrast to lakes, where
dissolved oxygen levels are most likely to vary vertically in the water
column, changes in DO in rivers and streams move horizontally along the
course of the waterway. This is especially true in smaller, shallow
streams. In larger, deeper rivers, some vertical stratification of
dissolved oxygen might occur. The DO levels in and below riffle areas,
waterfalls, or dam spillways are typically higher than those in pools
and slower moving stretches. If we want to measure the effect of a dam,
sampling for DO behind the dam immediately below the spillway and
upstream of the dam would be important. Because DO levels are critical
to fish, a good place to sample is in the pools that fish tend to favor
or in the spawning areas they use. An hourly time profile of DO levels
at a sampling site is a valuable set of data, because it shows the
change in DO levels from the low point (just before sunrise) to the high
point (sometime near midday); however, this might not be practical for a
volunteer monitoring program. Note the time of the DO sampling to help
judge when in the daily cycle the data were collected. Dissolved oxygen
is measured in either milligrams per liter (mg/L) or as percent
saturation. Milligrams per liter is the amount of oxygen in a liter of
water. Percent saturation is the amount of oxygen in a liter of water
relative to the total amount of oxygen that the water can hold at that
temperature. DO samples are collected using

a special BOD bottle: a glass bottle with a ``turtleneck'' and a ground
glass stopper. You can fill the bottle directly in the stream if the
stream is wadeable or boatable, or you can use a sampler dropped from a
bridge or boat into water deep enough to submerse the sampler. Samplers
can be made or purchased. Dissolved oxygen is measured primarily either
by using some variation of the Winkler method or by using a meter and
probe.

Winkler Method The Winkler method involves filling a sample bottle
completely with water (no air is left to bias the test). The dissolved
oxygen is then fixed using a series of reagents that form a titrated
acid compound. Titration involves the drop-by-drop addition of a reagent
that neutralizes the acid compound, causing a change in the color of the
solution. The point at which the color changes is the endpoint and is
equivalent to the amount of oxygen dissolved in the sample. The sample
is usually fixed and titrated in the field at the sample site, but
preparing the sample in the field and delivering it to a lab for
titration is possible. Dissolved oxygen field kits using the Winkler
method

are relatively inexpensive, especially compared to a meter and probe.
Field kits run between \$35 and \$200, and each kit comes with enough
reagents to run 50 to 100 DO tests. Replacement reagents are inexpensive
and are available already measured out for each test in plastic pillows.
The reagents can also be bought in larger quantities in bottles and
measured out with a volumetric scoop. The advantage of the pillows is
that they have a longer shelf life and are much less prone to
contamination or spillage, but buying larger quantities in bottles has
the advantage of considerably lower cost per test. The major factor in
the expense of the kits is the method of titration used---eyedropper,
syringe-type titrator, or digital titrator. Eyedropper and syringe-type
titration is less precise than digital titration, because a larger drop
of titrant is allowed to pass through the dropper opening, and on a
microscale the drop size (and thus the volume of titrant) can vary from
drop to drop. A digital titrator or a burette (a long glass tube with a
tapered tip like a pipette) permits much more precision and uniformity
in the amount of titrant it allows to pass. If a high degree of accuracy
and precision in DO results is required, a digital titrator should be
used. A kit that uses an eye dropper or syringe type of titrator is
suitable for most other purposes. The lower cost of this type of DO
field kit might be attractive if several teams of samplers and testers
at multiple sites at the same time are necessary.

Meter and Probe A dissolved oxygen meter is an electronic device that
converts signals from a probe placed in the water into units of DO in
milligrams per liter. Most meters and probes also measure temperature.
The probe is filled with a salt solution and has a selectively permeable
membrane that allows DO to pass from the stream water into the salt
solution. The DO that has diffused into the salt solution changes the
electric potential of the salt solution, and this change is sent by
electric cable to the meter, which converts the signal to milligrams per
liter on a scale that the volunteer can read. DO meters are expensive
compared to field kits that use the titration method. Meter and probe
combinations run between \$500 and \$1200, including a long cable to
connect the probe to the meter. The advantage of a meter and probe is
that DO and temperature can be quickly read at any point where the probe
is inserted into the stream; also, DO levels can be measured at a
certain point on a continuous basis. The results are read directly as
milligrams per liter, unlike the titration methods, where the final
titration result might have to be converted by an equation to milligrams
per liter. DO meters are more fragile than field kits, however, and
repairs to a damaged meter can be costly. The meter and probe must be
carefully maintained and must be calibrated before each sample run or,
if many tests are done, between sampling. Because of the expense, a
small waterworks might only have one meter and probe, which means that
only one team of samplers can sample DO and they must test all the
sites. With field kits, on the other hand, several teams can sample
simultaneously. Laboratory Testing of Dissolved Oxygen If a meter and
probe are used, the testing must be done in the field because dissolved
oxygen levels in a sample bottle change quickly due to the decomposition
of organic material by microorganisms or the production of oxygen by
algae and other plants in the sample, which can lower the DO reading. If
a variation of the Winkler method is used, it is possible to fix the
sample in the field and then deliver it to a lab for titration. This
might be preferable if sampling is conducted under adverse conditions or
if time spent collecting samples is an issue. Titrating samples in the
lab is a little easier, and greater quality control is possible because
the same person can do all the titrations.

What is biochemical oxygen DemanD anD Why is it important? Biochemical
oxygen demand (BOD) measures the amount of oxygen consumed by
microorganisms in decomposing organic matter in stream water. BOD also
measures the chemical oxidation of inorganic matter (the extraction of
oxygen from water via chemical reaction). A test is used to measure the
amount of oxygen consumed by these organisms during a specified period
of time (usually 5 days at 20°C). The rate of oxygen consumption in a
stream is affected by a number of variables: temperature, pH, the
presence of certain kinds of microorganisms, and the type of organic and
inorganic material in the water. BOD directly affects the amount of
dissolved oxygen in rivers and streams. The greater the BOD, the more
rapidly oxygen is depleted in the river or stream, leaving less oxygen
available to higher forms of aquatic life. The consequences of high BOD
are the same as those for low dissolved oxygen: Aquatic organisms become
stressed, suffocate, and die. Most river waters used as water supplies
have a BOD of less than 7 mg/L; therefore, dilution is not necessary.
Sources of BOD include leaves and woody debris; dead plants and animals;
animal manure; effluents from pulp and paper mills, wastewater treatment
plants, feedlots, and foodprocessing plants; failing septic systems; and
urban stormwater runoff. Note: To evaluate the potential of raw water
for use as a drinking water supply, the water is usually sampled,
analyzed, and tested for biochemical oxygen demand when turbid, polluted
water is the only source available.

Sampling Considerations Biochemical oxygen demand is affected by the
same factors that affect dissolved oxygen (see above). Aeration of
stream water---by rapids and waterfalls, for example--- will accelerate
the decomposition of organic and inorganic material. BOD levels at a
sampling site with slower, deeper waters might be higher for a given
column of organic and inorganic material than the levels for a similar
site in high aerated waters. Chlorine can also affect BOD measurement by
inhibiting or killing the microorganisms that decompose the organic and
inorganic matter in a sample. When sampling in chlorinated waters (such
as those below the effluent from a sewage treatment plant), it is
necessary to neutralize the chlorine with sodium thiosulfate (see
Standard Methods). Biochemical oxygen demand measurement requires taking
two samples at each site. One is tested immediately for dissolved
oxygen, and the second is incubated in the dark at 20°C for 5 days and
then tested for the dissolved oxygen remaining. The difference in oxygen
levels between the first test and the second test (in milligrams per
liter) is the amount of BOD. This represents the amount of oxygen
consumed by microorganisms and used to break down the organic matter
present in the sample bottle during the incubation period. Because of
the 5-day incubation, the tests are conducted in a laboratory. Sometimes
by the end of the 5-day incubation period, the dissolved oxygen level is
zero. This is especially true for rivers and streams with a lot of
organic pollution. Because it is not possible to know when the zero
point was reached, determining the BOD level is also impossible. In this
case, diluting the original sample by a factor that results in a final
dissolved oxygen level of at least 2 mg/L is necessary. Special dilution
water should be used for the dilutions. Some experimentation is
necessary to determine the appropriate dilution factor for a particular
sampling site. The final result is the difference in dissolved oxygen
between the first measurement and the second, after multiplying the
second result by the dilution factor. Standard Methods prescribes all
phases of procedures and calculations for BOD determination. A BOD test
is not required for monitoring water supplies.

TEMPERATURE An ideal water supply should have, at all times, an almost
constant temperature or one with minimum variation. Knowing the
temperature of the water supply is important because the rates of
biological and chemical processes depend on it. Temperature affects the
oxygen content of the water (oxygen levels become lower as temperature
increases), the rate of photosynthesis by aquatic plants, the metabolic
rates of aquatic organisms, and the sensitivity of organisms to toxic
wastes, parasites, and diseases. Causes of temperature change include
weather, removal of shading stream-bank vegetation, impoundments (a body
of water confined by a barrier, such as a dam), discharge of cooling
water, urban stormwater, and groundwater inflows to the stream.

sampling anD eQuipment consiDerations Temperature in a stream varies
with width and depth, and the temperature of wellsunned portions of a
stream can be significantly higher than the shaded portion of the water
on a sunny day. In a small stream, the temperature will be relatively
constant as long as the stream is uniformly in sun or shade. In a large
stream, temperature can vary considerably with width and depth,
regardless of shade. If safe to do so, temperature measurements should
be collected at varying depths and across the surface of the stream to
obtain vertical and horizontal temperature profiles. This can be done at
each site at least once to determine the necessity of collecting a
profile during each sampling visit. Temperature should be measured at
the same place every time. Temperature is measured in the stream with a
thermometer or a meter. Alcohol-filled thermometers are preferred over
mercury-filled ones because they are less hazardous if broken. Armored
thermometers for field use can withstand more abuse than unprotected
glass thermometers and are worth the additional expense. Meters for
other tests, such as pH (acidity) or dissolved oxygen, also measure
temperature and can be used instead of a thermometer.

HARDNESS Water hardness refers primarily to the amount of dissolved
calcium and magnesium in the water and their effects on scaling,
corrosion, and soap. With hard water, it is difficult to produce a soap
lather. Hard waters leave spots on glasses, a film on laundry and hair,
and crusty deposits on bathroom fixtures. The presence of hardness in
water supplies contributes to taste, odor, color, or turbidity, but
water hardness has no health significance (Hauser, 2002). Calcium and
magnesium enter water mainly by leaching of rocks. Calcium is the most
abundant dissolved cationic constituent of natural freshwaters. Calcium
is an important component of aquatic plant cell walls and the shells and
bones of many aquatic organisms. Magnesium is an essential nutrient for
plants and is a component of the chlorophyll molecule. Hardness test
kits express test results in ppm of CaCO3, but these results can be
converted directly to calcium or magnesium concentrations:

Calcium hardness as ppm CaCO3 × 0.40 = ppm Ca (10.1) Magnesium hardness
as ppm CaCO3 × 0.24 = ppm Mg (10.2) Note: Because of less contact with
soil minerals and more contact with rain, surface raw water is usually
softer than groundwater. As a general rule of thumb, when hardness is
greater than 150 mg/L, softening treatment may be required for public
water systems. Hardness determination via testing is required to monitor
the efficiency of treatment. measuring harDness A hardness test follows
a procedure similar to an alkalinity test, but the reactions involved
are different. After the sample is carefully measured, a buffer is added
to the sample to correct pH for the test and an indicator to signal the
titration endpoint.

The indicator reagent is normally blue in a sample of pure water, but if
calcium or magnesium ions are present in the sample the indicator
combines with them to form a red complex. The titrant in this test is
ethylenediaminetetraacetic acid (EDTA), used with its salts in the
titration method; EDTA serves as a chelant that pulls the calcium and
magnesium ions away from the red complex. The EDTA is added dropwise to
the sample until all the calcium and magnesium ions have been chelated
away from the complex and the indicator returns to its normal blue
color. The amount of EDTA required to cause the color change is a direct
indication of the amount of calcium and magnesium ions in the sample.
Some hardness kits include an additional indicator that is specific for
calcium. This type of kit will provide three readings: total hardness,
calcium hardness, and magnesium hardness. For more information on
interference, precision, and accuracy, consult the latest edition of
Standard Methods.

pH It was pointed out earlier that pH is a term used to indicate the
alkalinity or acidity of a substance as ranked on a scale from 1.0 to
14.0. Acidity increases as the pH gets lower. Figure 10.3 presents the
pH of some common liquids.

analytical anD eQuipment consiDerations The pH of water can be analyzed
in the field or in the lab. If analyzed in the lab, it must be measured
within 2 hours of the sample collection, because the pH will change due
to the carbon dioxide from the air dissolving in the water, bringing the
pH toward 7. If a high degree of accuracy and precision in pH results is
required, the pH should be measured with a laboratory-quality pH meter
and electrode. Meters of this quality range in cost from around \$250 to
\$1000. Color comparators and pH ``pocket pals'' are suitable for most
other purposes. The cost of either of these is in the \$50 range. The
lower cost of the alternatives might be attractive if multiple samplers
are used to sample several sites at the same time.

ph meters A pH meter measures the electric potential (millivolts) across
an electrode when immersed in water. This electric potential is a
function of the hydrogen ion activity in the sample; therefore, pH
meters can display results in either millivolts (mV) or pH units. A pH
meter consists of a potentiometer, which measures electric potential
where it meets the water sample; a reference electrode, which provides a
constant electric potential; and a temperature compensating device,
which adjusts the readings according to the temperature of the sample
(because pH varies with temperature). The reference and glass electrodes
are frequently combined into a single probe called a combination
electrode. A wide variety of meters is available, but the most important
part of the pH meter is the electrode; thus, purchasing a good, reliable
electrode and following the manufacturer's instructions for proper
maintenance are important. Infrequently used or improperly maintained
electrodes are subject to corrosion, which makes them highly inaccurate.

``pocket pals'' anD color comparators pH ``pocket pals'' are electronic
handheld ``pens'' that are dipped in the water to obtain a digital
readout of the pH. They can be calibrated to only one pH buffer. (Lab
meters, on the other hand, can be calibrated to two or more buffer
solutions and thus are more accurate over a wide range of pH
measurements.) Color comparators involve adding a reagent to the sample
that colors the sample water. The intensity of the color is proportional
to the pH of the sample and is matched against a standard color chart.
The color chart equates particular colors to associated pH values, which
can be determined by matching the colors from the chart to the color of
the sample. For instructions on how to collect and analyze samples,
consult the latest edition of Standard Methods.

TURBIDITY Turbidity is a measure of water clarity---how much the
material suspended in water decreases the passage of light through the
water. Suspended materials include soil particles (clay, silt, and
sand), algae, plankton, and microbes, among other substances. These
materials are typically in the size range of 0.004 mm (clay) to 1.0 mm
(sand). Turbidity can affect the color of the water, and higher
turbidity increases water temperature because suspended particles absorb
more heat. This in turn reduces the concentration of DO because warm
water holds less DO than cold. Higher turbidity also reduces the amount
of light penetrating the water, which reduces photosynthesis and the
production of DO. Suspended materials can clog fish gills, reducing
resistance to disease in fish, lowering growth rates, and affecting egg
and larval development. As the particles settle, they can blanket the
stream bottom (especially in slower waters) and smother fish eggs and
benthic macroinvertebrates. Sources of turbidity include

• Soil erosion • Waste discharge • Urban runoff • Eroding stream banks •
Large numbers of bottom feeders (such as carp), which can stir up bottom
sediments • Excessive algal growth

sampling anD eQuipment consiDerations Turbidity can be useful as an
indicator of the effects of runoff from construction, agricultural
practices, logging activity, discharges, and other sources. Turbidity
often increases sharply during rainfall, especially in developed
watersheds, which typically have relatively high proportions of
impervious surfaces. The flow of stormwater runoff from impervious
surfaces rapidly increases stream velocity, which increases the erosion
rates of streambanks and channels. Turbidity can also rise sharply
during dry weather if earth-disturbing activities occur in or near a
stream without erosion control practices in place. Regular monitoring of
turbidity can help detect trends that might indicate increasing erosion
in developing watersheds; however, turbidity is closely related to
stream flow and velocity and should be correlated with these factors.
Comparisons of the change in turbidity over time, therefore, should be
made at the same point at the same flow. Turbidity is not a measure of
the amount of suspended solids present or the rate of sedimentation of a
stream, as it represents only the amount of light that is scattered by
suspended particles. Measurement of total solids is a more direct
measurement of the amount of material suspended and dissolved in water.
Turbidity is generally measured by using a turbidity meter. Volunteer
programs may also take samples to a lab for analysis. Another approach
is to measure transparency (an integrated measure of light scattering
and absorption) instead of turbidity. Water clarity or transparency can
be measured using a Secchi disk (see Figure 10.4) or transparency tube.
The Secchi disk can only be used in deep, slow-moving rivers; the
transparency tube (a comparatively new development) is gaining
acceptance around the country but is not yet in wide use. A turbidity
meter consists of a light source that illuminates a water sample and a
photoelectric cell that measures the intensity of light scattered at a
90° angle by the particles in the sample. It measures turbidity in
nephelometric turbidity units (NTUs). Meters can measure turbidity over
a wide range---from 0 to 1000 NTUs. A clear mountain stream might have a
turbidity of around 1 NTU, whereas a large river such as the Mississippi
might have a dry-weather turbidity of 10 NTUs. Because these values can
jump into hundreds of NTUs during runoff events, the turbidity meter to
be used should be reliable over the range in which you will be working.
Meters of this quality cost about \$800. Many meters in this price range
are designed for field or lab use.

Although turbidity meters can be used in the field, samplers might want
to collect samples and take them to a central point for turbidity
measurements because of the expense of the meter. Most programs can
afford only one and would have to pass it along from site to site,
complicating logistics and increasing the risk of damage to the meter.
Meters also include glass cells that must remain optically clear and
free of scratches for operation. Samplers can also take turbidity
samples to a lab for meter analysis at a reasonable cost.

Using a Secchi Disk A Secchi disk is a black and while disk that is
lowered by hand into the water to the depth at which it vanishes from
sight (see Figure 10.4). The distance to vanishing is then
recorded---the clearer the water, the greater the distance. Secchi disks
are simple to use and inexpensive. For river monitoring they have
limited use, because in most cases the river bottom will be visible and
the disk will not reach a vanishing point. Deeper, slower moving rivers
are the most appropriate places for Secchi disk measurement, although
the current might require that the disk be weighted so it does not sway
and make measurement difficult. Secchi disks cost about \$50 but can be
homemade. The line attached to the Secchi disk must be marked in
waterproof ink according to units designated by the sampling program.
Many programs require samplers to measure to the nearest 1/10 meter.
Meter intervals can be tagged (e.g., with duct tape) for ease of use. To
measure water clarity with a Secchi disk:

\begin{enumerate}
\item
  Check to make sure that the Secchi disk is securely attached to the
  measured line.
\item
  Lean over the side of the boat and lower the Secchi disk into the
  water, keeping your back to the sun to block glare.
\item
  Lower the disk until it disappears from view. Lower it one-third of a
  meter and then slowly raise the disk until it just reappears. Move the
  disk up and down until you find the exact vanishing point.
\item
  Attach a clothespin to the line at the point where the line enters the
  water. Record the measurement on your data sheet. Repeating the
  measurement provides you with a quality control check.
\end{enumerate}

The key to consistent results is to train samplers to follow standard
sampling procedures and, if possible, have the same individual take the
reading at the same site throughout the season.

Transparency Tube Pioneered by Australia's Department of Conservation,
the transparency tube is a clear, narrow plastic tube marked in units
with a dark pattern painted on the bottom. Water is poured into the tube
until the pattern disappears. Some U.S. volunteer monitoring programs,
such as the Tennessee Valley Authority (TVA) Clean Water Initiative and
the Minnesota Pollution Control Agency (MPCA), are testing the
transparency tube in streams and rivers. The MPCA uses tubes marked in
centimeters and has found tube readings to relate fairly well to lab
measurements of turbidity and total suspended solids, although they do
not recommend the transparency tube for applications where precise and
accurate measurement is required or in highly colored waters. The TVA
and MPCA have suggested the following sampling considerations:

\begin{enumerate}
\item
  Collect the sample in a bottle or bucket in midstream and at mid-depth
  if possible. Avoid stagnant water, and sample as far from the
  shoreline as is safe. Avoid collecting sediment from the bottom of the
  stream.
\item
  Face upstream as you fill the bottle or bucket.
\item
  Take readings in open but shaded conditions. Turn your back to the sun
  to avoid direct sunlight.
\item
  Carefully stir or swish the water in the bucket or bottle until it is
  homogeneous, taking care not to produce air bubbles (these scatter
  light and affect the measurement). Pour the water slowly in the tube
  while looking down the tube. Measure the depth of the water column in
  the tube at the point where the symbol just disappears.
\end{enumerate}

ORTHOPHOSPHATES As discussed earlier, both phosphorus and nitrogen are
essential nutrients for the plants and animals that make up the aquatic
food web. Because phosphorus is the nutrient in short supply in most
freshwater systems, even a modest increase in phosphorus can (under the
right conditions) set off a whole chain of undesirable events in a
stream, including accelerated plant growth, algae blooms, low dissolved
oxygen, and the death of certain fish, invertebrates, and other aquatic
animals. Phosphorus comes from many sources, both natural and human.
These include soil and rocks, wastewater treatment plants, runoff from
fertilized lawns and cropland, failing septic systems, runoff from
animal manure storage areas, disturbed land areas, drained wetlands,
water treatment, and commercial cleaning preparations. forms of
phosphorus Phosphorus has a complicated story. Pure, elemental
phosphorus (P) is rare. In nature, phosphorus usually exists as part of
a phosphate molecule (PO4). Phosphorus in aquatic systems occurs as
organic phosphate and inorganic phosphate. Organic phosphate consists of
a phosphate molecule associated with a carbon-based molecule, as in
plant or animal tissue. Phosphate that is not associated with organic
material is inorganic, the form required by plants. Animals can use
either organic or inorganic phosphate. Both organic and inorganic
phosphorus can be dissolved in the water or suspended (attached to
particles in the water column).

phosphorus cycle Phosphorus cycles though the environment, changing form
as it does so. Aquatic plants take in dissolved inorganic phosphorus as
it becomes part of their tissues. Animals get the organic phosphorus
they need by eating aquatic plants, other animals, or decomposing plant
and animal material. In water bodies, as plants and animals excrete
wastes or die, the organic phosphorus they contain sinks to the bottom,
where bacterial decomposition converts it back to inorganic phosphorus,
both dissolved and attached to particles. This inorganic phosphorus gets
back into the water column when the bottom is stirred up by animals,
human activity, chemical interactions, or water currents. Then it is
taken up by plants and the cycle begins again. In a stream system, the
phosphorus cycle tends to move phosphorus downstream as the current
carries decomposing plant and animal tissue and dissolved phosphorus. It
becomes stationary only when it is taken up by plants or is bound to
particles that settle to the bottom of ponds. In the field of water
quality chemistry, phosphorus is described by several terms. Some of
these terms are chemistry based (referring to chemically based
compounds), and others are methods based (they describe what is measured
by a particular method). The term orthophosphate is a chemistry-based
term that refers to the phosphate molecule all by itself. Reactive
phosphorus is a corresponding method-based term that describes what is
actually being measured when the test for orthophosphate is being
performed. Because the lab procedure is not quite perfect, mostly
orthophosphate is obtained along with a small fraction of some other
forms. More complex inorganic phosphate compounds are referred to as
condensed phosphates or polyphosphates. The method-based term for these
forms is acid hydrolyzable phosphorus.

monitoring phosphorus Monitoring phosphorus is challenging because it
involves measuring very low concentrations---down to 0.01 mg/L or even
lower. Even such very low concentrations of phosphorus can have a
dramatic impact on streams. Less sensitive methods should be used only
to identify serious problem areas. Although many tests for phosphorus
exist, only four are likely to be performed by most monitors: 1. The
total orthophosphate test is largely a measure of orthophosphate.
Because the sample is not filtered, the procedure measures both
dissolved and suspended orthophosphate. The USEPA-approved method for
measuring total orthophosphate is known as the ascorbic acid method.
Briefly, a reagent (either liquid or powder) containing ascorbic acid
and ammonium molybdate reacts with orthophosphate in the sample to form
a blue compound. The intensity of the blue color is directly
proportional to the amount of orthophosphate in the water. 2. The total
phosphorus test measures all the forms of phosphorus in the sample
(orthophosphate, condensed phosphate, and organic phosphate) by first
``digesting'' (heating and acidifying) the sample to convert all the
other forms to orthophosphate. The orthophosphate is then measured by
the ascorbic acid method. Because the sample is not filtered, the
procedure measures both dissolved and suspended orthophosphate. 3. The
dissolved phosphorus test measures that fraction of the total phosphorus
that is in solution in the water (as opposed to being attached to
suspended particles). It is determined by first filtering the sample,
then analyzing the filtered sample for total phosphorus. 4. Insoluble
phosphorus is calculated by subtracting the dissolved phosphorus result
from the total phosphorus result.

All of these tests have one thing in common---they all depend on
measuring orthophosphate. The total orthophosphate test measures the
orthophosphate that is already present in the sample. The others measure
that which is already present and that which is formed when the other
forms of phosphorus are converted to orthophosphate by digestion.

sampling anD eQuipment consiDerations Monitoring phosphorus involves two
basic steps:

\begin{enumerate}
\item
  Collect a water sample.
\item
  Analyze the sample in the field or lab for one of the types of
  phosphorus described above.
\end{enumerate}

Refer to Standard Methods for the specific laboratory methods.

Sample Containers Sample containers made of either some form of plastic
or Pyrex® glass are acceptable to the USEPA. Because phosphorus
molecules have a tendency to adsorb (attach) to the inside surface of
sample containers, containers that are to be reused must be acidwashed
to remove adsorbed phosphorus. The container must be able to withstand
repeated contact with hydrochloric acid. Plastic containers, either
high-density polyethylene or polypropylene, might be preferable to glass
from a practical standpoint, because they are better able to withstand
breakage. Some programs use disposable, sterile, plastic Whirl-Pak®
bags. The size of the container depends on the sample amount needed for
the phosphorus analysis method chosen and the amount needed for other
analyses to be performed.

Dedicated Labware All containers that will hold water samples or come
into contact with reagents used in this test must be dedicated. They
should not be used for other tests, to eliminate the possibility that
reagents containing phosphorus will contaminate the labware. All labware
should be acid-washed. The only form of phosphorus this text recommends
for field analysis is total orthophosphate, which requires use of the
ascorbic acid method on an untreated sample. Analysis of any of the
other forms requires adding potentially hazardous reagents, heating the
sample to boiling, and using too much time and too much equipment to be
practical. In addition, analysis for other forms of phosphorus is prone
to errors and inaccuracies in field situations. Pretreatment and
analysis for these other forms should be handled in a laboratory.

Ascorbic Acid Method In the ascorbic acid method, a combined liquid or
prepackaged powder reagent consisting of sulfuric acid, potassium
antimonyl tartrate, ammonium molybdate, and ascorbic acid (or comparable
compounds) is added to either 50 or 25 mL of the water sample. This
colors the sample blue in direct proportion to the amount of
orthophosphate in the sample. Absorbance or transmittance is then
measured after 10 minutes, but before 30 minutes, using a color
comparator with a scale in milligrams per liter that increases with the
increase in color hue, or an electronic meter that measures the amount
of light absorbed or transmitted at a wavelength of 700 to 880 nm
(again, depending on manufacturer's directions). A color comparator may
be useful for identifying heavily polluted sites with high
concentrations (greater than 0.1 mg/L); however, matching the color of a
treated sample to a comparator can be very subjective, especially at low
concentrations, and can lead to variable results. A field
spectrophotometer or colorimeter with a 2.5-cm light path and an
infrared photocell (set for a wavelength of 700 to 880 nm) is
recommended for accurate determination of low concentrations (between
0.2 and 0.02 mg/L). Use of a meter requires that a prepared known
standard concentration be analyzed ahead of time to convert the
absorbance readings of a stream sample to milligrams per liter or that
the meter reads directly in milligrams per liter. For information on how
to prepare standard concentrations and on how to collect and analyze
samples, consult the latest edition of Standard Methods or Method 365.2
of Methods for Chemical Analysis of Water and Wastes (USEPA, 1983).

NITRATES Nitrates are a form of nitrogen found in several different
forms in terrestrial and aquatic ecosystems. These forms of nitrogen
include ammonia (NH3), nitrates (NO3), and nitrites (NO2). Nitrates are
essential plant nutrients, but excess amounts can cause significant
water quality problems. Together with phosphorus, nitrates in excess
amounts can accelerate eutrophication, causing dramatic increases in
aquatic plant growth and changes in the types of plants and animals that
live in the stream. This, in turn, affects dissolved oxygen,
temperature, and other indicators. Excess nitrates can cause hypoxia
(low levels of dissolved oxygen) and can become toxic to warm-blooded
animals at higher concentrations (10 mg/L or higher) under certain
conditions. The natural level of ammonia or nitrate in surface water is
typically low (less than 1 mg/L), but in the effluent of wastewater
treatment plants it can range up to 30 mg/L. Sources of nitrates include
wastewater treatment plants, runoff from fertilized lawns and cropland,
failing on-site septic systems, runoff from animal manure storage areas,
and industrial discharges that contain corrosion inhibitors.

sampling anD eQuipment consiDerations Nitrates from land sources end up
in rivers and streams more quickly than other nutrients such as
phosphorus because they dissolve in water more readily than phosphorus,
which has an attraction for soil particles. As a result, nitrates serve
as a better indicator of the possibility of a source of sewage or manure
pollution during dry weather. Water that is polluted with nitrogen-rich
organic matter might show low nitrates. Decomposition of the organic
matter lowers the dissolved oxygen level, which in turn slows the rate
at which ammonia is oxidized to nitrite (NO2) and then to nitrate (NO3).
Under such circumstances, monitoring for nitrites or ammonia
(considerably more toxic to aquatic life than nitrate) might also be
necessary. (See Standard Methods, Section 4500-NH3 and Section 4500-NH2,
for appropriate nitrite methods). Water samples to be tested for nitrate
should be collected in glass or polyethylene containers that have been
prepared by using Method B (presented earlier). Most monitoring programs
usually use two methods for nitrate testing: the cadmium reduction
method and the nitrate electrode method. The more commonly used cadmium
reduction method produces a color reaction measured either by comparison
to a color wheel or by use of a spectrophotometer. A few programs also
use a nitrate electrode, which can measure in the range of 0 to 100 mg/L
nitrate. A newer colorimetric immunoassay technique for nitrate
screening is also now available.

Cadmium Reduction Method The cadmium reduction method is a colorimetric
method that involves contact of the nitrate in the sample with cadmium
particles, which cause nitrates to be converted to nitrites. The
nitrites then react with another reagent to form a red color, in
proportional intensity to the original amount of nitrate. The color is
measured either by comparison to a color wheel with a scale in
milligrams per liter that increases with the increase in color hue or by
use of an electronic spectrophotometer that measures the amount of light
absorbed by the treated sample at a 543-nm wavelength. The absorbance
value converts to the equivalent concentration of nitrate against a
standard curve. Methods for making standard solutions and standard
curves are presented in Standard Methods. Before each sampling run, the
sampling/monitoring supervisor should create this curve. The curve is
developed by making a set of standard concentrations of nitrate,
reacting them, and developing the corresponding color, then plotting the
absorbance value for each concentration against concentration. A
standard curve could also be generated for the color wheel. Use of the
color wheel is appropriate only if nitrate concentrations are greater
than 1 mg/L. For concentrations below 1 mg/L, use a spectrophotometer.
Matching the color of a treated sample at low concentrations to a color
wheel (or cubes) can be very subjective and can lead to variable
results. Color comparators can, however, be effectively used to identify
sites with high nitrates. This method requires that the samples being
treated are clear. If a sample is turbid, filter it through a 0.45-µm
filter. Be sure to test to make sure the filter is nitrate free. If
copper, iron, or other metals are present in concentrations above
several milligrams per liter, the reaction with the cadmium will slow
down and the reaction time must be increased. The reagents used for this
method are often prepackaged for different ranges, depending on the
expected concentration of nitrate in the stream. For example, the Hach
Company provides reagents for the following ranges: low (0 to 0.40
mg/L), medium (0 to 4.5 mg/L), and high (0 to 30 mg/L). Determining the
appropriate range for the stream being monitored is important.

Nitrate Electrode Method A nitrate electrode (used with a meter) is
similar in function to a dissolved oxygen meter. It consists of a probe
with a sensor that measures nitrate activity in the water; this activity
affects the electric potential of a solution in the probe. This change
is then transmitted to the meter, which converts the electric signal to
a scale that is read in millivolts, and the millivolts are converted to
milligrams per liter of nitrate by plotting them against a standard
curve. The accuracy of the electrode can be affected by high
concentrations of chloride or bicarbonate ions in the sample water.
Fluctuating pH levels can also affect the meter reading. Nitrate
electrodes and meters are expensive compared to field kits that employ
the cadmium reduction method. (The expense is comparable, however, if a
spectrophotometer is used rather than a color wheel.) Meter and probe
combinations run between \$700 and \$1200, including a long cable to
connect the probe to the meter. If the program has a pH meter that
displays readings in millivolts, it can be used with a nitrate probe and
no separate nitrate meter is needed. Results are read directly as
milligrams per liter. Although nitrate electrodes and spectrophotometers
can be used in the field, they have certain disadvantages. These devices
are more fragile than the color comparators and are therefore more at
risk of breaking in the field. They must be carefully maintained and
must be calibrated before each sample run or, if many tests are being
run, between samplings. This means that samples are best tested in the
lab. Note that samples to be tested with a nitrate electrode should be
at room temperature, whereas color comparators can be used in the field
with samples at any temperature.

TOTAL SOLIDS Total solids are dissolved solids plus suspended and
settleable solids in water. In stream water, dissolved solids consist of
calcium, chlorides, nitrate, phosphorus, iron, sulfur, and other
ions---particles that will pass through a filter with pores of around 2
µm (0.002 cm) in size. Suspended solids include silt and clay particles,
plankton, algae, fine organic debris, and other particulate matter.
These are particles that will not pass through a 2-µm filter. The
concentration of total dissolved solids affects the water balance in the
cells of aquatic organisms. An organism placed in water with a very low
level of solids (distilled water, for example) swells because water
tends to move into its cells, which have a higher concentration of
solids. An organism placed in water with a high concentration of solids
shrinks somewhat, because the water in its cells tends to move out. This
in turn affects the ability of the organism to maintain the proper cell
density, which makes keeping its position in the water column difficult.
It might float up or sink down to a depth to which it is not adapted,
and it might not survive. Higher concentrations of suspended solids can
serve as carriers of toxics, which readily cling to suspended particles.
This is particularly a concern where pesticides are being used on
irrigated crops. Where solids are high, pesticide concentrations may
increase well beyond those of the original application as the irrigation
water travels down irrigation ditches. Higher levels of solids can also
clog irrigation devices and might become so high that irrigated plant
roots will lose water rather than gain it. A high concentration of total
solids will make drinking water unpalatable and might have an adverse
effect on people who are not used to drinking such water. Levels of
total solids that are too high or too low can also reduce the efficiency
of wastewater treatment plants, as well as the operation of industrial
processes that use raw water. Total solids affect water clarity. Higher
solids decrease the passage of light through water, thereby slowing
photosynthesis by aquatic plants. Water heats up more rapidly and holds
more heat; this, in turn, might adversely affect aquatic life adapted to
a lower temperature regime. Sources of total solids include industrial
discharges, sewage, fertilizers, road runoff, and soil erosion. Total
solids are measured in milligrams per liter (mg/L).

sampling anD eQuipment consiDerations Total solids are important to
measure in areas where discharges from sewage treatment plants,
industrial plants, or extensive crop irrigation may occur. In
particular, streams and rivers in arid regions where water is scarce and
evaporation is high tend to have higher concentrations of solids and are
more readily affected by the human introduction of solids from land use
activities. Total solids measurements can be useful as an indicator of
the effects of runoff from construction, agricultural practices, logging
activities, sewage treatment plant discharges, and other sources. As
with turbidity, concentrations often increase sharply during rainfall,
especially in developed watersheds. They can also rise sharply during
dry weather if earth-disturbing activities occur in or near the stream
without erosion control practices in place. Regular monitoring of total
solids can help detect trends that might indicate increasing erosion in
developing watersheds. Total solids are closely related to stream flow
and velocity and should be correlated with these factors. Any change in
total solids over time should be measured at the same site at the same
flow. Total solids are measured by weighing the amount of solids present
in a known volume of sample. This is accomplished by weighing a beaker,
filling it with a known volume, evaporating the water in an oven, and
completely drying the residue, then weighing the beaker with the
residue. The total solids concentration is equal to the

difference between the weight of the beaker with the residue and the
weight of the beaker without it. Because the residue is so light in
weight, the lab must have a balance that is sensitive to weights in the
range of 0.0001 g. Balances of this type are called analytical or
Mettler balances, and they are expensive (around \$3000). The technique
requires that the beakers be kept in a desiccator---a sealed glass
container that contains material that absorbs moisture and ensures that
the weighing is not biased by water condensing on the beaker. Some
desiccants change color to indicate moisture content. The measurement of
total solids cannot be done in the field. Samples must be collected
using clean glass or plastic bottles or Whirl-Pak® bags and taken to a
laboratory where the test can be run.

CONDUCTIVITY Conductivity is a measure of the ability of water to pass
an electrical current. Conductivity in water is affected by the presence
of inorganic dissolved solids such as chloride, nitrate, sulfate, and
phosphate anions (ions that carry a negative charge) or sodium,
magnesium, calcium, iron, and aluminum cations (ions that carry a
positive charge). Organic compounds such as oil, phenol, alcohol, and
sugar do not conduct electrical current very well and therefore have a
low conductivity when in water. Conductivity is also affected by
temperature---the warmer the water, the higher the conductivity.
Conductivity in streams and rivers is affected primarily by the geology
of the area through which the water flows. Streams that run through
areas with granite bedrock tend to have lower conductivity because
granite is composed of more inert materials that do not ionize (dissolve
into ionic components) when washed into the water. On the other hand,
streams that run through areas with clay soils tend to have higher
conductivity, because of the presence of materials that ionize when
washed into the water. Groundwater inflows can have the same effects,
depending on the bedrock they flow through. Discharges to streams can
change the conductivity depending on their make-up. A failing sewage
system would raise the conductivity because of the presence of chloride,
phosphate, and nitrate; an oil spill would lower conductivity. The basic
unit of measurement of conductivity is the mho or siemens. Conductivity
is measured in micromhos per centimeter (µmho/cm) or microsiemens per
centimeter (µS/cm). Distilled water has a conductivity in the range of
0.5 to 3 µmho/cm. The conductivity of rivers in the United States
generally ranges from 50 to 1500 µmho/cm. Studies of inland freshwaters
indicate that streams supporting good mixed fisheries have a range
between 150 and 500 µmho/cm. Conductivity outside this range could
indicate that the water is not suitable for certain species of fish or
macroinvertebrates. Industrial waters can range as high as 10,000
µmho/cm.

sampling anD eQuipment consiDerations Conductivity is useful as a
general measure of stream water quality. Each stream tends to have a
relatively constant range of conductivity that, once established, can be
used as a baseline for comparison with regular conductivity
measurements. Significant changes in conductivity could indicate that a
discharge or some other

source of pollution has entered a stream. Conductivity is measured with
a probe and a meter. Voltage is applied between two electrodes in a
probe immersed in the sample water. The drop in voltage caused by the
resistance of the water is used to calculate the conductivity per
centimeter. The meter converts the probe measurement to micromhos per
centimeter (µmho/cm) and displays the result for the user. Note: Some
conductivity meters can also be used to test for total dissolved solids
and salinity. The total dissolved solids concentration in milligrams per
liter (mg/L) can also be calculated by multiplying the conductivity
result by a factor between 0.55 and 0.9, which is empirically determined
(see Method 2510 in Standard Methods). Suitable conductivity meters cost
about \$350. Meters in this price range should also measure temperature
and automatically compensate for temperature in the conductivity
reading. Conductivity can be measured in the field or the lab. In most
cases, collecting samples in the field and taking them to a lab for
testing is probably better. In this way, several teams can collect
samples simultaneously. If testing in the field is important, meters
designed for field use can be obtained for around the same cost
mentioned above. If samples will be collected in the field for later
measurement, the sample bottle should be a glass or polyethylene bottle
that has been washed in phosphate-free detergent and rinsed thoroughly
with both tap and distilled water. Factory-prepared Whirl-Pak® bags may
also be used.

TOTAL ALKALINITY Alkalinity is a measure of the capacity of water to
neutralize acids. Alkaline compounds in the water, such as bicarbonates
(baking soda is one type), carbonates, and hydroxides, remove H+ ions
and lower the acidity of the water (increasing the pH). They usually do
this by combining with the H+ ions to make new compounds. Without this
acid-neutralizing capacity, any acid added to a stream would cause an
immediate change in the pH. Measuring alkalinity is important in
determining the ability of a stream to neutralize acidic pollution from
rainfall or wastewater---one of the best measures of the sensitivity of
the stream to acid inputs. Alkalinity in streams is influenced by rocks
and soils, salts, certain plant activities, and certain industrial
wastewater discharges. Total alkalinity is determined by measuring the
amount of acid (e.g., sulfuric acid) required to bring the sample to a
pH of 4.2. At this pH, all the alkaline compounds in the sample are used
up. The result is reported as milligrams per liter of calcium carbonate
(mg/L CaCO3).

analytical anD eQuipment consiDerations For total alkalinity, a
double-endpoint titration using a pH meter (or pH ``pocket pal'') and a
digital titrator or burette is recommended. This can be done in the
field or in the lab. If alkalinity must be analyzed in the field, a
digital titrator should be used instead of a burette, because burettes
are fragile and more difficult to set up and use in the field. The
alkalinity method described below was developed by the Acid Rain
Monitoring Project of the University of Massachusetts Water Resources
Research Center (River Watch Network, 1992).

Burettes, Titrators, and Digital Titrators for Measuring Alkalinity The
total alkalinity analysis involves titration. In this test, titration is
the addition of small, precise quantities of sulfuric acid (the reagent)
to the sample until the sample reaches a certain pH (the endpoint). The
amount of acid used corresponds to the total alkalinity of the sample.
Alkalinity can be measured using a burette, titrator, or digital
titrator (described below):

• A burette is a long, graduated glass tube with a tapered tip like a
pipette and a valve that opens to allow the reagent to drip out of the
tube. The amount of reagent used is calculated by subtracting the
original volume in the burette from the column left after the endpoint
has been reached. Alkalinity is calculated based on the amount used. •
Titrators forcefully expel the reagent by using a manual or mechanical
plunger. The amount of reagent used is calculated by subtracting the
original volume in the titrator from the volume left after the endpoint
has been reached. Alkalinity is then calculated based on the amount used
or is read directly from the titrator. • Digital titrators have counters
that display numbers. A plunger is forced into a cartridge containing
the reagent by turning a knob on the titrator. As the knob turns, the
counter changes in proportion to the amount of reagent used. Alkalinity
is then calculated based on the amount used. Digital titrators cost
about \$100 or more. Digital titrators and burettes allow for much more
precision and uniformity in the amount of titrant that is used.

FECAL BACTERIA* Members of two bacteria groups (coliforms and fecal
streptococci) are used as indicators of possible sewage contamination,
because they are commonly found in human and animal feces. Although they
are generally not harmful themselves, they indicate the possible
presence of pathogenic (disease-causing) bacteria, viruses, and protozoa
that also live in human and animal digestive systems. Their presence in
streams suggests that pathogenic microorganisms might also be present
and that swimming in that water or eating shellfish from that water
might present a health risk. Because testing directly for the presence
of a large variety of pathogens is difficult, time consuming, and
expensive, water is usually tested for coliforms and fecal streptococci
instead. Sources of fecal contamination of surface waters include
wastewater treatment plants, on-site septic systems, domestic and wild
animal manure, and storm runoff. In addition to the possible health risk
associated with the presence of elevated levels of fecal bacteria, the
bacteria can also cause cloudy water, unpleasant odors, and an increased
oxygen demand.

\begin{itemize}
\item
  Much of the information presented in the following sections is adapted
  from USEPA, Test Methods for Escherichia coli and Enterococci in Water
  by the Membrane Filter Procedure, EPA 600/4-85-076, Office of Research
  and Development, U.S. Environmental Protection Agency, Cincinnati, OH,
  1985; USEPA, Bacteriological Ambient Water Quality Criteria for Marine
  and Fresh Recreational Waters, EPA 440/5-84-002, Office of Research
  and Development, U.S. Environmental Protection Agency, Cincinnati, OH,
  1986.
\end{itemize}

inDicator bacteria types The most commonly tested fecal bacteria
indicators are total coliforms, fecal coliforms, Escherichia coli, fecal
streptococci, and enterococci. All but E. coli are composed of a number
of species of bacteria that share common characteristics, including
shape, habitat, or behavior; E. coli is a single species in the fecal
coliform group. Total coliforms are widespread in nature. All members of
the total coliform group can occur in human feces, but some can also be
present in animal manure, soil, and submerged wood and in other places
outside the human body. The usefulness of total coliforms as an
indicator of fecal contamination depends on the extent to which the
bacteria species found are fecal and human in origin. For recreational
waters, total coliforms are no longer recommended as an indicator. For
drinking water, total coliforms are still the standard test, because
their presence indicates contamination of a water supply by an outside
source. Fecal coliforms, a subset of total coliform bacteria, are more
fecal specific in origin; however, even this group contains a genus,
Klebsiella, with species that are not necessarily fecal in origin.
Klebsiella are commonly associated with textile and pulp and paper mill
wastes. If these sources discharge to a local stream, consideration
should be given to monitoring more fecal and human-specific bacteria.
For recreational waters, this group was the primary bacteria indicator
until relatively recently, when the USEPA began recommending E. coli and
enterococci as better indicators of health risk from water contact.
Fecal coliforms are still being used in many states as indicator
bacteria. E. coli is a species of fecal coliform bacteria specific to
fecal material from humans and other warm-blooded animals. The USEPA
recommends E. coli as the best indicator of health risk from water
contact in recreational waters; some states have changed their water
quality standards and are monitoring accordingly. Fecal streptococci
generally occur in the digestive systems of humans and other
warm-blooded animals. In the past, fecal streptococci were monitored
together with fecal coliforms, and a ratio of fecal coliforms to
streptococci was calculated. This ratio was used to determine whether
the concentration was of human or nonhuman origin; however, this is no
longer recommended as a reliable test. Enterococcus spp. are a subgroup
within the fecal Streptococcus group. Enterococci are distinguished by
their ability to survive in saltwater, and in this respect they more
closely mimic many pathogens than do the other indicators. Enterococci
are typically more human specific than the larger fecal Streptococcus
group. The USEPA recommends enterococci as the best indicator of health
risk in saltwater used for recreation and as a useful indicator in
freshwater, as well.

Which bacteria shoulD be monitoreD? Which bacteria chosen for testing
depends on what is to be determined. Does swimming in the local stream
pose a health risk? Does the local stream meet state water quality
standards? Studies conducted by the USEPA to determine the correlation
between different bacterial indicators and the occurrence of digestive
system illness at swimming beaches suggest that the best indicators of
health risk from recreational

water contact in freshwater are Escherichia coli and Enterococcus spp.
For saltwater, enterococci are the best. Interestingly, fecal coliforms
as a group were determined to be a poor indicator of the risk of
digestive system illness; however, many states continue to use fecal
coliforms as their primary health risk indicator. If your state is still
using the total of fecal coliforms measurement as the indicator bacteria
and you want to know whether the water meets state water quality
standards, you should monitor fecal coliforms. If, however, you want to
determine the health risk from recreational water contact, the results
of the USEPA studies suggest that you should consider switching to the
E. coli or enterococci method for testing freshwater. In any case,
consulting with the water quality division of your state's environmental
agency is best, especially if you expect them to use your data.

sampling anD eQuipment consiDerations Bacteria can be difficult to
sample and analyze, for many reasons. Natural bacteria levels in streams
can vary significantly; bacteria conditions are strongly correlated with
rainfall; thus, comparing wet and dry weather bacteria data can be a
problem. Many analytical methods have a low level of precision yet can
be quite complex to accomplish; and absolutely sterile conditions are
essential to maintain while collecting and handling samples. The primary
equipment decision to make when sampling for bacteria is what type and
size of sample container you will use. Once you have made that decision,
the same straightforward collection procedure is used, regardless of the
type of bacteria being monitored. When monitoring bacteria, all
containers and surfaces with which the sample will come into contact
must be sterile. Containers made of either some form of plastic or
Pyrex® glass are acceptable to the USEPA; however, if the containers are
to be reused, they must be sturdy enough to survive sterilization using
heat and pressure. The containers can be sterilized by using an
autoclave, a machine that sterilizes with pressurized steam. When an
autoclave is used, the container material must be able to withstand high
temperatures and pressure. Plastic containers---either high density
polyethylene or polypropylene---might be preferable to glass from a
practical standpoint because they will better withstand breakage. In any
case, be sure to check the manufacturer's specifications to see whether
the container can withstand 15 minutes in an autoclave at a temperature
of 121°C without melting. (Extreme caution is advised when working with
an autoclave.) Disposable, sterile, plastic Whirl-Pak® bags are used by
a number of programs. The size of the container depends on the sample
amount needed for the bacteria analysis method you choose and the amount
needed for other analyses. The two basic methods for analyzing water
samples for bacteria in common use are the membrane filtration and
multiple-tube fermentation methods. Given the complexity of the analysis
procedures and the equipment required, field analysis of bacteria is not
recommended. Bacteria can either be analyzed at a well-equipped lab or
sent to a state-certified lab for analysis. If a bacteria sample is sent
to a private lab, make sure that the lab is certified by the state for
bacteria analysis. Consider state water quality labs, university and
college labs, private labs, wastewater treatment plant labs, and
hospitals. This text does not address laboratory methods, because

several bacteria types are commonly monitored and the methods are
different for each type. For more information on laboratory methods,
consult the latest edition of Standard Methods. Note: If you decide to
analyze your samples in your own lab, be sure to carry out a quality
assurance/quality control program.

APPARENT COLOR Some aspects of water quality can be judged by its color.
Noticeable color is an objectionable characteristic that makes the water
psychologically unacceptable to the consumer (DeZuane, 1997). Pure water
is colorless, but water in nature is often colored by foreign
substances. The color in water that is partly due to dissolved solids
that remain after removal of suspended matter is known as true color.
Apparent color (the topic of this section) results from dissolved
substances and suspended matter and provides useful information about
the source and content of the water. Simply stated, when turbidity is
present, so is apparent color. Natural metallic ions, plankton, algae,
industrial pollution, and plant pigments from humus and peat may all
produce color in water. Pure water absorbs different wavelengths
(colors) of light at different rates. Blue and blue--green light are the
wavelengths best transmitted through water, so a white surface under
``colorless'' water looks blue (e.g., Caribbean and some South Pacific
Island waters above white sand). Over the years, several attempts to
standardize the method of describing the ``apparent'' color of water
using comparisons to color standards have been made. Standard Methods
recognizes the visual comparison method as a reliable method of
analyzing water from the distribution system. One of the visual
comparison methods is the Forel--Ule Color Scale, consisting of a dozen
shades ranging from deep blue to khaki green, typical of offshore and
coastal bay waters. By using established color standards, people in
different areas can compare test results. Another visual comparison
method is the Borger Color System, which provides an inexpensive,
portable color reference for shades typically found in natural waters
and can also be used for its original purpose---describing the colors of
insects and larvae found in streams or lakes. The Borger Color System
also allows the recording of the color of algae and bacteria on stream
beds. Note: Do not leave color standard charts and comparators in direct
sunlight. Measured levels of color in water can serve as indicators for
a number of conditions; for example, transparent water with a low
accumulation of dissolved minerals and particulate matter usually
appears blue and indicates low productivity. Yellow to brown color
normally indicates that the water contains dissolved organic materials,
humic substances from soil, peat, or decaying plant material water.
Deeper yellow to reddish colors in water indicate the presence of some
algae and dinoflagellates in the water. Water rich in phytoplankton and
other algae appears green. A variety of yellow, reds, browns, and grays
is indicative of soil runoff. To ensure reliable and accurate
descriptions of apparent color, use a system of color comparison that is
reproducible and comparable to the systems used by other groups.

ODOR Odor in water is caused by chemicals that may come from municipal
and industrial waste discharges or natural sources such as decomposing
vegetable matter or microbial activity. Odor affects the acceptability
of drinking water, the aesthetics of recreation water, and the taste of
aquatic foodstuffs. A wide variety of smells can be accurately detected
by the human nose, which is the best odor-detection and -testing device
currently available. To measure odor, collect a sample in a large
mouthed jar. After waving off the air above the water sample with your
hand, smell the sample. Use the list of odors provided in Standard
Methods (a system of qualitative description that helps monitors
describe and record detected odors; see Table 10.6) to describe the
smells. Record all observations.

SUMMARY All of the elements that comprise the standard practices
associated with proper water monitoring provide drinking water
practitioners with the technical and scientific data necessary to
determine the level and types of treatment required to successfully
condition water obtained from surface and groundwater sources. Drinking
water treatment is covered in Chapter 11.

Water Treatment Developing world cities with private water-management
companies have been plagued by lapses in service, soaring costs,
corruption and worse. In Manila, where the water system is controlled by
Suez, San Francisco-based Bechtel and the prominent Ayala family, water
is only reliably available for a few hours a day, and rate increases
have been so severe that the poorest families must choose each month
between paying for water and two days' worth of food. In 2001 the
government of Ghana agreed to privatize local water systems as a
condition for an IMF loan. To attract investors, the government doubled
water rates, setting off protests in a country where the average annual
income is less than \$400 a year and the water bill (for those fortunate
enough to have running water) can run upwards of \$110. In Cochabamba,
the third-largest city in Bolivia, water rates shot up by 35 per cent
after a consortium led by Bechtel took over the city's water system in
1999; some residents found themselves paying 20 per cent of their income
on water. An initial round of peaceful street protests led to riots in
which six people were killed. Eventually, the Bolivian government voided
Bechtel's contract and told the company's officials it could not
guarantee their safety if they stayed in town. Privatization has also
spawned protests (and, in some cases, even dominated elections) in
Paraguay, where police turned water cannons on anti-privatization
protesters, Panama, Brazil, Peru, Colombia, India, Pakistan, Hungary and
South Africa. Louma (2004)

It is common practice to treat wastewater to the point where it is
cleaner than the local waterways into which it is ultimately released.
Eventually, it arrives at the ocean, with absolutely no downstream
use---this is referred to as one-and-done usage. Why waste such a
valuable resource? Why not reuse it? But we do already reuse it to some
extent \ldots{} through de facto water recycling, as shown in Figure
11.1.

INTRODUCTION We live on a planet with a surface that is three-fourths
covered with water, so we recognize the irony inherent in the fact that
many areas of the world face critical shortages of drinking water. Most
of Earth's water is seawater, of course---far too saline for human
consumption. Of the little ``fresh'' water that remains, most is trapped
in polar ice caps, where harnessing it for use is difficult. Much of the
accessible natural supply of potable water is stressed by a growing
world population, which increases the basic demand for this natural
resource while reducing the supply further through contamination. Major
population centers in developing nations (those without established
waste treatment or water treatment infrastructure) often suffer from
epidemics of waterborne disease. In these areas, raw sewage can directly
contaminate the rivers and streams used for drinking, washing, and
cooking. In other cases, unchecked industrialization leads to water
contamination through improperly disposed of chemical and nuclear
wastes. The drinking water purveyor must ensure that the drinking water
supplied is safe for human consumption. In fact, the primary reason for
the development and installation of a public water system is the
protection of public health. Basically, a properly operated water system
serves as a line of defense between disease and the public. Properly
operated water treatment and supply systems are defined as those that

• Remove or inactivate pathogenic microorganisms including bacteria,
viruses, and protozoa. • Reduce or remove chemicals that can be
detrimental to health. • Provide quality water, thus discouraging the
customer from seeking better tasting or better looking water that may be
contaminated.

This last point is critical, but one often overlooked in the operation
and management of public water systems. When the water produced by a
system is objectionable because of odor, taste, or appearance, customers
will seek another source for their drinking water. Ironically, these
alternative sources, although they look, taste, and smell fine (``better
than city water''), could contain microorganisms or chemicals that are
harmful. This chapter discusses the drinking water practitioner's most
important function: ensuring that water delivered to the public is
properly treated and arrives as the clean, wholesome, safe product that
it must be. Moreover, it also covers the innovative approach taken by
the Hampton Roads Sanitation District (HRSD) to replacing one-and-done
usage with one-and-redone usage.

CONVENTIONAL WATER TREATMENT A typical water treatment plant treats
stream or river water (turbid surface water with organics) and processes
the raw water using various unit processes, including: (1) screening,
(2) coagulation, (3) flocculation, (4) sedimentation or settling, (5)
filtra- tion, (6) hardness treatment, (7) disinfection, and (8)
fluoridation (see Figure 11.2). This chapter provides a brief overview
of each of these unit processes, which constitute a typical drinking
water treatment system for surface water supplies, in addition to a
brief discussion of alternative approaches.

screening Screening (the first important step in treating water
containing large solids) is defined as the process whereby relatively
large and suspended debris is removed from the water before it enters
the plant. River water (the source of water used in our discussion)
frequently contains suspended and floating debris varying in size from
small rags to logs. Removing these solids is important, not only because
these items have no place in potable water but also because this river
trash may cause damage to downstream equipment (e.g., clog and damage
pumps), may increase chemical requirements, may impede hydraulic flow in
open channels or pipes, or may hinder the treatment process (Pankratz,
1995). The most important criteria used in the selection of a particular
screening system for water treatment technology are the screen opening
size and flow rate. Other important criteria include costs related to
operation and equipment, plant hydraulics, debris handling requirements,
and operator qualifications and availability. Large surface water
treatment plants may employ a variety of screening devices, such as
trash screens (or trash rake), traveling water screens, drum screens,
bar screens, or passive intake screens. Each of these screening devices
is briefly discussed in the following sections. Trash Screens (Rakes)
Trash screens or trash rakes are used to remove rough or large debris
retained on a trash rack. They protect pumping equipment and may be used
as a preliminary screening device to protect finer screens---drum or
traveling water screens, for example. A trash screen consists of one or
more stationary trash rakes and a screen raking device. Trash rack bar
spacings range from 1.5 to 4 inches and are mostly constructed of steel
bars. Those constructed of high-density polyethylene polymers are
beginning to replace many of the older steel bar models---these
synthetic screens are lighter and less prone to microbial growth,
corrosion, and ice. Raking mechanisms are available for use in a variety
of intake configurations, including installation on vertical building
and dam walls. Rakes are typically mounted on fixed structures designed
to clean a single trash rack, suspended from an overhead gantry,
wheelmounted to traverse the entire width of an intake structure and
clean individual sections of a wide trash rack, or suspended from an
overhead gantry. Traveling Water Screens Traveling water screens
(sometimes called bandscreens) are placed in a channel of flowing water
to remove floating or suspended debris. These automatically cleaned
screening devices protect pumping or other downstream equipment from
debris in surface water intakes. Consisting of a continuous series of
wire mesh panels bolted to basket frames, or trays, and attached to two
matched strands of roller chain, the traveling water screen operates in
a vertical path over a sprocket assembly through the flow. As raw water
passes through the revolving baskets, debris is collected and retained
on the upstream face of the wire mesh panels. The debris-laden baskets
are lifted out of the flow and above the operating flow, where a
high-pressure water spray directed outward removes the impinged debris.
This process can be continuous or intermittent. For intermittent
operation, the screen activates when a specified headloss or time
elapsed has occurred. When located on a river, traveling water screens
may be subject to large fluctuations in flow conditions, debris loading,
water depths, and salinity. Depending upon application, the size of the
traveling water screen is determined by considering such factors as
maximum and average flow; maximum, minimum, and average water levels;
wire mesh size; velocity through mesh; basket or channel width; number
of screens; type of service; and/or starting and operating headloss
requirements. Drum Screens A drum screen (or cylinder screen) has very
few moving parts and is mounted on a horizontal axis with a series of
wire mesh panels attached or mounted on the periphery of its cylinder.
The cylinder slowly rotates on its axis. Because of its simplicity of
construction, the maintenance requirements and operating costs of a drum
screen installation are usually lower than those of a traveling water
screen.

Bar Screens Primarily used in wastewater treatment applications, bar
screens are also employed in some water treatment facilities. A bar
screen consists of straight steel bars welded at both ends to two
horizontal steel members and is automatically cleaned by one or more
power operated rakes. As a rake is operated up the face of the bar rack,
it removes accumulated debris (usually large solid objects and rags) and
elevates in and out of the flow. At the top of the operating cycle of
the rake, the debris is swept from the rake into a debris receptacle by
a wiper mechanism. When installed in a waterway, the bar screen assembly
normally is placed at an angle of 60 to 80 degrees from the horizontal.

Passive Intake Screens Passive intake screens (stationary screening
cylinders) have no moving parts and require no debris handling or debris
removal equipment. Passive intake screens are placed in a surface water
body in such a manner so as to take advantage of natural ambient
currents and controlled through-screen velocities to minimize debris
buildup. Usually mounted on a horizontal axis and oriented parallel to
the natural current flow within the water body, current flow action
works to keep the screen clean. Maximum intake velocity of about 0.5
foot per second (fps) is typical and works to minimize debris
impingement on the screen surface.

coagulation Coagulation, the second step in water purification, is a
unit process that has been used for several years in the treatment of
raw water. Basically, coagulation works to settle very fine material of
suspended solids. Note: Chemicals employed for coagulation are expected
to be safe for drinking water when used according to the American Water
Works Association (AWWA) coagulation standards (e.g., Coagulation, Nos.
42402 to 42407).

Coagulants Typically, after screening, raw water is pumped into large
settling basins, also known as clarifiers or sedimentation tanks. Within
the confines of the settling basin, the screened raw water is allowed to
sit for some predetermined time. Although screened, the raw water still
contains impurities that may be either dissolved or suspended. The
settling basin provides the most convenient way to remove the suspended
matter, as it lets the force of gravity do the work. Within the basin,
when flow and turbulence are minimal (quiescent conditions), particles
more dense than water settle to the bottom of the tank. This process is
called sedimentation, and the layer of accumulated solids at the bottom
of the tank is called sludge (or biosolids in some wastewater treatment
unit processes). The size and density of the suspended particles have a
direct bearing on the speed at which they will settle toward the bottom
of the basin. The larger or heavier particles will, of course, settle
faster than smaller or lighter particles. The forces opposing the
downward force of gravity include buoyancy and drag (friction). The
particle-settling rate is also affected by the temperature and viscosity
of the water.

Note: The nature of the sedimentation process also varies with the
concentration of suspended solids and their tendency to interact with
one another.

In the sedimentation process just described, not all suspended solids or
particles can be completely removed from water, even when given very
long detention times. Very small particles called colloids (e.g.,
bacteria, fine clays, silts) will not settle out of suspension by
gravity without some help. This is where coagulants come into play. If
we rapidly mix chemical coagulants in the water and then slowly stir the
mixture before allowing sedimentation to occur, the colloidal particles
will settle. Colloids or finer particles must be chemically coagulated
to produce larger floc that is removable in subsequent settling and
filtration. The coagulation process (along with flocculation) works to
neutralize or reduce the natural repelling electrical force of particles
in water, keeping them apart and in suspension. Particles in water
usually carry a negative electrical charge. Because all of these
particles carry this same negative electrical charge, they repel each
other---in the same way that like poles of a magnet do. The object of
coagulation (and subsequently flocculation) is to turn the small
particles into larger flocs, either as precipitates or suspended
particles. These flocs are then conditioned for ready removal in
subsequent processes. Stated another way, in this text we define
coagulation as a method to alter the colloids so they will be able to
approach and adhere to each other to form larger floc particles.

Types of Coagulants Two types of coagulants are used in the coagulation
process: coagulants and coagulant aids. Generally, the types of
coagulants and aids available are defined by the plant process scheme.
To determine optimum chemical dosages for treatment, jar tests are
normally used. Jar Tests Jar tests are widely used to simulate a
full-scale coagulation and flocculation process to determine optimum
chemical dosages---the cost-effective dose of a coagulant for the time
and intensity of agitation selected. Such tests have been used for many
years by the water treatment industry; the test conditions are intended
to reflect the normal operation of a chemical treatment facility and
allow evaluation of the type and quantity of sludge and physical
properties of the floc. The test can be used to * Select the most
effective chemical. * Select the optimum dosage.Determine the value of a
flocculant aid and the proper dose. The testing procedure requires a
series of samples to be placed in testing jars and mixed at 100 rpm.
Varying amounts of the process chemical or specified amounts of several
flocculants are added (one volume/sample container). The mix is
continued

for one minute. The mixing slows to 30 rpm to provide gentle agitation,
then the floc is allowed to settle. The flocculation period and settling
process are observed carefully to determine the floc strength,
settleability, and clarity of the supernatant liquor (the water that
remains above the settled floc). The supernatant can then be tested to
determine the efficiency of the chemical addition for removal of total
suspended solids (TSS), biochemical oxygen demand (BOD), and phosphorus.
The equipment required for the jar test includes a six-position,
variable-speed paddle mixer; six 2-quart wide-mouth jars; an interval
timer; and assorted glassware, pipettes, graduates, and so forth.

Coagulation Chemicals Several different chemicals can be used for
coagulation. Commonly used metal coagulants are those based on aluminum
(aluminum sulfate) and those based on iron (ferric sulfate). The most
common coagulant is aluminum sulfate (alum, Al2(SO4)3). Other common
coagulation chemicals are provided in Table 11.1.

Coagulant Aids Coagulation problems often occur because of slow-settling
precipitates or fragile flocs that are easily fragmented under hydraulic
forces in basins and filters (Hammer and Hammer, 1996). A coagulant aid
is a chemical added during coagulation to improve coagulation; to build
stronger, more settleable floc; to overcome the effect of temperature
drops that slow coagulation; to reduce the amount of coagulant needed;
and to reduce the amount of sludge produced (AWWA, 1995). Coagulant aids
benefit

flocculation by improving the settling qualities and toughness of flocs.
Polymers are the most widely used materials. Synthetic polymers are
water-soluble, high-molecular-weight organic compounds with multiple
electrical charges along a molecular chain of carbon atoms. In drinking
water treatment, polymers are extensively used as coagulant aids to
build large floc prior to sedimentation and filtration. Other coagulant
aids are activated silica, adsorbent weighting agents, and oxidants.

Coagulation Process Operation The common coagulation unit process
operation involves the addition of coagulant chemicals by rapid
mixing---detention time in the rapid mix tank is typically on the order
of minutes (Masters, 1991). During this mixing process, polymer (or some
other coagulant aid) is added and blended into the destabilized water
prior to flocculation. The removal of impurities by coagulation depends
on their nature and concentration, the use of both coagulants and
coagulants aids, and characteristics of the water, including pH,
temperature, and ionic strength. Because of the complex nature of
coagulation reactions, chemical treatment is based on empirical data
derived from jar testing or other laboratory tests and field studies
(Viessman and Hammer, 1998).

flocculation The destabilized particles and chemical precipitates
resulting from coagulation are designed to enhance their settling
qualities and thus their removal from water; however, even after
coagulation has taken place, these particles and chemical precipitates
may still settle very slowly (too slowly). To speed up the settling
process, flocculation is employed. Note: Flocculation is the clumping
together of the fine particles formed by coagulation. Although the two
terms are often used interchangeably, flocculation and coagulation are
actually distinct concepts. Flocculation is the most important factor
affecting particle-removal efficiency. In water treatment operations,
flocculation is a slow mixing process in which the coagulated particles
are brought into contact so they will collide, stick together, and grow
(agglomerate) to a size that will readily settle. Enough mixing must be
provided (e.g., gentle agitation for approximately half an hour) to
bring the floc particles into contact with each other and to keep the
floc from settling in the flocculation basin. (The heavier the floc and
the higher the suspended solids concentration, the more mixing is
required to keep the floc in suspension.) The most common type of mixer
or flocculator is the paddle type, which uses redwood slats mounted
horizontally on motor-driven shafts. Rotating slowly at about one
revolution per minute, the paddles provide gentle agitation that
promotes floc growth. The rate of agglomeration or flocculation depends
on the number of particles present, the relative volume that they
occupy, and the velocity gradient in the basin. Note: The statement that
the rate of agglomeration or flocculation depends on velocity gradient
refers to the fact that too much mixing can shear the floc particles,
tearing them apart again; the floc then becomes smaller and more finely
dispersed, a situation we are obviously trying to avoid. For this
reason, the velocity gradient must be controlled within a relatively
narrow range. The theory of flocculation is complex and beyond the needs
of this text, but on an elemental level we can say that flocculation is
generally accomplished by slowly rotating, large-diameter mixers.
Current practice incorporates dispersion of the coagulant (flash
mixing), flocculation, and sedimentation in a single unit called a
contact clarifier. Note: Flocculation is the principal mechanism for
removing turbidity from water.

seDimentation In a conventional water treatment plant, the process of
coagulation and flocculation precedes the sedimentation process for
better results and improved utilization of the settling basins.
Sedimentation is then followed by the filtration process. Filtration
occasionally may be preceded only by coagulation, in which case
filtration is provided after only a few minutes of contact, adding
additional stress to the filters. Lack of sedimentation results in less
reliable operation of filters when water quality suddenly changes
characteristics (DeZuane, 1997). Sedimentation (also known as
clarification) is the gravity-induced removal of particulate matter,
chemical floc, precipitates from suspension, and other settleable
solids. Simply stated, sedimentation separates the liquid from the
solids. The process takes place in a rectangular, square, or round tank
called a settling or sedimentation tank or basin. Flow patterns within
such basins may be rectilinear flow in rectangular basins, radial flow
in center-feed settling tanks or square settling tanks, or radial flow
or spiral flow in peripheral-feed settling tanks. Sedimentation, in the
conventional water treatment process, is typically the step between
flocculation and filtration. Design criteria are based on empirical data
from the performance of full-scale sedimentation tanks. The common
criteria for sizing settling basins are detention time (typically from 1
to 10 hr), overflow rate, weir loading, and, with rectangular tanks,
horizontal velocity. In water treatment, the majority of settling basins
are essentially upflow clarifiers where the water rises vertically for
discharge through effluent channels. More specifically, in the idealized
sedimentation tank, water flows horizontally through the basin and then
rises vertically, overflowing the weir of a discharge channel at the
tank

surface. Floc settles downward, opposite the upflow of water, and is
removed from the bottom by a continuous mechanical sludge removal
apparatus. The particles with a settling velocity greater than the
overflow rate are removed (settled) while lighter flocs are carried out
in the effluent. The effluent is then filtered. Note: Sedimentation
tanks, either circular or rectangular, are designed for slow, uniform
water movement with a minimum of short-circuiting.

filtration Even after chemical coagulation and sedimentation by gravity,
not all of the suspended solids or impurities are removed from water.
Nonsettleable floc particles (about 5\% of the suspended solids) may
still remain in the water, and with only that small percentage left we
might ask, ``Isn't this good enough?'' No, it isn't. This remaining floc
would cause problems (including noticeable turbidity), and particles
shield microorganisms from the subsequent disinfection processes. The
goal of water treatment is to produce potable water that is perceptually
crystal clear and that satisfies the Safe Drinking Water Act (SDWA)
requirement of 0.5 NTU for turbidity. To accomplish this, an additional
treatment step is required that follows coagulation, flocculation, and
sedimentation. Filtration (sometimes called a polishing process)
involves the removal of suspended particles from water by passing it
through a layer or bed of a porous granular material---sand, for
example. As water flows through the filter bed, the suspended particles
become trapped within the pore spaces of the filter material (or filter
media). When purifying a surface water source (as in the discussion
here), filtration is a very important process, even though filtration is
only one step in the overall treatment process. Note: Filtration is the
process that occurs naturally as surface waters migrate (percolate)
through the porous layers of soil to recharge groundwater. This natural
filtration removes most suspended matter and microorganisms and is the
reason why many wells produce water that does not require any further
treatment. The Surface Water Treatment Rule (SWTR) specifies certain
filtration technologies. The most common treatment filter systems
include rapid gravity filters (either built onsite or packaged plants)
and pressure filters. Other types include direct filtration, slow sand
filters, and diatomaceous earth (DE) filters. The SWTR also allows the
use of alternative filtration technologies, such as cartridge filters.

Filtration treatment unit processes most commonly used in water
purification systems include slow or rapid sand filtration, diatomaceous
earth filtration, and package filtration systems. Slow and rapid filter
systems refer to the rate of flow per unit of surface area. Filters are
also classified by the type of granular material used in them. Sand,
anthracite coal, coal--sand, multilayered, mixed bed, and diatomaceous
earth are examples of different filtering media. Filtration systems may
also be classified by the direction the water flows through the medium:
downflow, upflow, fine-tocoarse, coarse-to-fine. Finally, filters are
commonly distinguished by whether they are gravity or pressure filters.
Gravity filters rely only on the force of gravity to move the water down
through the grains and typically use upflow for washing (backwashing)
the filter media to remove the collected foreign material. Gravity
filters are free surface filters commonly used for municipal
applications. Pressure filters are completely enclosed in a shell so
most of the water pressure in the lines leading to the filter is not
lost and can be used to push the water through the filter.

Rapid Filter Systems Slow sand filtration has been used in the United
States since 1872. It is still used in many older plants but is not
commonly used today in most modern water treatment plants because of
various problems associated with this technique. One of the problems is
related to the tiny size of the pore spaces in the fine sand, which
slows down the water's progress through the filter bed. These filter
types also have problems with suspended particles clogging the surface,
requiring the filter to be manually scraped clean. These units take up a
considerable amount of land area because slow filtration rates require a
greater filter surface area to produce the necessary filtered water
qualities. In modern water treatment plants, the rapid filter has
largely replaced the slow sand filter. The rapid filter consists of a
layer of carefully sieved silica sand ranging from 0.6 to 0.75 m in
depth on top of a bed of graded gravels. The pore openings between the
grains of sand are often greater than the size of the floc particles
that are to be removed, so much of the filtration is accomplished by
means other than simple straining. Note: The ideal filter medium is
coarse enough for large pore openings to retain large quantities of
floc, yet sufficiently fine to prevent the passage of suspended solids.
It must have adequate depth to allow relatively long filter runs and be
graded to permit effective cleaning during backwash.

Adsorption, continued flocculation, and sedimentation in the pore spaces
are also important removal mechanisms. When the filter becomes clogged
with particles (which occurs approximately once a day, depending on the
turbidity of the water), the filter is shut down for a short period of
time and cleaned by forcing water backward through the sand for 10 to 15
minutes. After cleaning, the sand settles back in place and operation
resumes.

Other Common Filter Types Rapid flow filters are the most common type
used for treating water supplies, primarily because they are the most
reliable, but other types of filters are sometimes used to clarify
water, including pressure filters and diatomaceous earth filters. A
pressure filter is similar to a rapid filter in that the water flows
through a granular filter bed; however, instead of being open to the
atmosphere and using the force of gravity, the pressure filter is
enclosed in a cylindrical steel tank and the water is pumped through the
bed under pressure. They are not as reliable as rapid filters, because
pressure may force solids through the bed in the effluent. Because of
this problem, they are seldom employed in municipal water treatment
works but instead are used for filtering water for industrial use or for
swimming pools. Diatomaceous earth filters contain a thin layer of a
natural, powdery material formed from the shells of diatoms; they are
also used primarily for industrial or swimming pool applications because
they are not as reliable as rapid sand filters.

harDness treatment Two commonly used methods to reduce hardness are the
lime-soda process and ion exchange. The lime-soda process is applicable
for large facilities, whereas ion exchange is normally employed in
smaller water works. The lime-soda process will not remove all of the
hardness and is usually operated to produce a residual hardness of about
100 mg/L as CaCO3. Greater reductions are not economical and may have
adverse health consequences as well (McGhee, 1991). The discussion in
this text focuses on ion exchange. Ion exchange is accomplished by
charging a resin with sodium ions and allowing the resin to exchange the
sodium ions for calcium or magnesium ions. Common resins include
zeolites---natural and manmade minerals that will collect from a
solution certain ions (sodium or KMnO4), and either exchange these ions
(in the case in water softening) or use the ions to oxidize a substance
(in the case of iron or manganese removal). The negative side of using
ion exchange is that, even though the process softens water by removing
all (or nearly all) of the hardness and adds sodium ions to the water,
the water may be more corrosive than before. The addition of sodium ions
to the water may also increase the health risk of those with high blood
pressure.

Disinfection At the turn of the last century, 35,000 people per
1,000,000 people did not reach 20 years of age. Today, however, the rate
of births exceeds the rate of deaths, and the average lifespan is much
longer. Curbing waterborne disease through disinfection has made a
significant contribution to birth rates outpacing death rates worldwide.
The Safe Drinking Water Act requires that public water supplies be
disinfected, and the U.S. Environmental Protection Agency (USEPA) sets
standards and establishes processes for the treatment and distribution
of disinfected water to ensure that no significant risks to human health
occur. The USEPA Science Advisory Board has ranked pollutants in
drinking water as one of the highest health risks meriting the Agency's
attention because of large-scale population exposure to contaminants,
including lead, disinfectants and disinfection byproducts (DBPs), and
disease-causing organisms. Disinfectants are used by virtually all
surface water systems in the United States and by an unknown percentage
of systems that rely on groundwater. For nearly a century, chlorine has
been the most widely used and most cost-effective disinfectant; however,
disinfection treatments can produce a wide variety of byproducts, many
of which have been shown to cause cancer or other toxic effects.
Recently, concern has been raised over water quality deterioration, a
problem that can grow dramatically during distribution unless systems
are properly designed and operated. Disinfection is an integral part of
water treatment, but filtration prior to disinfection is necessary to
reduce pathogen levels and make disinfection more reliable by removing
turbidity and other interfering constituents. To solve the disinfectant
and disinfection byproducts problem, we need innovative upgrades for the
existing techniques, as well as new approaches to address these
problems. Areas of interest include

• Alternatives to chlorine disinfection for removing pathogenic
microorganisms, including innovative applications of ultraviolet (UV)
radiation and processes that improve overall effectiveness while using
reduced amounts of disinfectant • Development of innovative unit
processes, particularly for small systems, for removal of organic and
inorganic contaminants (such as arsenic), particulates, and pathogens,
such as cyst-like organisms and emerging pathogens such as
caliciviruses, microsplorida (septata and enterocytozoan), hepatitis A
virus, Mycobacterium avium--intracellulare complex (MAC), Helicobacter
pylori, Legionella pneumophila, adenovirus 40/41/1-39, and Toxoplasma
gondii • Development of efficient, cost-effective treatment processes
for removing disinfection byproduct precursors (e.g., trihalomethanes,
haloacetic acids), for ozonation (bromate, aldehydes), for chlorination
(chloropicrin, haloacetonitriles), and for chloramination (organic
chloramines, cyanogen chloride) • Improved methods for controlling
pathogens through coagulation/settling, filtration, or other
cost-effective means • Drinking water contamination control between the
treatment plant and the user, especially considering potential chemical
leaching from distribution system materials and surfaces (e.g., lead,
copper, iron, and other pipe materials; protective coatings) as a result
of instability, interaction with microorganisms, disinfection agents,
and water treatment chemicals\\
The unit processes described thus far---screening, coagulation,
flocculation, sedimentation, and filtration---together comprise a type
of treatment called clarification. Along with removing turbidity and
suspended solids, clarification also removes many microorganisms from
the water; however, clarification by itself is not sufficient to ensure
the complete removal of pathogenic bacteria and viruses. Earlier it was
stated that one of the primary goals of water treatment is to treat raw
water to the point where it is possible to deliver to the consumer a
water product that is perceptually crystal clear. Obviously, the
consumer does not want to drink a glass full of mud, a glass full of
slime, a glass full of metal-colored, foul-smelling water--- or even a
glass of water that looks like it was dipped from a creek. Would you?
The point is, when the water has been treated to the point of crystal
clarity, the treatment process must be taken a step further---to the
point where the water is completely free of disease-causing
microorganisms. To accomplish this, the final treatment process in water
treatment plants occurs---disinfection, which destroys or inactivates
pathogens.

Key Disinfection Terms* Before moving on to a discussion of the major
disinfection methods used in treating water for human consumption, it is
necessary to first define a few pertinent terms related to disinfection
in general. To begin with, let's establish the distinction between
primary and secondary disinfection:

• Primary disinfection---Initial killing of Giardia cysts, bacteria, and
viruses • Secondary disinfection---Maintenance of a disinfectant
residual that prevents regrowth of microorganisms in the water
distribution system between treatment and consumer

Other terms the reader should understand include

• Disinfection---Inactivation of virtually all recognized pathogenic
microorganisms, but not necessarily all microbial life (which would be
considering pasteurization or sterilization). • Disinfectant---(1) Any
oxidant, including but not limited to, chlorine, chlorine dioxide,
chloramine, and ozone, added to water in any part of the treatment or
distribution process that is intended to kill or inactivate pathogenic
microorganisms. (2) A chemical or physical process that kills pathogenic
organisms in water; chlorine is often used to disinfect sewage treatment
effluent, water supplies, wells, and swimming pools. • Disinfectant
time---The time required for water to move from one point of
disinfectant application (or the previous point of residual disinfectant
measurement) to a point before or at the point where the residual
disinfectant is measured. • Disinfectant contact time (T in C×T
calculation)---The time (in minutes) required for water to move from the
point of disinfectant application or the previous point of disinfection
residual measurement to a point before or at

\begin{itemize}
\item
  Adapted from Spellman, F.R., Disinfection Alternatives, Technomic,
  Lancaster, PA, 1999.
\end{itemize}

the point where residual disinfectant concentration (C) is measured.
Where only one C is measured, T is the time (in minutes) required for
water to move from the point of disinfectant application to a point
before or at where residual disinfectant concentration (C) is measured.
Where more than one C is measured, T is defined as follows: • For the
first measurement of C, the time (in minutes) required for water to move
from the first or only point of disinfectant application to a point
before or at the point where the first C is measured • For subsequent
measurements of C, the time in minutes that water takes to move from the
previous C measurement point to the C measurement point for which the
particular T is being calculated • Disinfection byproduct---A compound
formed by the reaction of a disinfectant such as chlorine with organic
material in the water supply. • Presence or absence of
coliforms---Presence of coliform bacteria in water is an indication that
the water may be contaminated by pathogenic organisms. Absence of
coliform bacteria is considered to be sufficient evidence that pathogens
are absent---if the source is good, a chlorine residual level is
maintained and the supply has a good history. • Sterilization---The
destruction of all microorganisms. Sterilizing potable water requires
the application of a much higher dose of chemical disinfectants, which
would greatly increase operating costs and would create taste problems
for the consumer. Excessive application of disinfectants also generates
excessive levels of unwanted disinfection byproducts. For these reasons,
current treatment practices are used for turbidity removal and
subsequent disinfection to the extent necessary to eliminate known
diseasecausing organisms sufficient to protect public health. Note:
Sterilization should not be confused with disinfection. • Waterborne
disease---Caused by pathogenic organisms in water.

Disinfection Methods Although chlorination is the best known and the
most common disinfection method, other methods are available and can be
used in various situations. The three general types of disinfection are
• Heat treatment---Probably one of the first methods employed to
disinfect water was to boil it. For small quantities of water, boiling
water is still a good emergency procedure to use. • Radiation
treatment---Uses ultraviolet radiation to disinfect water.

• Chemical treatment---Employs the use of chemicals to disinfect water.
Examples of chemical disinfectants include oxidizing agents such as
chlorine, ozone, bromine, iodine, and potassium permanganate; metal ions
such as silver, copper, and mercury; and acids and alkalis.

Obviously, several different disinfectants are available for use in
treating water, and several of these are discussed in detail in
subsequent sections. For now, it is important to understand that, even
though several choices are available, whichever disinfectant is chosen
must meet certain criteria---more specifically, the disinfectant chosen
must be effective for disinfecting water (and wastewater) and must
possess certain desirable characteristics. Desirable Characteristics of
a Disinfectant 1. It must act in a reasonable time. 2. It must act as
temperature or pH changes. 3. It must be nontoxic. 4. It must not add
unpleasant taste or odor. 5. It must be readily available. 6. It must be
safe and easy to handle and apply. 7. It must be easy to determine the
concentration of. 8. It must be able to provide residual protection. 9.
Pathogenic organisms must be more sensitive to the disinfectant than are
nonpathogens. 10. It must be capable of being applied continually. 11.
The amount applied must be sufficient to produce a safe water.

In addition to the desirable characteristics of a disinfectant listed
above, the disinfectant chosen must be able to kill off or deactivate
pathogenic microorganisms by one of several possible methods, including:
(1) damaging the cell wall, (2) altering the ability to pass food and
waste through the cell membrane, (3) altering the cell protoplasm, (4)
inhibiting the cells' conversion of food to energy, or (5) inhibiting
reproduction.

Chlorination For the past several decades, chlorine dispensed as a solid
(calcium hypochlorite), liquid (sodium hypochlorite), or gas (elemental
chlorine, Cl2) has been the disinfectant of choice, particularly in the
United States. Chlorine (sometimes referred to as the workhorse of
disinfection) has proven its worth both because of its effectiveness and
because it is relatively inexpensive; it also provides a chlorine
residual in the water distribution system, ensuring that the water
remains disease free. Gaseous chlorine (Cl2), 2.5 times as heavy as air,
is a greenish-yellow toxic gas. One volume of liquid chlorine confined
in a container under pressure yields about 450 volumes of gas. Large
water treatment works usually use chlorine gas, supplied in liquid form,
in high-strength, high-pressure steel cylinders. The liquid immediately
vaporizes in the form of gas when released from these pressurized
containers. Chlorine gas is lethal at concentrations as low as 0.1\% air
by volume. In nonlethal concentrations, it irritates the eyes, nasal
membranes, and respiratory tract.

Sodium hypochlorite is most commonly used in smaller systems, because it
is simpler to use and has less extensive safety requirements than
gaseous chlorine; in the form used, it is less toxic. Recently, many
larger water facilities that have used chlorine for disinfection are
beginning to substitute sodium hypochlorite for chlorine because of
regulatory pressure. Note: The Occupational Safety and Health
Administration's Process Safety Management Standard (29 CFR 1910.119)
and USEPA's Risk Management Program (Clean Air Act, Section 112(r)(7))
have come to be known in the industry as the ``chlorine killers,''
because of their effect on industrial processes. The USEPA is attempting
to steer industry away from the use of chlorine. Although the Agency
cannot absolutely outlaw this substance from use, it is following the
path of simply regulating it to death. In an effort to avoid having to
comply with strict (in some cases, unworkable) regulations, many water
treatment and wastewater facilities in the United States are
substituting some other chemical product that is not regulated (at least
for the moment) such as sodium hypochlorite. Sodium hypochlorite
provides 5 to 15\% available chlorine (common laundry bleach is a 5\%
solution of sodium hypochlorite). Usually diluted with water before
application as a disinfectant, it is very corrosive and should be
handled and stored with care and kept away from equipment that can be
damaged by corrosion. Sodium hypochlorite solution is more costly per
pound of available chlorine and does not provide the same level of
protection of chlorine gas.

Calcium hypochlorite is a white solid in granular, powdered, or tablet
form containing 65\% available chlorine. In packaged form, calcium
hypochlorite is stable--- more stable than solutions of sodium
hypochlorite, which deteriorate over time; however, calcium hypochlorite
is hygroscopic, which means it readily absorbs moisture. It reacts
slowly with moisture in the air to form chlorine gas. It is a corrosive
material with a strong odor and requires proper handling. Some practical
difficulty is involved in dissolving calcium hypochlorite. It must be
kept away from organic materials such as wood, cloth, and petroleum
products. Reactions between it and organic materials can generate enough
heat to cause a fire or explosion.

Chlorine Use Whatever form of chlorine is used for disinfection
(elemental chlorine, sodium hypochlorite, or calcium hypochlorite), it
may be added to the incoming flow (prechlorination) to assist with the
oxidation of inorganics or to arrest biological action that may produce
undesirable gases in the bottom of clarifiers. More often, however,
chlorine is added just prior to filtration to keep algae from growing at
the medium surface and to prevent large populations of bacteria from
developing within the filter medium. Safe and effective application of
chlorine requires specialized equipment and considerable care and skill
on the part of the plant operator. Various means of feeding chlorine
have been developed, but probably one of the widest used and safest
types of chlorine feed devices is the all-vacuum chlorinator. Mounted
directly on the chlorine cylinder, the gaseous chlorine is always under
a partial vacuum in the line that carries it to the point of
application. In a typical vacuum chlorine feed system, the vacuum is
formed by water flowing through the ejector unit at high velocity.

Hypochlorites are usually applied to water in liquid form by means of
positive displacement-type pumps, which deliver a specific amount of
liquid on each stroke of a piston or flexible diaphragm. Chlorine, when
added to water, reacts with various substances or impurities in the
water (e.g., organic materials, sulfides, ferrous iron, nitrites), which
creates a chlorine demand. Chlorine demand is a measure of the amount of
chlorine that will combine with impurities and is therefore available to
act as a disinfectant. Chlorine combines with ammonia or other nitrogen
compounds to form chlorine compounds that have some disinfectant
properties. These compounds are called combined available chlorine
residual. In the context used here, ``available'' means available to act
as a disinfectant. The uncombined chlorine that remains in the water
after combined residual is formed is called free available chlorine
residual. Free chlorine is a much more effective disinfectant than
combined chlorine.

Factors Affecting Successful Chlorination The factors important to
successful chlorination are

• Concentration of free chlorine • Contact time • Temperature • pH •
Turbidity

The effectiveness of chlorination is directly related to the contact
time with and concentration of free available chlorine. At lower
chlorine concentrations, contact times must be increased. Maintaining a
lower pH will also increase the effectiveness of disinfection. The
higher the temperature, the faster the disinfection rate. Chlorine (or
any other disinfectant for that matter) is effective only if it comes
into contact with the organisms to be killed. Good contact between
chlorine and microorganisms is prevented whenever high turbidity levels
exist. For this and aesthetic reasons, turbidity should be reduced where
necessary through the coagulation and sedimentation methods previously
discussed. Chlorination Byproducts A serious disadvantage of
chlorination is the potential formation of byproducts. Chlorine, for
example, can mix with the organic compounds in water (such as decaying
vegetation) to form trihalomethanes (THMs). One THM, chloroform, is a
suspected carcinogen. Other common trihalomethanes are similar to
chloroform and may cause cancer. At the present time, about 90\% of U.S.
water utilities use chlorine to disinfect water. Although chlorine has
virtually eliminated the risks of waterborne disease such as typhoid
fever, cholera, and dysentery, recent studies have shown risks
associated with byproducts of chlorine---a reason why water utilities
already have been looking at alternative methods for disinfecting water.
Several approaches for reducing harmful chlorination byproducts have
been used. For example, one approach is to remove more of the organics
before any chlorination takes place. This can be accomplished (to a
degree) by not chlorinating the incoming

raw water before coagulation and filtration, thus reducing the formation
of THMs. Aeration or adsorption on activated carbon will remove organic
materials at higher concentrations or those not removed by other
techniques. Another approach is to reevaluate the amount of chlorine
used---the same degree of disinfection might be possible with lower
chlorine dosages. Changing the point in treatment where chlorine is
added is another approach commonly employed; rather than adding chlorine
as chemical feed during coagulation, sedimentation, or filtration, it
can instead be added after filtration. Another current approach is using
alternative disinfection methods. Note: Because of OSHA's Process Safety
Management (PSM) standard and USEPA's Risk Management Program (RMP),
many facilities currently using elemental chlorine have used or are
actively pursuing the use of alternative disinfection methods. We
further reemphasize that the problem of THMs is also helping spur
interest in alternatives to chlorination as the preferred method of
disinfection.

Alternative Disinfection Methods Currently, several alternative
disinfection methods are available for use in treating water, but the
following discussion focuses on two of these alternatives: ozonation and
ultraviolet (UV) radiation. These commonly used alternatives (especially
in small water treatment systems) are also increasingly being
substituted for existing chlorination systems at larger plants because
of regulatory pressure. Note: Before discussing the ozonation and
ultraviolet disinfection alternatives, it is important to point out that
neither one of these two alternative disinfectants is an easy solution
to problems created by chlorination. It is true that each has the
advantages of not creating THMs and not being covered by the
requirements under the PSM standard and RMP, but each has uncertainties
and known disadvantages that have restricted their more widespread use.
In addition, ozonation and ultraviolet irradiation cannot be used as
disinfectants by themselves. Both require secondary disinfectant
(usually chlorine) to maintain a residual in the distribution system.

Ozonation Ozone (O3), a gas at ordinary temperature and pressures, is a
very powerful disinfectant that breaks up molecules in water; it is even
more effective against some viruses and cysts than chlorine. It has the
added advantage of leaving no taste or odor and is unaffected by pH or
the ammonia content of the water. When ozone reacts with reduced
inorganic compounds and with organic material, an oxygen atom instead of
a chloride atom is added to the organics, the end result being an
environmentally acceptable compound. But, because ozone is unstable and
cannot be stored, it must be produced onsite. Ozonation usually costs
more than chlorination. Ultraviolet Ultraviolet (UV) light is
electromagnetic radiation just beyond the blue end of the light
spectrum, outside the range of visible light. It has a much higher
energy level than visible light, and in large doses it inactivates both
bacteria and viruses. UV energy is absorbed by genetic material in the
microorganisms, interfering with their ability to reproduce and survive,
as long as the radiation contacts the microorganisms without
interference from turbidity. The big advantage of UV disinfection over
chlorine and

ozone is that UV does not involve chemical use. Generally, UV light used
for disinfecting water is generated by a series of submerged,
low-pressure mercury lamps. Continuing advances in UV germicidal lamp
technology are making UV disinfection a more reliable and economical
option for disinfection in many plants.

NONCONVENTIONAL WATER TREATMENT TECHNOLOGIES Stage 1 of the USEPA's
Disinfectants and Disinfection Byproduct Rule and the Interim Enhanced
Surface Water Treatment Rule, designed to significantly lower THM
byproducts of chlorine disinfection in water, has driven (along with the
regulatory requirements of the PSM standard and RMP) many water and
wastewater treatment utilities to find and use alternative disinfection
methodologies. Although ozonation and ultraviolet irradiation might be
suitable disinfection alternatives, switching from chlorine to chlorine
dioxide (a chemical that has been proven to form fewer THMs) might also
be another viable disinfection alternative. Whichever disinfection
alternative is ultimately selected, remember that the selection is
driven not only by regulatory requirements but also by site-specific
requirements. The disinfection issues covered to this point are
important---the overall ramifications of regulatory pressure and
environmental impact cannot be overstated---but other issues besides
disinfection must be considered when deciding which water treatment
methodology to employ. Most of the time, clarification by coagulation,
flocculation, sedimentation, and filtration removes suspended impurities
and turbidity from drinking water, and disinfection (the final step in
the process) produces potable water, free of harmful pathogens. Simply
put, the water treatment processes discussed in the previous sections of
this part of the text are usually sufficient to render most natural
surface water (such as a river) potable. In some instances, however, the
water supply may contain materials that are not removed by conventional
water treatment processes, and other treatment processes may be required
to remove many of the dissolved organic and inorganic substances.
Examples would include groundwater with excessive dissolved solids and
surface waters containing organic compounds from domestic or industrial
wastewaters or organics occurring naturally such as humic acid or
products of algae blooms. Additional processes are available for
removing these contaminants. Note: These additional water treatment
processes involve sophisticated equipment and require highly skilled
operators; therefore, they are quite expensive (Peavy et al., 1985).
Additional water unit treatment processes may be used in addition to
clarification or applied separately, depending on the source and quality
of the raw water. Let's take a closer look at groundwater. The question
is---does a typical groundwater source require treatment beyond
conventional means? The answer is that groundwater does not normally
require processing by the unit treatment steps listed above, other than
disinfection, because groundwater is filtered naturally by the layers of
soil from which it is withdrawn. Disinfection is only applied (in many
cases) as a precautionary step required by law for public water systems.
Groundwater is usually free

of bacteria or other microorganisms; however, that all groundwater comes
into contact with soil and rock is a cause for concern. With such
contact, groundwater may become contaminated by high levels of dissolved
minerals that must be removed.

FLUORIDATION Fluoride, when added to drinking water supplies in small
concentrations (about 1.0 mg/L), can be beneficial. In some locations,
common practice is to mix a 4\% solution of sodium fluoride and feed
that into the flow of the water system. The amount that is fed depends
on the air temperature and on the fluoride levels in the raw water.
Experience has shown that drinking water containing a proper amount of
fluoride can reduce tooth decay by 65\% in children. Fluoride combines
chemically with tooth enamel when permanent teeth are forming, and the
result is teeth that are harder, stronger, and more resistant to decay.
The USEPA sets the upper limits for fluoride in drinking water supplies
based on ambient temperatures; for example, people drink more water in
warmer climates, so fluoride concentrations should be lower in these
areas.

WATER TREATMENT OF ORGANIC AND INORGANIC CONTAMINANTS Manmade compounds
that contain carbon---synthetic organic chemicals (SOCs)--- are, from
time to time, detected in U.S. water supplies. Some of these are
volatile organic chemicals (VOCs), such as the solvent
trichloroethylene. The problem with VOCs in a water supply (i.e., any
water supply used by the public) is twofold. They are easily absorbed
through the skin and they volatize into gases that can then be inhaled
by those taking a shower or a bath or while washing dishes. How do water
supplies become contaminated by organic compounds? Good question.
Basically, sources of organic contaminants are usually improperly
disposed wastes, pesticides, industrial effluents, and leaking fuel oil
tanks (gasoline in particular). Water supplies may also contain
inorganic contaminants consisting mainly of substances occurring
naturally in the ground, such as sulfate, fluoride, arsenic, barium,
radium, selenium, and radon. Metallic substances from industrial sources
can contaminate surface waters. The inorganic ion nitrate (from
fertilizers and feedlot runoff in agricultural areas) occurs frequently
in groundwater supplies. Another source of inorganic chemical
contamination in drinking water supplies is corrosion or deterioration
of water supply equipment, such as plumbing systems, which release metal
and nonmetal substances into the water, including lead, cadmium, zinc,
copper, iron, and plumbing cement. Inorganic contaminants can be treated
by corrosion controls and removal techniques. Corrosion controls reduce
the presence of corrosion byproducts (e.g., lead) at the consumer's tap.
Removal technologies, coagulation and filtration, reverse osmosis, and
ion exchange are used to treat source water that is contaminated with
metals or radioactive substances. The following sections discuss
processes for removing inorganic and organic dissolved solids from water
intended for potable use. Keep in mind that (with some modifications)
these same processes may act as tertiary treatment for wastewater.

aeration Aeration (air stripping) is a physical treatment process in
which air is thoroughly mixed with water---a technique effective for
removing dissolved gases and highly volatile odorous compounds. Contact
with air and oxygen can improve water quality in a number of ways. When
aeration is a first step in processing well water, for example, it may
achieve any or all of the following: removal of hydrogen sulfide,
reduction of dissolved carbon dioxide, and addition of dissolved oxygen
for oxidation of iron and manganese (the oxygen in the air reacts with
the iron and manganese to form an insoluble precipitate---rust). One of
the most common uses of aeration is for taste and odor control.
Sedimentation and filtration are then necessary to clarify the water.
Note: Aeration is rarely effective in processing surface waters, simply
because the odor-producing substances are generally nonvolatile. Several
methods to aerate the water are available. The method selected depends
primarily on the type and concentration of material to be removed from
the water and on the available pressure. Aeration in water treatment can
be accomplished using spray nozzles, cascade systems, multiple-tray
aerators, diffused-air aerators, and mechanical aerators.

oxiDation Simply stated, oxidation is a reaction in which a substance
loses electrons, thus increasing its charge. A substance that oxidizes
another is referred to as an oxidizing agent or oxidizer. In water
treatment, oxidation is used to remove or destroy undesirable tastes or
odors, to aid in removal of iron and manganese, and to help improve
clarification and color removal in source water. Chlorine dioxide,
potassium permanganate, and ozone are strong oxidants capable of
destroying many odorous compounds. Because they do not produce THMs,
these chemicals are favored over heavy chlorination. Note: Atmospheric
oxygen, through aeration, can be used to oxidize the organic substances
responsible for undesirable tastes and odors, but the process is usually
too slow to be of value. If dissolved gases such as hydrogen sulfide are
the cause of taste and odor problems, aeration will effectively remove
them through oxidation and stripping.

aDsorption When we speak of adsorption, we are referring primarily to a
surface phenomenon---the adsorption that results when one substance
attaches itself to the surface of another. The two most common
adsorptive media used in water treatment are activated carbon and
activated alumina. These adsorptive materials are generally most
effective for taste and odor control and for removal of organic
pollutants; however, the most important applications of adsorption in
water treatment are the removal of arsenic and organic pollutants.

Adsorption of organic materials using activated carbon has been a common
practice in water treatment for many years. Activated carbon is
manufactured from carbonaceous material such as wood, coal, and
petroleum residues. A char is made by burning the material in the
absence of air, and it is then oxidized at higher temperatures to create
a very porous structure. This activation step provides irregular
channels and pores in the solid mass, resulting in a very large
surface-area-to-mass ratio. This large surface area gives activated
carbon its effectiveness as an adsorbing agent. The larger the surface
area of an adsorber, the greater its power. Each activated carbon
contains a huge number of pores and crevices into which organic
molecules enter and are adsorbed onto the activated carbon surface.
Activated carbon has a particularly strong attraction for organic
molecules such as the aromatic solvents benzene, toluene, and
nitrobenzene; the chlorinated aromatics polychlorinated biphenyls
(PCBs), chlorobenzenes, and chloronaphthalene; phenol and chlorophenols;
the polynuclear aromatics acenaphthene and benzopyrenes; pesticides and
herbicides; chlorinated aliphatics such as carbon tetrachloride and
chloroalkyl ethers; and high-molecular-weight hydrocarbons such as dyes,
gasoline, amines, and humics. Two forms of activated carbon are used in
water treatment: powdered and granular. Powdered activated carbon is
often used for taste and odor control. Its effectiveness depends on the
source of the undesirable tastes and odors. It is also effective in
removing the organic precursors that react with chlorine to form harmful
THM compounds after disinfection. Powdered activated carbon is a finely
ground, insoluble black powder that can be added at any point in the
treatment process ahead of the filters. It is fed to water either as a
dry powder or as a wet slurry. Although adsorption is nearly
instantaneous, a contact time of 15 minutes or more is desirable before
sedimentation or filtration. Activated carbon media must periodically be
replaced with new or regenerated activated carbon. Replacement cycles
can vary from 1 to 3 years for taste and odor treatment to as little as
4 or 5 weeks for removal of organics. The activated carbon regeneration
process involves (1) removing the spent carbon as a slurry, (2)
dewatering the slurry, (3) feeding the carbon into a special furnace
where regeneration occurs (i.e., the organics are driven from the carbon
surface by heat), and (4) returning it to use. Activated alumina (a
highly porous and granular form of aluminum oxide) is also an adsorptive
medium used in water treatment. It is used primarily to remove arsenic
and excess fluoride ions. Water is percolated through a column of
alumina media, and a combination of adsorption and ion exchange performs
the actual removal of arsenic and fluoride ions. Like the regeneration
process used to restore used activated carbon to full potency, activated
alumina also requires periodic regeneration, accomplished by passing a
caustic soda solution through the media. Excess caustic soda is
neutralized by rinsing the activated alumina with an acid. Disposal of
these wash waters, laden with toxic arsenic and fluoride ions, must be
done in accordance with applicable laws. Note: Powdered activated carbon
is much more difficult to regenerate than granular activated carbon.
Granular activated carbon is sometimes used in the filter bed itself,
combining both filtration and adsorption in one treatment unit. The
major problem associated with granular activated carbon systems is
suspended solids in the water plugging up the bed.

Demineralization Demineralization refers to the removal of dissolved
solids (inorganic mineral substances) from water. Dissolved solids
contain both cations and anions and therefore require two types of ion
exchange resins. Cation exchange resins used for demineralization
purposes have hydrogen exchange sites and are divided into strong acid
and weak acid classes. The anion exchange resins commonly used contain
hydroxide ions and are divided into strong and weak base classes.
Demineralization is commonly used in industry in waste treatment for
removal of arsenic, barium, cadmium, chromium, fluoride, sulfate, and
zinc. Some general advantages of using ion exchange to remove these
contaminants are the low capital investment required and the mechanical
simplicity of the process. In addition, the ion exchange process can be
used to recover valuable chemicals for reuse, or harmful ones for
disposal. For example, it is often used to recover chromic acid from
metal finishing waste for reuse in chromeplating baths. It also has some
application in the removal of radioactivity. The major disadvantages are
the high chemical requirements needed to regenerate the resins and to
dispose of chemical wastes from the regeneration process. These factors
make ion exchange more suitable for small systems than for large ones.

membrane processes Membrane processes used in water treatment are
primarily demineralization processes. Demineralization of water can be
accomplished using thin, microporous membranes. Electrodialysis and
reverse osmosis are the most common membrane processes. Before we
briefly discuss these two membrane processes, you need a basic
understanding of osmosis. During osmosis, two solutions containing
different concentrations of minerals are separated by a semipermeable
membrane. Water tends to migrate through the membrane from the side of
the more dilute solution to the side of the more concentrated solution.
This is osmosis, and it continues until the build-up of hydrostatic
pressure on the more concentrated solution is sufficient to stop the net
flow. In reverse osmosis, the flow of water through the semipermeable
membrane is reversed by applying external pressure to offset the
hydrostatic pressure. This results in a concentration of minerals on one
side of the membrane and pure water on the other side. Reverse osmosis
can treat for a wide variety of health and aesthetic contaminants in
water. Effectively designed, reverse osmosis equipment can treat
aesthetic contaminants that cause unpleasant taste, color, and odor
problems, such as a salty or soda taste caused by chlorides or sulfates.
Reverse osmosis can also be effective for treating arsenic, asbestos,
atrazine, fluoride, lead, mercury, nitrate, and radium. When used with
appropriate carbon prefiltering, additional treatment can also be
provided for such ``volatile'' contaminants as benzene,
trichloroethylene, trihalomethanes, and radon. Some reverse osmosis
equipment is also capable of treating for Cryptosporidium. Reverse
osmosis can be expected to play a major role in water treatment for
years to come. Reverse osmosis (also called ultrafiltration) is the most
common process for reducing the salinity of brackish groundwater. In
operation, a semipermeable membrane (the most essential element in the
reverse osmosis method of demineralization)

separates salty water of two different concentrations. Concentrations
have a natural tendency to become equalized by a flow of water from the
dilute side to the concentrated side (osmosis). But high pressure
applied to the high concentration side of the membrane can reverse this
direction of flow. Freshwater diffuses through the membrane, leaving a
more concentrated salt solution behind. The performance of reverse
osmosis units is highly dependent on a number of water quality
parameters. Suspended solids, dissolved organics, hydrogen sulfide,
iron, and strong oxidizing agents (chlorine, ozone, and permanganate)
are harmful to membranes. Electrodialysis is the demineralization of
water using the principles of osmosis---but it uses ion-selective
membranes and an electric field to separate anions and cations in
solution. In the past, electrodialysis was most often used for purifying
brackish water, but it is now finding a role in industrial waste
treatment as well. For example, metals salts from plating rinses are
sometimes removed in this way.

A PARADIGM SHIFT IN PROGRESS Water shortage is the lack of adequate
accessible water resources to meet water needs within a locality. More
than 1.2 billion people lack access to clean drinking water (United
Nations, 2017). For localities where access to drinking water is readily
available, an issue that is not necessarily recognized at this time is
the one-and-done scenario discussed earlier---that is, safe drinking
water quality water is drawn from a tap and used for a variety of
purposes and that is that. After being used, this water is poured down
drains or flushed down toilets---out of sight and out of mind. But, a
significant paradigm shift is beginning to occur. The idea of
toilet-to-tap reuse is not palatable to many people, but we need water.
We cannot live without water. Fortunately, we can clean used water and
reuse it, a task that Mother Nature often can do for us naturally. We
have no other choice. Regions where water is readily accessible today
may not be able to brag about that in the future. Population growth,
overuse, misuse, abuse, and other events and actions affect water use
and have detrimental impacts on water quality. We need to change the
one-and-done scenario to a one-and-redone scenario by using technology
to purify used water. The use of advanced treatment and purification of
used water (wastewater) to drinking water quality is a paradigm change
in progress.

ADVANCED TREATMENT OF WASTEWATER TO DRINKING WATER QUALITY Advanced
technologies and processes used for wastewater treatment and
purification provided at indirect potable reuse (IPR) plants varies (see
Figure 11.3) but are typically focused on providing multiple barriers
for the removal of pathogens and organics. Nitrogen and TDS removal is
provided at some locations where necessary. Table 11.2 shows most of the
indirect potable reuse projects that have been implemented in the United
States. The table has been sorted according to the type of potable reuse
application (i.e., direct aquifer injection, aquifer recharge with
surface spreading, and surface water augmentation). The first five
projects shown in this table are direct injection

projects that match the proposed HRSD concept. Water extracted from
direct injection and surface spreading projects that recharge
groundwater is not typically treated again prior to distribution into
the potable water system; however, water from surface water augmentation
projects is typically treated again at water treatment plants because of
water treatment requirements stipulated by the USEPA's Surface Water
Treatment Rule (SWTR). For example, Fairfax County's Griffith Water
Treatment Plant provides coagulation, sedimentation, ozone oxidation,
biological activated carbon filtration, and chlorine disinfection for
water extracted from the Occoquan Reservoir that is augmented by the
Upper Occoquan Service Authority's indirect potable reuse plant. As
shown in Table 11.2, the treatment provided for indirect potable reuse
projects is typically a combination of multiple barriers for the removal
of pathogens and organics. Multiple barriers for pathogens are typically
provided through a combination of coagulation, flocculation,
sedimentation, lime clarification, filtration (granular or membrane),
and disinfection (chlorine, ultraviolet, or ozone). Multiple barriers
for organics removal are typically provided through a combination of
advanced treatment processes (e.g., reverse osmosis, granular activated
carbon, ozone in combination biological activated carbon), although
conventional treatment processes (e.g., coagulation, softening) also
provide removal at some locations. All potable reuse plants listed in
Table 11.2 include a robust organics removal process of granular
activated carbon (GAC), granular media filtration (GMF), biological
activated carbon (BAC), reverse osmosis (RO), microfiltration (MF),
ultraviolet advanced oxidation process (UVAOP), membrane bioreactor
(MBR), or soil aquifer treatment (SAT), which are effective barriers to
bulk and trace organics and represent the backbone of the potable
treatment process. SAT is the controlled application of wastewater to
earthen basins in permeable soils at a rate typically measured in terms
of meters of liquid per week. The purpose of a soil aquifer treatment
system is to provide a receiver aquifer capable of accepting liquid
intended to recharge shallow groundwater, and system design and
operating criteria are developed to achieve that goal. However, there
are several alternatives with respect to the utilization or final fate
of the treated water (USEPA, 2006):

• Groundwater recharge • Recovery of treated water for subsequent reuse
or discharge • Recharge of adjacent surface streams • Seasonal storage
of treated water beneath the site with seasonal recovery for agriculture
The SAT process typically includes application of the reclaimed water
using spreading basins and subsequent percolation through the vadose
zone. SAT provides significant removal of both pathogens and organics
through biological activity and natural filtration. However, because
some aquifers are confined, it is not possible to utilize the SAT for
treatment through the vadose zone to recharge them. On the other hand,
movement of reclaimed water through the aquifer after direct injection
will provide significant treatment benefits, including excellent removal
of pathogens. Advanced water treatment plants based on reverse osmosis
and granular activated carbon are often utilized at locations where SAT
treatment through the vadose zone is not feasible, because it is
possible for these processes to be implemented at most locations. As a
case in point, consider that in the Hampton Roads region of Chesapeake
Bay in 1607, when Captain John Smith and his team settled in Jamestown,
oysters up to 13 inches in size were plentiful---more than could ever be
harvested and consumed by the handful of early settlers. This population
of oysters and other aquatic lifeforms remained plentiful until the
population gradually increased in the Bay region. Overharvesting of
oysters by the increased numbers of humans living in the Chesapeake Bay
region has been a major factor in the decline of the oyster population;
however, the real culprit is pollution. Before the Bay became polluted
by sewage, sediment, and garbage, oysters could handle natural pollution
from stormwater runoff and other sources. It has been estimated that a
century ago, when there was a much larger oyster population than today,
the oyster population could filter pollutants from the Bay and clean it
up in as little as 4 days. By the 1930s, however, the declining oyster
population was overwhelmed by the increasing pollution levels. For
years, the author has stated that pollution is a judgment call. Why a
judgment call? Because people's opinions differ as to what they consider
to be a pollutant based on their assessment of benefits and risks to
their health and economic wellbeing. For example, visible and invisible
chemicals spewed into the air or water by an industrial facility might
be harmful to people and other forms of life living nearby, but if the
facility is required to install expensive pollution controls it might
have to shut down or move away. Workers who would lose their jobs and
merchants who would lose their livelihoods might feel that the risks
posed by polluted air and water are minor weighed against the benefits
of profitable employment. The same level of pollution can also affect
two people quite differently. Some forms of air pollution, for example,
might cause a slight irritation for a healthy person but cause
life-threatening problems for someone with chronic obstructive pulmonary
disease, such as emphysema. Differing priorities lead to differing
perceptions of pollution (e.g., concern about the level of pesticides in
foodstuffs that leads to wholesale banning of insecticides is unlikely
to help the starving). No one wants to hear that cleaning up the
environment is going to have a negative impact on them. Public
perception lags behind reality because the reality is sometimes
unbearable. In addition to the dwindling oyster population, the current
problems with Chesapeake Bay are multifaceted; for example, land
subsidence and relative sealevel rise in the Hampton Roads region are
ongoing issues. However, HRSD has developed the innovative Sustainable
Water Initiative for Tomorrow (SWIFT) program to address many of these
issues. Do not associate the acronym SWIFT with the adjectives fast,
speedy, rapid, hurried, immediate, or quick. SWIFT is a long-term
project, such that installation of the technical equipment and
operational procedures have an anticipated completion date of 2030. The
SWIFT goal is to inject treated wastewater into the subsurface;
specifically, it is designed to inject wastewater treated to drinking
water quality into the Potomac Aquifer. Injection of water into the
subsurface is expected to raise groundwater pressures, thereby
potentially expanding the aquifer system, raising the land surface, and
counteracting land subsidence occurring in the Virginia

Coastal Plain. In 2016, construction began on a project at the HRSD
Nansemond treatment plant in Suffolk, Virginia, to test injection into
the aquifer system. HRSD asked the U.S. Geological Survey to prepare a
proposal for installation of an extensometer monitoring station at the
test site to monitor groundwater levels and aquifer compaction and
expansion. PROBLEM SWIFT is designed to counter land subsidence at
various locations in the Hampton Roads area of southern Chesapeake Bay,
where land subsidence rates of 1.1 to 4.8 millimeters per year have been
observed (Eggleston and Pope, 2013; Holdahl and Morrison, 1974). An
obvious indicator of relative sea-level rise in the Hampton Roads region
can be seen in Figure 11.4. The dashed lines outline the original 1607
fort palisades at the historic Jamestown Settlement on the James River.
As shown, the western section of the original bulwark is completely
inundated today; a section of the eastern bulwark is also covered by the
James River. A major cause of land subsidence is extensive groundwater
pumping, which causes regional aquifer system compaction (Pope and
Burbey, 2004). Although aquifer system compaction was measured from the
late 1970s to the mid-1990s at two stations in Virginia (Suffolk and
Franklin), it has not been measured anywhere in Virginia since 1995.
Regular and highly accurate measurements of aquifer compaction are
needed to provide information critical for understanding coastal
flooding; to protect water resources, natural habitat, and historic
sites such as the Jamestown Settlement; and to plan urban and coastal
infrastructure in the region. Injection of treated wastewater (treated
to drinking water quality) is expected to counteract land subsidence, or
raise land surface elevations in the region. Careful monitoring of
aquifer system compaction and groundwater levels can be used to optimize
the injection process and to improve fundamental understanding of the
relation between groundwater pressures and aquifer system compaction and
expansion. There is more to HRSD's treated wastewater injection project,
SWIFT, than just arresting or mitigating land subsidence and relative
sea-level rise in the Hampton Roads region (see Figure 11.5). One of the
additional goals of the project is to stop discharge of treated
wastewater from seven of its plants, which would mean 18 million pounds
a year less of nitrogen, phosphorus, and sediment outfalling into the
bay. Assuming SWIFT works as designed this is a huge benefit to
Chesapeake Bay in that it may help to prevent or reduce the formation of
algae bloom dead zones. Not only would success with the project benefit
the bay but it would also be a huge benefit for the ratepayers at HRSD.
To meet regulatory guidelines to remove nutrients from discharged
treated wastewater would cost millions of dollars and almost non-stop
retrofitting at the treatment plants to keep up with advances in
treatment technology and regulatory requirements. Another goal of HRSD's
SWIFT project is to restore or restock potable groundwater supplies in
the local aquifers. The drawdown of water from the groundwater supply
has contributed to land subsidence and a reduction of water available
for potable use. The planned restocking of the Hampton Roads groundwater
supply with injected wastewater treated to potable water quality is not
without its critics, who state that HRSD's wastewater injection project
would contaminate potable water aquifers. This is where the so-called
``yuck factor'' comes into play. The yuck factor, in this particular
instance, has to do with the thought that groundwater for consumptive
use will be contaminated, basically, with toilet water. This is the
common view of many of the critics who feel that HRSD's SWIFT project is
nothing more than direct reuse of wastewater; that is, a pipe-to-pipe
connection of toilet water to their home water taps. What the critics
and others do not realize is that we are already using and drinking
treated and recycled toilet water, as was explained earlier in the
discussion about an urban water cycle where wastewater is indirectly
used. The point is, whether we like it or not, we are using recycled
wastewater for potable water use. With regard to the idea that HRSD's
SWIFT project would contaminate existing aquifers with toilet water, it
is important to point out that this water will be treated (and already
is at the York River Treatment Plant) to drinking water quality, with to
drinking water quality being key here. A sophisticated and extensive
train of unit drinking water quality treatment processes produces
treated wastewater that the HRSD general manager and several others have
drunk right out of the process, which is discussed in detail later in
the text. The bottom line is that the yuck factor involved in drinking
treated toilet water is being grossly overstated. WasteWater to Drinking
Water Quality Now that HRSD's SWIFT program goals have been described,
it is important to return to the focus of this book, drinking water. As
pointed out, one of HRSD's main goals is to produce drinking water
quality water for human use. How can this be accomplished? How is nasty,
polluted, contaminated, flushed, drained wastewater turned into a
product that is safe to drink? HRSD uses a treatment train of unit
processes that produce quality drinking water Advanced treatment trains
based on reverse osmosis and granular activated carbon were developed
for the HRSD groundwater recharge project using the historical WWTP
effluent water quality data and the preliminary aquifer recharge water
quality goals discussed previously. Three treatment trains were
developed from this analysis: (1) reverse osmosis (RO)-based train, (2)
nanofiltration (NF)-based train, and (3) granular activated carbon
(GAC)-based train (see Figure 11.3). Consideration for each of these
treatment trains includes the following:

\begin{enumerate}
\item
  RO-based train---Reverse osmosis is common for potable water reuse
  projects in California and many international locations (e.g.,
  Singapore, Australia) because of its effective removal of total
  dissolved solids (TDS), total organic carbon (TOC), and trace
  organics. California regulations require the use of RO for direct
  injection reuse projects or a comparable alternative with regulatory
  approval. RO creates a waste (concentrate) stream that can be
  difficult and costly to dispose of, especially at inland locations.
  Most locations where RO has been implemented are located near the
  ocean where disposal of RO concentrate is convenient and much less
  costly than at inland locations.
\item
  NF-based train---Although similar to the RO-based train, the NF-based
  train operates at significantly lower pressure and generates a less
  saline concentrate, which results in significant cost savings. This
  process does not meet California's IPR regulatory requirements but it
  does provide excellent treatment with significant removal of pathogens
  and organics. This train offers partial TDS removal by providing a
  high level of removal of divalent ions (e.g., calcium, magnesium) and
  moderate removal of monovalent ions (e.g., sodium chloride). NH3 and
  NOx-N removal is much lower with NF compared to RO, which results in a
  lower total nitrogen concentration in the concentrate.
\item
  GAC-based train---This is a modernized version of full-scale
  operational IPR plants that have operated successfully for decades in
  Virginia, Texas, and Georgia. GAC adsorption serves as the backbone
  process for organics removal, and other treatments provide multiple
  barriers to pathogens and organics. Flocculation and sedimentation
  remove solids, pathogens, organics, and phosphorus. Ozone provides
  disinfection of pathogens and oxidation of organics, including
  oxidation of contaminants of emerging concern and
  high-molecular-weight organic matter to smaller organic fractions that
  can be assimilated by biological activity present on GAC media, which
  is referred to as biological activated carbon (BAC) filtration. This
  treatment train does not provide any TDS removal and, therefore, does
  not generate a TDS-enriched waste that might require further treatment
  prior to discharge.
\end{enumerate}

treatment plant effluent Water Quality HRSD provided historic effluent
water quality data for seven WWTPs to identify specific water quality
challenges requiring treatment. The WWTPs analyzed include Army Base
(AB), Boat Harbor (BH), James River (JR), Nansemond (NP), Virginia
Initiative Plant (VIP), and York River (YR) (see Figure 11.5).Data
Sources for Evaluation The following three primary data sources were
used in the evaluation:

• 2013/2014 water quality data---Detailed water quality data were
provided for each WWTP effluent for October 2013 through September 2014.
The data were provided as raw data in tables. With few exceptions, these
data included the following parameters at the stated frequency: chloride
(1x/ week), calcium (2x/week), magnesium (2x/week), potassium (2x/week),
sodium (2x/week), total alkalinity (3x/week), 5-day biochemical oxygen
demand (BOD5; 5x/week), pH (1x/day), turbidity (1x/day),
ammonia-nitrogen (NH3-N; 2x/week), NOx-N (4x/week), orthophosphate
(3x/week), total Kjeldahl nitrogen (TKN; 4x/week), total phosphorus (TP;
5x/week), and total suspended solids (TSS; 5x/week). • 2011, 2012, and
2013 water quality data---Detailed influent and effluent water quality
data were provided for each WWTP from the 2011, 2012, and 2013 HRSD
wastewater characteristics studies (no James River data were provided
for 2013). The data provided were presented as minimum, average, and
maximum values. The data included flow (continuous), temperature
(3x/day), pH (12x/day), total alkalinity (1x/week), BOD5 (5x/week), TSS
(5x/week), turbidity (5x/week), fecal coliform (5x/week), TKN (frequency
not reported), NOx-N (frequency not reported), and TP (frequency not
reported). Data were also provided for influent chloride (1x/week) and
influent sulfate (4x/week), selected heavy metals (1x/year), and a
variety of organics (volatile, base/acid, pesticides, total
trihalomethanes {[}TTHMs{]}; 1x/year). • 2014 total dissolved solids
data---Effluent TDS data were provided for each WWTP for January through
September 2014. The data were provided as raw data in tables, and data
points were provided once per week.

Effluent COD and TOC data collected by HRSD on a weekly basis from
February 2015 through April 2015 were also used in the evaluation.

Data Evaluation Evaluation of effluent quality from each of the seven
treatment plants involved in the HRSD's SWIFT project included
identification of the strength of each data source and was qualitatively
documented as excellent, good, or limited. Excellent data included
detailed 2013/2014 raw data and minimum, average, and maximum annual
data from 2011 through 2013. Good data included a full dataset from only
one of the sources. Limited data included data that were only collected
once per year. Table 11.3 shows the average WWTP effluent water quality
for select parameters and the strength of each data source. In addition,
the data in Table 11.3 were analyzed to identify any problematic
parameters that would require a specific treatment process or additional
treatment to meet probable regulatory requirements or aesthetic issues.
Specific challenges revealed in Table 11.3 include the following:

• Total dissolved solids---The drinking water secondary MCL for TDS is
500 mg/L. The average effluent TDS from each WWTP except James River
exceeds 500 mg/L; the Army Base plant (1292 mg/L) and Virginia
Initiative Plant (853 mg/L) have notably high TDS concentrations. •
Ammonia---Army Base and Boat Harbor plants have an average effluent NH3
of 25.2 and 16.0 mg-N/L, respectively, although the Army Base plant was
recently upgraded to biological nutrient removal (BNR) and now produces
effluent with low ammonia and total nitrogen (TN) concentrations that
are comparable to HRSD's other biological nutrient removal (BNR) plants.
Total nitrogen concentrations in excess of 10 mg/L are typically not
allowed for groundwater recharge into potable aquifers; therefore,
additional nitrogen treatment would likely be required at the Boat
Harbor plant. • Total trihalomethanes---Elevated TTHM levels were
recorded at Nansemond (82.4 µg/L) and Williamsburg (64.7 µg/L). TTHM
levels at the other plants ranged from 3 µg/L to 50 µg/L. More TTHM data
should be collected as the data sources used were limited. The drinking
water primary MCL for TTHMs is 80 µg/L. • Total hardness---Hardness in
water is caused by the presence of certain positively charged metallic
ions in solution in the water. The most common of these hardness-causing
ions are calcium and magnesium; others include iron, strontium, and
barium. The two primary constituents of water that determine the
hardness of water are calcium and magnesium. If the concentration of
these elements in the water is known, the total hardness of the water
can be calculated. To make this calculation, the equivalent weights of
calcium, magnesium, and calcium carbonate must be known; the equivalent
weights are given below: Equivalent weight Calcium (Ca) 20.04 Magnesium
(Mg) 12.15 Calcium carbonate (CaCO3) 50.045 Calcium and magnesium ions
are the two constituents that are the primary cause of hardness in
water. To find total hardness (mg/L as CaCO3), we simply add the
concentrations of calcium and magnesium ions (mg/L as CaCO3), using
Equation 11.1: Total hardness = Calcium hardness + Magnesium hardness
(11.1) When total hardness has been calculated, it is sometimes used to
determine another expression of hardness---carbonate and noncarbonate.
When hardness is numerically greater than the sum of bicarbonate and
carbonate alkalinity, that amount of hardness equivalent to the total
alkalinity (both in units of mg/L as CaCO3) is called the carbonate
hardness; the amount of hardness in excess of this is the noncarbonate
hardness. When the hardness is numerically equal to or less than the sum
of carbonate and noncarbonate alkalinity, all hardness is carbonate
hardness and noncarbonate hardness is absent. Again, the total hardness
is comprised of carbonate hardness and noncarbonate hardness: Total
hardness = Carbonate hardness + Noncarbonate hardness (11.2)

During the evaluation, total hardness data were not specifically
provided but were calculated using the detailed calcium and magnesium
data. Total hardness in drinking water systems is often limited to 150
mg/L as CaCO3 or less to avoid customer complaints. The Boat Harbor
plant (161 mg/L), Virginia Initiative Plant (181 mg/L), and York River
Treatment Plant (194 mg/L) all showed average effluent data above this
value. Total hardness concentrations in potable water in the surrounding
area were less than the secondary MCL of 500 mg/L. • Dissolved organic
carbon (DOC) and soluble chemical oxygen demand (sCOD)---These are
important parameters to measure for potable reuse plants because
advanced treatment goals and regulatory requirements are often developed
for these constituents. The average DOC and sCOD concentrations in the
effluent from the seven WWTPs ranged from 8.6 to 11.2 mg/L and 25 to 49
mg/L, respectively, values that are within the typical range for WWTPs
practicing biological nutrient removal. TOC concentrations above 10 mg/L
become increasingly difficult to treat to recommended levels for certain
advanced treatment trains so additional DOC sampling and bench and pilot
testing are recommended to confirm adequate treatment performance. is
ultimately pursued and implemented. In addition to the average WWTP
effluent data shown in Table 11.3, selected 99th-percentile effluent
data from the 2013/2014 dataset were also analyzed to determine peak
loadings from the WWTP that could be problematic for various treatment
processes. Peak loadings can be accounted for either by selecting a
treatment process that is designed for the maximum values or by
providing a large enough equalization volume of primary or secondary
effluent to attenuate the loading. Table 11.4 shows the selected
parameters of concern based on the 99th-percentile data: •
Nitrate/nitrite-nitrogen---Average effluent NOx-N levels were well under
the nitrate MCL of 10 mg-N/L; however, 99th-percentile data at the
Virginia Initiative Plant (10.5 mg/L) and at Williamsburg (0.3 mg/L)
show that NOx-N levels could periodically exceed the nitrate MCL, which
could require NOx-N-specific treatment or additional storage. The
99th-percentile NOx-N concentration at Boat Harbor was also high, but
biological nutrient removal (BNR) is not currently practiced at this
plant. When BNR is implemented at Boat Harbor, the variability of the
NOx-N data should be reevaluated. • Biochemical oxygen demand and total
suspended solids---High 99thpercentile 5-day biochemical oxygen demand
(BOD5) and TSS levels suggest occasional plant upsets that could be
problematic for filtration (granular or membrane). This could require
increased storage or treatment or automated monitoring to divert flow
away from the AWT plant during high BOD and TSS loadings. • Total
dissolved solids---WWTP effluent TDS values are not expected to
fluctuate significantly; yet, Army Base, Virginia Initiative Plant, and
Boat Harbor each show 99th-percentile values significantly higher than
the average. Periodically high TDS values could violate treatment goals
if reverse osmosis is not selected and would require additional storage
or provision for divisions. ADVANCED TREATMENT PRODUCT WATER QUALITY
inorganic Water Quality Using the historical water quality data
presented previously and the expected performance of each unit process
based on professional judgment, mass balance calculations for key
inorganic parameters were performed for each treatment train at seven of
HRSD's WWTPs. Summary tables for each treatment train are provided in
Tables 11.5, 11.6, and 11.7. The detailed mass balance calculations
revealed the following:

• The RO-based treatment process provided the lowest concentration of
all water quality parameters (Table 11.5); however, treatment to this
level may not be necessary in all cases. For example, the finished water
TDS concentration was about 50 mg/L, well below the secondary MCL (500
mg/L) and the minimum background TDS in the Potomac Aquifer
(\textasciitilde750 mg/L). The very low TDS reverse osmosis permeate may
increase mobilization of trace metals in the aquifer, which is
undesirable. • The NF-based treatment process provided excellent water
quality, as shown in Table 11.6. The NF process removes very little
nitrogen; therefore, the TN concentration in the finished water exceeded
the recommended upper range (10 mg/L) at Boat Harbor and approached the
10-mg/L limit at two other WWTPs (VIP and Williamsburg). Nitrification
and denitrification improvements at these WWTPs may be necessary to
ensure regular compliance with the recommended TN limit. Alternatively,
NF membranes that have higher nitrogen removal can be considered;
however, their use will result in a higher TDS concentrate stream.

• The GAC-based treatment process provided excellent water quality, as
shown in Table 11.7. Specific considerations related to this process
include the following: • Although some incidental nitrification may
occur in the biological activated carbon filters, the process is not
intended nor typically designed to remove nitrogen. Therefore, nitrogen
removal should be considered at the upstream WWTPs, where nitrification
and denitrification improvements would be necessary to ensure regular
compliance with the recommended TN limit. • No TDS are removed through
this process. The Army Base and VIP plants showed elevated TDS levels
that regularly exceeded 750 mg/L. Upstream mitigation, such as reducing
infiltration and inflow in areas with high TDS or eliminated industrial
discharge high in TDS, may be required at these locations if a TDS limit
of 750 mg/L is established. • Hardness removal with chemical
precipitation may be required at three plants (Boat Harbor, VIP, and
York River), although more investigation is necessary to determine
whether or not the total hardness (161 to 194 mg/L as CaCO3) at these
plants is acceptable from aesthetic and aquifer geochemistry
perspectives. If not, the proposed flocculation--sedimentation process
shown for the GAC-based treatment train could be modified to a chemical
softening process for those plants with elevated hardness.

organic Water Quality Bulk Organics The application of robust treatment
barriers for the removal of organics has historically been a central
tenet of implementing full-scale potable reuse projects to address the
presence of unknown organic compounds of chronic health concern that may
be present in the secondary effluent---a significant and pressing
example of the old ``we do not know what we do not know'' syndrome. As
presented in Table 11.8, regulations and permits for potable reuse
projects have been developed by establishing limits on bulk organic
parameters, such as COD and TOC, which act as surrogates for organic
compounds of wastewater origin. Virginia established a COD limit of 10
mg/L for the Occoquan and Dulles area watershed policies, which apply to
the Upper Occoquan Sewage Authority (UOSA) IPR project (constructed in
1978) and the Broad Run Water Reclamation Facility (BRWRF) (constructed
in 2008), respectively. California and Florida have established TOC
limits of 0.5 mg/L and 3 mg/L, respectively, in their IPR regulations.
An advanced water treatment plant's finished water COD and TOC
concentration that would need to comply with the established permit
limit is dependent on the initial concentration in the WWTP effluent and
the specific treatment processes employed at the advanced water
treatment plant. Table 11.9 shows the estimated finished water TOC
concentrations from the three proposed advanced water treatment plant
treatment trains (i.e., RO, NA, and BAC) when treating effluent from
each of HRSD's WWTPs. The calculations use full-scale advanced water
treatment plant effluent TOC and DOC sampling and treatment process
pilot testing. The following can be concluded from the information in
the table: • Compliance with a California-based TOC limit of 0.5 mg/L
could only be achieved by implementing an RO-based treatment train. •
Compliance with a Florida-based UOSA-type permit (3 mg/L TOC and 10 mg/L
COD) could likely be achieved at most WWTPs by any of the three proposed
treatment trains or by a hybrid treatment train that combined partial RO
treatment with GAC-based treatment. • GAC-based advanced water treatment
plants will require regular replacement or regeneration of the GAC media
for consistent TOC removal. Pilot testing is necessary to determine the
GAC regeneration frequency requirements. • Measurement on a regular
basis of TOC and DOC in the final effluent from each WWTP is recommended
to accurately determine TOC removal requirements.

Trace Organics Earlier, contaminants of emerging concern (CECs) were
mentioned. Additional concerns about CECs continue to be expressed with
regard to the potential for chronic human health effects related to the
thousands of organic chemicals that may end up in wastewater effluent at
trace levels (mg/L). Furthermore, the efficacy of conventional water
treatment processes that may end up treating source waters that have
some effluent contribution is typically low. Each advanced treatment
process considered in this discussion differs in its effectiveness at
removing CECs. Research has shown that RO-based, NF-based, and GAC-based
potable reuse treatment trains provide multiple unit processes that are
effective barriers to a wide range of CECs. The ROand NF-based treatment
trains provide substantial removal through membranes (RO/NF) and
ultraviolet advanced oxidation process (UVAOP), while the GAC-based
treatment train provides significant removal through ozone--biological
activated carbon (BAC) and granular activated carbon (GAC). Table 11.10
shows representative removals by advanced treatment processes for a
variety of CECs as determined through recent research and monitoring of
full-scale treatment facilities. These processes are redundant in the
removal of some CECs (i.e., provide multiple barriers to their passage)
and are complementary in the removal of others. For example, both ozone
and GAC are effective barriers to the anticonvulsant drug carbamazepine,
but only GAC acts as an effective barrier to the flame-retardant TCEP.
No one process provides complete removal of all compounds, but RO
generally provides the best removal of a wide range of compounds.
However, these compounds are not destroyed or transformed by RO but
instead are transferred to the RO concentrate (at a higher
concentration); thus, their presence in the concentrate must be
considered, particularly when the concentrate is discharged to a
receiving water body. At the present time, treatment for all CECs does
not appear to be a differentiator among potable reuse treatment trains.
Although health effects of many CECs--- either alone or as
mixtures---have not been demonstrated at the ng/L concentrations
typically detected in wastewater effluent, the proposed treatment trains
do reduce the concentrations of many of these chemicals to a significant
degree. Meanwhile, the USEPA is prioritizing and studying a number of
chemicals through their candidate contaminant list program.

RO CONCENTRATE DISPOSAL* To this point in the book we have discussed the
benefits of reverse osmosis operating systems. It is important to point
out, however, that along with the good there is the not so good; that
is, RO systems have their advantages but they also have a few
disadvantages. The one disadvantage pointed out and discussed here is
the major one---that is, concentrate disposal. Where is the concentrated
wastestream to be disposed of?

mass balance principle To gain a better understanding of membrane
disposal issues and techniques we begin with a discussion of mass
balance. The simplest way to express the fundamental engineering
principle of mass balance is to say, ``Everything has to go somewhere.''
More precisely, the law of conservation of mass says that when chemical
reactions take place matter is neither created nor destroyed. What this
important concept allows us to do is track materials
(concentrates)---that is, pollutants, microorganisms, chemicals, and
other materials---from one place to another. The concept of mass balance
plays an important role in reverse osmosis system operations (especially
in desalinization) where we assume a balance exists between the material
entering and leaving the RO system: ``What comes in must equal what goes
out.'' The concept is very helpful for evaluating biological systems and
developing sampling and testing procedures, as well as many other unit
processes within any treatment or processing system. All desalinization
processes have two outgoing process streams: (1) the product water,
which is lower in salt than the feed water, and (2) a concentrated
stream that contains the salts removed from the product water. Even
distillation has a ``bottoms'' solution that contains salt from the
vaporized water. The nature of the concentrate stream depends on the
salinity of the feed water, the amount of product water recovered, and
the purity of the product water. To determine the volume and
concentration of the two outgoing streams, a mass balance is
constructed. It is necessary to know the recovery rate of water, the
rejection rate of salt, and the input flow and concentration to solve
equations for the flow and concentration of the product and concentrate.

reVerse osmosis concentrate Disposal practices Reverse osmosis
concentrate is disposed of by several methods, including surface water
discharge, sewer discharge, deep well injection, evaporation ponds,
spray irrigation, and zero liquid discharge.

Surface Water and Sewer Disposal Disposal of concentrate to surface
water and sewers are the two most widely used disposal options for both
desalting membrane processes. Post-1992 data provide the following
statistics:

Disposal OptionPercent of Desalting PlantsSurface water
disposal45\%Disposal to sewer42\%Total87\% This disposal option,
although not always available, is the simplest option in terms of
equipment involved and is frequently the lowest cost option. As will be
seen, however, the design of an outfall structure for surface water
disposal can be complex. Disposal to surface water involves conveyance
of the concentrate or backwash to the site of disposal and an outfall
structure that typically involves a diffuser and outlet ports or valve
mounted on the diffuser pipe. Factors involved in the outfall design are
discussed in this section, and cost factors are presented. However, due
to the large number of cost factors and the large variability in design
conditions associated with surface water disposal, a relatively simple
cost model cannot be developed. Disposal to surface waters requires a
National Pollutant Discharge Elimination System (NPDES) permit. Disposal
to the sewer involves conveyance to the sewer site and typically a
negotiated fee to be paid to the WWTP. Because the negotiated fees can
range from zero to substantial, there is no model that can be presented.
No disposal permits are required for this disposal option. Disposal of
concentrate or backwash to the sewer, however, 260 The Drinking Water
Handbook, Third Edition

affects WWTP effluent that requires an NPDES permit. With regard to
design considerations for disposal to surface water, a brief discussion
of ambient conditions, discharge conditions, regulations, and the
outfall structure is provided below. Because receiving waters can
include rivers, lakes, estuaries, canals, oceans, and other bodies of
water, the range of ambient conditions can vary greatly. Ambient
conditions include the geometry of the receiving water bottom and the
receiving water salinity, density, and velocity. Receiving water
salinity, density, and velocity may vary with water depth, distance from
the discharge point, and time of day and year. Discharge conditions
include the discharge geometry and the discharge flow conditions. The
discharge geometry can vary from the end of the pipe to a lengthy
multiple-port diffuser. The discharge can be at the water surface or
submerged. The submerged outfall can be buried (except for ports) or
not. Much of the historical outfall design work deals with discharges
from WWTPs. These discharges can be very large---up to several hundred
million gallons per day in flow. In ocean outfalls and in many inland
outfalls, these discharges are of lower salinity than the receiving
water, and the discharge has positive buoyancy. The less dense effluent
rises in the more dense receiving water after it is discharged. The
volume of flow of membrane concentrates is on the lower side of the
range of WWTP effluent volumes, extending up to perhaps 15 MGD. Membrane
concentrate, as opposed to WWTP effluent, tends to be of higher salinity
than most receiving waters, resulting in a condition of negative
buoyancy where the effluent sinks after it is discharged. This raises
concerns about the potential impact of the concentrate on the benthic
community at the receiving water bottom. Any possible effect on the
benthic community is a function of the local ecosystem, the composition
of the discharge, and the degree of dilution present at the point of
contact. The chance of an adverse impact is reduced by increasing the
amount of dilution at the point of bottom contact through diffuser
design. With regard to concentrate discharge regulations it is important
to note that receiving waters can differ substantially in their volume,
flow, depth, temperature, composition, and degree of variability in
these parameters. The effect of discharge of a concentrate or backwash
to a receiving water can vary widely depending on these factors. The
regulation of effluent disposal to receiving waters involves several
considerations, including the end-of-pipe characteristics of the
concentrate or backwash. Comparison is made between receiving water
quality standards (dependent on the classification of the receiving
water) and the water quality of the effluent to determine disposal
feasibility. In addition, in states such as Florida, the effluent must
also pass tests where test species, chosen based on the receiving water
characteristics, are exposed to various dilutions of the effluent.
Because the nature of the concentrate or backwash is different than that
of the receiving water, there is a region near the discharge area where
mixing and subsequent dilution of the concentrate or backwash occurs.
Where conditions cannot be met at the end of the discharge pipe, a
mixing zone may be granted by the regulatory agency. The mixing zone is
an administrative construct that defines a limited area or volume of the
receiving water where this initial dilution of the discharge is allowed
to occur. The definition of an allowable mixing zone is based on
receiving water modeling. The regulations require that certain
conditions be met at the edge of the mixing zone in terms of
concentration and toxicity.\\
When the mixing zone conditions have been met, the outfall structure can
be properly designed and installed. Actually, the purpose of the outfall
structure is to ensure that mixing conditions can be met and that
discharge of the effluent, in general, will not produce any damaging
effect on the receiving water, its lifeforms, wildlife, and the
surrounding area. In a highly turbulent and moving receiving water with
large volume relative to the effluent discharge, simple discharge from
the end of a pipe may be sufficient to ensure rapid dilution and mixing
of the effluent. For most situations, however, the mixing can be
improved substantially through the use of a carefully designed outfall
structure. Such a design may be necessary to meet regulatory
constraints. The most typical outfall structure for this purpose
consists of a pipe of limited length mounted perpendicular to the end of
the delivery pipe. This pipe, called a diffuser, has one or more
discharge ports along its length.

Disposal to the Sewer Where possible, this means of disposal is simple
and usually cost effective. Disposal to a sewer does not require a
permit but does require permission from the wastewater treatment plant.
The impact of both the flow volume and composition of the concentrate
will be considered by the WWTP, as it will affect their capacity buffer
and their NPDES permit. The high volume of some concentrates prohibits
their discharge to the local WWTP. In other cases, concerns are focused
on the increased TDS level of the WWTP effluent that results from the
concentrate discharge. The possibility of disposal to a sewer is highly
site dependent. In addition to the factors mentioned, the possibility is
influenced by the distance between the two facilities, by whether the
two facilities are owned by the same entry, and by future capacity
increases anticipated. Where disposal to a sewer is allowed, the WWTP
may be required to pay fees based on volume or composition.

pathogen remoVal Various states have developed different approaches to
regulating pathogen removal by indirect potable reuse plants. For
example, Virginia permitted the UOSA indirect potable reuse plant and
the Broad Run Water Reclamation Facility based on achieving a nondetect
concentration of Escherichia coli (less than 2 cfu/100 mL). Other states
have taken a different approach. For example, California requires 12-log
reduction of viruses and 10-log reduction of Cryptosporidium and Giardia
from the raw wastewater to the advanced water treatment plant finished
water. Pathogen removal by each of the three proposed treatment trains
is significant and would result in nondetect concentrations for all
indicator organisms typically used in wastewater treatment (e.g., E.
coli, total coliform, fecal coliform) assuming proper operation.
Therefore, compliance with a UOSA-type permit or Florida's
pathogenrelated indirect potable reuse regulations would be met by all
three proposed treatment trains. Compliance with the California
regulations may be more challenging, especially for GAC-based treatment,
because of the high log reduction requirements. Tables 11.11 and 11.12
show the calculated log reduction credits for each of the proposed
treatment trains. Note that log reduction credits associated with 6
months of subsurface travel time as water moves through the aquifer
(soil aquifer treatment) have been assumed in the calculations.
Discussion with the Virginia regulators is necessary to determine how
the proposed HRSD groundwater recharge project would be regulated with
respect to pathogen removal.

Disinfection byproDucts Excessive formation of trihalomethanes (THMs) at
WWTPs is fairly common, especially for plants that provide good
nitrification. Low effluent NH3 concentrations at these plants lead to
the formation of free chlorine (rather than chloramines) during chlorine
disinfection, which, when reacted with bulk organics, has a propensity
to form high levels of THMs. N-nitrosodimethylamine (NDMA), another
disinfection byproduct, can also form in significant concentrations
during the disinfection process at WWTPs depending on the precursors in
the water and the type of chlorination practiced. Little NDMA forms in
the presence of free chlorine, but significant concentrations typically
form in the presence of chloramines, with dichloramine resulting in more
rapid formation kinetics than monochloramine. Both THMs and NDMA can be
removed by advanced water treatment processes through specialized
design, but a more cost-efficient approach is to prevent their formation
by withdrawing the water from the WWTP prior to disinfection (upstream
of the chlorine contact basin). Specific withdrawal points at each WWTP
and the potential treatment required for THMs and NDMA removal should be
considered in the next stage of this project.

ANTICIPATED IMPROVEMENTS TO HRSD'S EXISTING WWTPS Some operational and
capital improvements to the existing WWTPs may be required depending on
the AWT train selected for implementation and the final effluent water
quality produced at each WWTP. Table 11.13 shows the improvements that
will likely be required. Analysis regarding WWTP improvements should be
ongoing.

THE BOTTOM LINE All of these processes---preliminary screening,
filtration, disinfection, and the advanced processes necessary for
specialized water problems---are in place to serve one primary,
essential purpose: to supply the consumer with safe potable water. The
consumer may put this water to a wide variety of uses, from drinking it
to using it to water the lawn, but those uses are in many ways beside
the point, as safe potable water is essential for human life. From the
information presented to this point, it is obvious that the bottom line
is that any of the three advanced water treatment (AWT)
trains---RO-based, NF-based, or GAC-based---is likely to be viable for
groundwater recharge by effluent generated from HRSD's WWTPs. Finished
water quality produced by each train will be excellent with respect to
pathogen and organics removal, but use of the RO-based or NF-based
treatment train is necessary if TDS reduction is required. Partial RO or
NF treatment could be used depending on the degree of TDS reduction
required. BNR improvements will be required at some of the WWTPs to
reduce the TN concentration and the propensity for organic fouling
membranes in the ROand NF-based trains.

Selection of the advanced water treatment train to be implemented at
each WWTP should be based on numerous factors, such as finished water
quality, wastewater discharge requirements, operability, sustainability,
site-specific characteristics (e.g., space, existing infrastructure,
hydraulics), and capital and operating costs. Ultimate selection of the
advanced water treatment train will also be dictated by regulatory
requirements related to treatment, finished water quality, and
wastewater discharge requirements that have not yet been established;
therefore, engaging the appropriate regulatory agencies is important
during the next phase of this project. Treatment selection may also be
influenced by public perception. Because HRSD's SWIFT project is a work
in progress, with time for adjustment here, there, and almost anywhere,
other action items that will influence advanced water treatment train
selection should be considered, including the following:

• Regularly sample at each WWTP for COD, sCOD, TOC, DOC, all
contaminants regulated by primary MCLs, selected CECs, and parameters
specific to the design of RO and NF treatment (e.g., barium, strontium,
fluoride, silica, alkalinity, pH). • Regularly measure water quality
(e.g., pH, alkalinity, TDS, hardness) in numerous Potomac Aquifer
product wells. • Evaluate site-specific conditions at each WWTP that may
influence AWT train selection, including site space, hydraulics,
geotechnical conditions, electrical service, and use of existing
infrastructure for AWT. • Conduct an industrial and commercial water
quality discharge study to characterize risk and to identify chemicals
of concern that may be discharged to the collection system. • Determine
potential causes for high TDS concentrations at WWTPs where effluent TDS
is greater than 500 mg/L.

Upgrading Security Worldwide conflicts are ongoing and seem never
ending. One of the most important conflicts of our time, including the
ongoing Israeli--Palestinian conflict, is in fact conflict over scarce
but vital water resources. This conflict over water, unfortunately, may
be a harbinger of things to come.

INTRODUCTION* Each of the public water systems (PWSs) in the United
States regularly supplies drinking water to at least 25 persons or 15
service connections. Of the total U.S. population, 90\% is served by
PWSs, while the remainder is served primarily by private wells. PWSs are
divided into community water systems (CWSs) and noncommunity water
systems (NCWSs). Examples of CWSs include municipal water systems that
serve mobile home parks or residential developments, and examples of
NCWSs include schools, factories, churches, commercial campgrounds,
hotels, and restaurants (USEPA, 2017). Because drinking water is
consumed directly, health effects associated with contamination have
long been major concerns. In addition, interruption or cessation of the
drinking water supply can disrupt society, impacting human health and
critical activities such as fire protection. Although they may have no
clue as to its true economic value and to its future worth, the general
public correctly perceives drinking water as being central to the life
of an individual and of society. Federal and state agencies have long
been active in addressing these risks and threats to water utilities
through regulations, technical assistance, research, and outreach
programs. As a result, an extensive system of regulations governing
maximum contaminant levels of 90 conventional contaminants (most
established by the U.S. Environmental Protection Agency), construction
and operating standards (implemented mostly by the states), monitoring,
emergency response planning, training, research, and education have been
developed to better protect the nation's drinking water supply and
receiving waters. Since the events of 9/11, the U.S. Environmental
Protection Agency (USEPA) has been designated as the sector-specific
agency responsible for infrastructure protection activities for the
nation's drinking water system. The USEPA is utilizing its position
within the water sector and working with its stakeholders to provide
information to help protect the nation's drinking water supply from
terrorism or other intentional acts.

CONSEQUENCES OF 9/11 One consequence of the events of September 11 was
the USEPA's directive to establish a Water Protection Task Force to
ensure that activities to protect and secure the country's water supply
and wastewater treatment infrastructure are comprehensive and carried
out expeditiously. Another consequence is a heightened concern among
citizens in the United States over the security of their critical water
infrastructure. The nation's water infrastructure, consisting of several
thousand publicly owned water treatment works, more than 100,000 pumping
stations, and hundreds of thousands of miles of water distribution
lines, is one of America's most valuable resources. This country's water
treatment and distribution/collection systems are valued at more than
\$2.5 trillion. Almost immediately after 9/11, then-Governor Tom Ridge
(Henry, 2002) pointed out the security role for public professionals:

Americans should find comfort in knowing that millions of their fellow
citizens are working every day to ensure our security at every
level---federal, state, county, municipal. These are dedicated
professionals who are good at what they do. I've seen it up close, as
Governor of Pennsylvania \ldots{} but there may be gaps in the system.
The job of the Office of Homeland Security will be to identify those
gaps and work to close them.

Eliminating these gaps in the system has driven many water and
wastewater facilities to increase their security. Moreover, the USEPA
made several recommendations to increase security and reduce threats
from terrorism (USEPA, 2001). The recommendations include the following:

\begin{enumerate}
\item
  Guard against unplanned physical intrusion (water/wastewater).
\end{enumerate}

\begin{enumerate}
\item
  Lock all doors and set alarms at your office, pumping stations,
  treatment plants, and vaults, and make it a rule that doors are locked
  and alarms are set.
\item
  Limit access to facilities and control access to pumping stations and
  chemical and fuel storage areas, giving close scrutiny to visitors and
  contractors.
\item
  Post guards at treatment plants, and post ``Employee Only'' signs in
  all restricted areas.
\item
  Control access to storm sewers.
\item
  Secure hatches, metering vaults, manholes, and other access points to
  the sanitary collection system.
\item
  Increase lighting in parking lots, treatment bays, and other areas
  with limited staffing.
\item
  Control access to computer networks and control systems, and change
  the passwords frequently.
\item
  Do not leave keys in equipment or vehicles at any time.
\end{enumerate}

\begin{enumerate}
\item
  Make security a priority for employees.
\end{enumerate}

\begin{enumerate}
\item
  Conduct background security checks on employees at hiring and
  periodically thereafter.
\item
  Develop a security program with written plans and train employees
  frequently.
\item
  Be sure that all employees are aware of communications protocols with
  relevant law enforcement, public health, environmental protection, and
  emergency response organizations.
\item
  Be sure that employees are fully aware of the importance of vigilance
  and the seriousness of breaches in security; make note of
  unaccompanied strangers on the site, and immediately notify designated
  security officers or local law enforcement agencies.
\item
  Consider varying the timing of operational procedures, if possible, so
  someone watching will realize that the pattern changes.
\item
  Upon the dismissal of an employee, change passcodes and make sure keys
  and access cards are returned.
\item
  Provide customer service staff with training and checklists of how to
  handle a threat if it is called in.
\end{enumerate}

\begin{enumerate}
\item
  Coordinate actions for an effective emergency response.
\end{enumerate}

\begin{enumerate}
\item
  Review existing emergency response plans, and ensure they are current
  and relevant.
\item
  Make sure employees have necessary training in emergency operating
  procedures.
\item
  Develop clear protocols and chains of command for reporting and
  responding to threats with relevant emergency, law enforcement,
  environmental, public health officials, consumers, and the media.
  Practice the emergency protocols regularly.
\item
  Be sure that key utility personnel (both on and off duty) have access
  to crucial telephone numbers and contact information at all times.
  Keep the call list up to date.
\item
  Develop close relationships with local law enforcement agencies, and
  make sure they know where critical assets are located. Request they
  add your facilities to their routine rounds.
\item
  Work with local industries to ensure that their pretreatment
  facilities are secure.
\end{enumerate}

\begin{enumerate}
\item
  Invest in security and infrastructure improvements.
\end{enumerate}

\begin{enumerate}
\item
  Assess the vulnerability of distribution systems, major pumping
  stations, water treatment plants, chemical and fuel storage areas,
  outfall pipes, and other key infrastructure elements.
\item
  Assess the vulnerability of the stormwater collection system.
  Determine where large pipes run near or beneath government buildings,
  banks, commercial districts, or industrial facilities or are
  contiguous with major communication and transportation networks.
\item
  Move as quickly as possible to make the most obvious and
  cost-effective physical improvements, such as perimeter fences,
  security lighting, tamperproof manhole covers and valve boxes, etc.
\item
  Improve computer system and remote operational security.
\item
  Use local citizen watches.
\item
  Seek financing to implement more expensive and comprehensive system
  improvements
\end{enumerate}

Ideally, in a perfect world, water infrastructure would be secured in a
layered fashion (i.e., the multiple barrier approach). Layered security
systems are vital. Utilizing the protection-in-depth approach, which
requires that adversaries defeat several protective barriers or security
layers to accomplish their goal, water infrastructure can be made more
secure. Protection in depth is a term commonly used by the military to
describe security measures that reinforce one another and mask the
defense mechanisms from view of intruders, thus allowing the defender
time to respond to intrusion or attack. A prime example of the use of
the multiple-barrier approach to ensure security and safety is
demonstrated by the practices of the bottled water industry. In the
aftermath of 9/11 and the increased emphasis on homeland security, a
shifted paradigm of national security and vulnerability awareness has
emerged. Recall that in the immediate aftermath of the 9/11 tragedies,
emergency responders and others responded quickly and worked to
exhaustion. In addition to the emergency responders, bottled water
companies responded immediately by donating several million bottles of
water to the crews at the crash sites in New York, at the Pentagon, and
in Pennsylvania. The International Bottled Water Association reported
that ``within hours of the first attack, bottled water was delivered
where it mattered most: to emergency personnel on the scene who required
ample water to stay hydrated as they worked to rescue victims and clean
up debris'' (IBWA, 2004). Bottled water companies continued to provide
bottled water to responders and rescuers at the 9/11 sites for the
duration. These patriotic actions by the bottled water companies,
however, beg the question: How do we ensure the safety and security of
the bottled water provided to anyone? IBWA's answer is to use a
multiple-barrier approach, along with other defense principles, to
enhance the safety and security of bottled water. IBWA (2004) described
its multiple-barrier approach as follows:

A multiple-barrier approach---Bottled water products are produced
utilizing a multiple-barrier approach, from source to finished product,
that helps prevent possible harmful contaminants (physical, chemical or
microbiological) from adulterating the finished product as well as
storage, production, and transportation equipment. Measures in a
multiple-barrier approach may include source protection, source
monitoring, reverse osmosis, distillation, filtration, ozonation or
ultraviolet (UV) light. Many of the steps in a multibarrier system may
be effective in safeguarding bottled water from microbiological and
other contamination. Piping in and out of plants, as well as storage
silos and water tankers are also protected and maintained through
sanitation procedures. In addition, bottled water products are bottled
in a controlled, sanitary environment to prevent contamination during
the filling operation. In water infrastructure security, protection in
depth is used to describe a layered security approach. A
protection-in-depth strategy uses several forms of security techniques
and devices against an intruder and does not rely on a single defensive
mechanism to protect infrastructure. By implementing multiple layers of
security, a hole or flaw in one layer is covered by the other layers,
and an intruder will have to break through each layer without being
detected. This layered approach implies that no matter how they attempt
to accomplish their goal, intruders will encounter effective elements of
the physical protection system. In the following sections, various
security hardware and devices are described. These devices serve the
main purpose of providing security against physical or digital
intrusion; that is, they are designed to delay and deny intrusion and
are normally coupled with detection and assessment technology. Keep in
mind, however, that when it comes to trying to make something absolutely
secure from intrusion or attack there is no absolute silver bullet.

SECURITY HARDWARE AND DEVICES Water infrastructure security devices or
products can be grouped into four general categories (USEPA, 2005):

• Physical asset monitoring and control devices • Water monitoring
devices • Communication/integration • Cyber protection devices

physical asset monitoring anD control DeVices Aboveground Outdoor
Equipment Enclosures Water and wastewater systems consist of multiple
components spread over a wide area and typically include a centralized
treatment plant, as well as distribution or collection system components
that are usually distributed at multiple locations throughout the
community. In recent years, however, distribution and collection system
designers have favored placing critical equipment---especially assets
that require regular use and maintenance---above ground. A primary
reason for doing so is that locating this equipment above ground
eliminates the safety risks associated with confined space entry, which
is often required for the maintenance of equipment located below ground.
In addition, space restrictions often limit the amount of equipment that
can be located inside, and there are concerns that some types of
equipment (such as backflow-prevention devices) can, under certain
circumstances, discharge water that could flood pits, vaults, or
equipment rooms; therefore, many pieces of critical equipment are
located outdoors and above ground. Examples of the many different system
components that can be installed outdoors and above ground include

• Backflow-prevention devices • Air release and control valves •
Pressure vacuum breakers • Pumps and motors • Chemical storage and feed
equipment • Meters • Sampling equipment • Instrumentation

Much of this equipment is installed in remote locations or in areas
where the public can access it. One of the most effective security
measures for protecting aboveground equipment is to place it inside a
building. Where this is not possible, enclosing the equipment or parts
of the equipment using some sort of commercial or homemade add-on
structure may help to prevent tampering with the equipment. Equipment
enclosures can generally be categorized into one of four main
configurations:

• One-piece, drop-over enclosures • Hinged or removable-top enclosures •
Sectional enclosures • Shelters with access locks

Other security features that can be implemented on aboveground, outdoor
equipment enclosures include locks, mounting brackets, tamper-resistant
doors, and exterior lighting.

Active Security Barriers (Crash Barriers) Active security barriers (or
crash barriers) are large structures that are placed in roadways at
entrance and exit points of protected facilities to control vehicle
access to these areas. These barriers are placed perpendicular to
traffic to block the roadway, and traffic can only pass the barrier if
it is moved out of the roadway. These types of barriers are typically
constructed from sturdy materials, such as concrete or steel, so
vehicles cannot penetrate them. They are also situated at such a height
off the roadway that vehicles cannot go over or under them. The key
difference between active security barriers, which include wedges, crash
beams, gates, retractable bollards, and portable barricades, and passive
security barriers, which include immovable bollards, jersey barriers,
and planters, is that active security barriers are designed so they can
be easily raised and lowered or moved out of the roadway to allow
authorized vehicles to pass them. Many of these types of barriers are
designed so they can be opened and closed automatically (i.e.,
mechanized gates, hydraulic wedge barriers), while others are easy to
open and close manually (swing crash beams, manual gates). In contrast
to active barriers, passive barriers are permanent, immovable barriers
that are typically used to protect the perimeter of a protected
facility, such as sidewalks and other areas that do not require
vehicular traffic to pass them. Several of the major types of active
security barriers such as wedge barriers, crash beams, gates, bollards,
and portable/removable barricades are described below. Wedge barriers
are plated, rectangular steel buttresses approximately 2 to 3 feet high
that can be raised and lowered from the roadway. When they are in the
open position, they are flush with the roadway, and vehicles can pass
over them; however, when they are in the closed (armed) position, they
project up from the road at a 45° angle, with the upper end pointing
toward the oncoming vehicle and the base of the barrier away from the
vehicle. Generally, wedge barriers are constructed from heavy-gauge
steel or concrete that contains an impact-dampening iron rebar core that
is resistant to breaking or cracking, thereby allowing the barrier to
withstand the impact of a vehicle attempting to crash through it. In
addition, both of these materials help to transfer the energy of the
impact over the entire volume of the barrier, thus helping to prevent
the barrier from being sheared off its base. Also, because of the angle
of the barrier, the force of any vehicle impacting the barrier is
distributed over the entire surface of the barrier and is not
concentrated at the base, which helps prevent the barrier from breaking
off at the base. Finally, any vehicles attempting to drive over the
barrier will most likely be hung up on it. Wedge barriers can be fixed
or portable. Fixed wedge barriers can be mounted on the surface of the
roadway (surface-mounted wedges) or in a shallow mount in the road's
surface, or they can be installed completely below the road surface.
Surfacemounted wedge barricades operate by rising from a flat position
on the surface of the roadway, whereas shallow-mount wedge barriers rise
from their resting position just below the road surface. In contrast,
below-surface wedge barriers operate by rising from beneath the road
surface. Both the shallow-mounted and surface-mounted barriers require
little or no excavation and thus do not interfere with buried utilities.
These types of barriers project above the road surface and block traffic
when they are raised into the armed position. When they are disarmed and
lowered, they are flush with the road, thereby allowing traffic to pass.
Portable wedge barriers are moved into place on wheels that are removed
after the barrier has been set into place. Installing rising wedge
barriers requires preparation of the road surface. Installing
surface-mounted wedges does not require that the road be excavated;
however, the road surface must be intact and strong enough to allow the
bolts anchoring the wedge to the road surface to attach properly.
Shallow-mount and below-surface wedge barricades require excavation of a
pit that is large enough to accommodate the wedge structure, as well as
any arming/disarming mechanisms. Generally, the bottom of the excavation
pit is lined with gravel to allow for drainage. A gravity drain or
selfpriming pump can be installed in areas not sheltered from rain or
surface runoff. Crash beam barriers consist of aluminum beams that can
be opened or closed across the roadway. Although crash beam designs
vary, every crash beam system consists of an aluminum beam that is
supported on each side by a solid footing or buttress, which is
typically constructed from concrete, steel, or some other strong
material. Beams typically contain an interior steel cable (at least 1
inch in diameter) to give the beam added strength and rigidity. The beam
is connected by a heavyduty hinge or other mechanism to one of the
footings so it can swing or rotate out of the roadway when it is open
and can swing back across the road when it is in the closed (armed)
position, blocking the road and inhibiting access by unauthorized
vehicles. The non-hinged end of the beam can be locked into its footing,
thus providing anchoring for the beam on both sides of the road and
increasing the resistance of the beam to vehicles attempting to
penetrate through it. In addition, if the crash beam is hit by a
vehicle, the aluminum beam transfers the impact energy to the interior
cable, which in turn transfers the impact energy through the footings
and into their foundation, thereby minimizing the chance that the impact
will snap the beam and allow the intruding vehicle to pass through.
Crash beam barriers can employ drop-arm, cantilever, or swing beam
designs. Drop-arm crash beams operate by raising and lowering the beam
vertically across the road. Cantilever crash beams are projecting
structures that are opened and closed by extending the beam from the
hinge buttress to the receiving buttress located on the opposite side of
the road. In the swing beam design, the beam is hinged to the buttress
such that it swings horizontally across the road. Generally, swing beam
and cantilever designs are used at locations where a vertical lift beam
is impractical; for example, the swing beam or cantilever designs are
utilized at entrances and exits with overhangs, trees, or buildings that
would physically block the operation of the drop-arm beam design.
Installing any of these crash beam barriers involves the excavation of a
pit approximately 48 inches deep for both the hinge and the receiver
footings. Due to the depth of excavation, the site should be inspected
for underground utilities before digging begins. In contrast to wedge
barriers and crash beams, which are typically installed separately from
a fence line, gates are often integrated units of a perimeter fence or
wall around a facility. Gates are basically movable pieces of fencing
that can be opened and closed across a road. When the gate is in the
closed (armed) position, the leaves of the gate lock into steel
buttresses that are embedded in a concrete foundation located on both
sides of the roadway, thereby blocking access to the roadway. Generally,
gate barricades are constructed from a combination of heavy-gauge steel
and aluminum that can absorb an impact from vehicles attempting to ram
through them. Any remaining impact energy not absorbed by the gate
material is transferred to the steel buttresses and their concrete
foundation. Gates can utilize a cantilever, linear, or swing design.
Cantilever gates are projecting structures that operate by extending the
gate from the hinge footing across the roadway to the receiver footing.
A linear gate is designed to slide across the road on tracks via a
rack-and-pinion drive mechanism. Swing gates are hinged so they can
swing horizontally across the road. Installation of the cantilever,
linear, or swing gate designs involves the excavation of a pit
approximately 48 inches deep for both the hinge and receiver footings to
which the gates are attached. Again, due to the depth of excavation, the
site should be inspected for underground utilities before digging
begins. Bollards are vertical barriers at least 3 feet tall and 1 to 2
feet in diameter that are typically set 4 to 5 feet apart from each
other so they block vehicles from passing between them. Bollards can be
fixed in place, removable, or retractable. Fixed and removable bollards
are passive barriers that are typically used along building perimeters
or on sidewalks to prevent vehicle access while still allowing
pedestrians to pass through them. In contrast to passive bollards,
retractable bollards are active security barriers that can easily be
raised and lowered to allow vehicles to pass between them; thus, they
can be used in driveways or on roads to control vehicular access. When
the bollards are raised, they project above the road surface and block
the roadway; when they are lowered, they sit flush with the road surface
and allow traffic to pass over them. Retractable bollards are typically
constructed from steel or other materials that have a low
weight-to-volume ratio so they require low power to raise and lower.
Steel is also more resistant to breaking than is a more brittle material
such as concrete, and it is better able to withstand direct vehicular
impact without breaking apart. Retractable bollards are installed in a
trench dug across a roadway, typically at an entrance or a gate.
Installing retractable bollards requires preparing the road surface.
Depending on the vendor, bollards can be installed either in a
continuous slab of concrete or in individual excavations with concrete
poured in place. The required excavation for a bollard is typically
slightly wider and slightly deeper than the bollard height when it is
extended above ground. The bottom of the excavation is typically lined
with gravel to allow drainage. The bollards are then connected to a
control panel that controls the raising and lowering of the bollards.
Installation typically requires mechanical, electrical, and concrete
work; if utility personnel with these skills are available, then the
utility can install the bollards themselves. Portable or removable
barricades, which can include removable crash beams and wedge barriers,
are mobile obstacles that can be moved in and out of position on a
roadway; for example, a crash beam may be completely removed and stored
off-site when it is not needed. An additional example would be wedge
barriers that are equipped with wheels that can be removed after the
barricade is towed into place. When portable barricades are needed, they
can be moved into position rapidly. To provide them with added strength
and stability, they are typically anchored to buttress boxes that are
located on either side of the road. These buttress boxes, which may or
may not be permanent, are usually filled with sand, water, cement,
gravel, or concrete to make them heavy and aid in stabilizing the
portable barrier. In addition, these buttresses can help dissipate any
impact energy from vehicles crashing into the barrier itself. Because
these barriers are not anchored into the roadway, they do not require
excavation or other related construction for installation, and they can
be assembled and made operational in a short period of time. The primary
shortcoming to this type of design is that these barriers may move if
they are hit by vehicles; therefore, it is important to carefully assess
the placement and anchoring of these types of barriers to ensure that
they can withstand the types of impacts that may be anticipated at that
location. Because the primary threat to active security barriers is that
vehicles will attempt to crash through them, their most important
attributes are their size, strength, and crash resistance. Other
important features for an active security barrier are the mechanisms by
which the barrier is raised and lowered to allow authorized vehicle
entry, as well as such other factors as weather resistance and safety
features.

Alarm Systems An alarm system is a type of electronic monitoring system
that is used to detect and respond to specific types of events, such as
unauthorized access to an asset, or a possible fire. In water and
wastewater systems, alarms are also used to alert operators when process
operating or monitoring conditions go out of preset parameters (i.e.,
process alarms). These types of alarms are primarily integrated with
process monitoring and reporting systems (e.g., SCADA systems). Note
that this discussion does not focus on alarm systems that are not
related to the processes of a utility. Alarm systems can be integrated
with fire detection systems, intrusion detection systems, access control
systems, or closed-circuit television (CCTV) systems such that these
systems automatically respond when the alarm is triggered. A smoke
detector alarm, for example, can be set up to automatically notify the
fire department when smoke is detected, or an intrusion alarm can
automatically trigger cameras to turn on in a remote location so
personnel can monitor that location. An alarm system consists of sensors
that detect different types of events; an arming station that is used to
turn the system on and off; a control panel that receives information,
processes it, and transmits the alarm; and an annunciator, which
generates a visual or audible response to the alarm. When a sensor is
tripped, it sends a signal to a control panel, which triggers a visual
or audible alarm or notifies a central monitoring station. A more
complete description of each of the components of an alarm system is
provided below. Detection devices (also called sensors) are designed to
detect a specific type of event (such as smoke or intrusion). Depending
on the type of event they are designed to detect, sensors can be located
inside or outside of the facility or other asset. When an event is
detected, the sensors use some type of communication method (such as
wireless radio transmitters, conductors, or cables) to send signals to
the control panel to generate the alarm; for example, a smoke detector
sends a signal to a control panel when it detects smoke. An arming
station, which is the main user interface with the security system,
allows the user to arm (turn on), disarm (turn off), and communicate
with the system. How a specific system is armed will depend on how it is
used. Although intrusion detection systems can be armed for continuous
operation (24 hours a day), they are usually armed and disarmed
according to the work schedule at a specific location so personnel going
about their daily activities do not set off the alarms. In contrast,
fire protection systems are typically armed 24 hours a day. The control
panel receives information from the sensors and sends it to an
appropriate location, such as to a central operations station or to a
24-hour monitoring facility. When the alarm signal is received at the
central monitoring location, personnel monitoring for alarms can respond
(such as by sending security teams to investigate or by dispatching the
fire department). The annunciator responds to the detection of an event
by emitting a signal. This signal may be visual, audible, or electronic,
or a combination of these three; for example, fire alarm signals will
always be connected to audible annunciators, whereas intrusion alarms
may not be. Alarms can be reported locally, remotely, or both locally
and remotely. A local alarm emits a signal at the location of the event
(typically using a bell or siren). A local-only alarm emits a signal at
the location of the event but does not transmit the alarm signal to any
other location (i.e., it does not transmit the alarm to a central
monitoring location). Typically, the purpose of a local-only alarm is to
frighten away intruders and possibly to attract the attention of someone
who might notify the proper authorities. Because no signal is sent to a
central monitoring location, personnel can only respond to a local alarm
if they are in the area and can hear or see the alarm signal. Fire alarm
systems must have local alarms, including both audible and visual
signals. Most fire alarm signal and response requirements are codified
in the National Fire Alarm Code, National Fire Protection Association
(NFPA) 72, which discusses the application, installation, performance,
and maintenance of protective signaling systems. In contrast to fire
alarms, which require a local signal when fire is detected, many
intrusion detection systems do not have a local alert device, because
monitoring personnel do not wish to inform potential intruders that they
have been detected. Instead, these types of systems silently alert
monitoring personnel that an intrusion has been detected, thus allowing
monitoring personnel to respond. In contrast to systems that are set up
to transmit local-only alarms when the sensors are triggered, systems
can also be set up to transmit signals to a central location, such as to
a control room or guard post at the utility or to a police or fire
station. Most fire and smoke alarms are set up to signal both at the
location of the event and at a fire station or central monitoring
station. Many insurance companies require that facilities install
certified systems that include alarm communication to a central station;
for example, systems certified by the Underwriters Laboratory (UL)
require that the alarm be reported to a central monitoring station. The
main differences among alarm systems lie in the types of event detection
devices used in different systems. Intrusion sensors, for example,
consist of two main categories: perimeter sensors and interior (space)
sensors. Perimeter intrusion sensors are typically applied on fences,
doors, walls, windows, etc. and are designed to detect intruders before
they gain access to a protected asset (e.g., perimeter intrusion sensors
are used to detect intruders attempting to enter through a door or
window). In contrast, interior intrusion sensors are designed to detect
an intruder who has already accessed the protected asset (i.e., interior
intrusion sensors are used to detect intruders when they are already
within a protected room or building). These two types of detection
devices can be complementary, and they are often used together to
enhance security for an asset. A typical intrusion alarm system, for
example, might employ a perimeter glass-break detector that protects
against intruders accessing a room through a window, as well as an
ultrasonic interior sensor that detects intruders that have gotten into
the room without using the window. Fire detection and fire alarm systems
consist of various combinations of fire detection devices and fire alarm
systems. These systems may detect fire, heat, or smoke, or a combination
of any of these. A typical fire alarm system might consist only of heat
sensors located throughout a facility that detect high temperatures or a
certain change in temperature over a fixed time period, whereas a
different system might be outfitted with both smoke and heat detection
devices. When a sensor in an alarm system detects an event, it must
communicate an alarm signal. The two basic types of alarm communication
systems are hardwired and wireless. Hardwired systems rely on wire that
is run from the control panel to each of the detection devices and
annunciators. Wireless systems transmit signals from a transmitter to a
receiver through the air---primarily using radio or other waves.
Hardwired systems are usually lower cost, more reliable (they are not
affected by terrain or environmental factors), and significantly easier
to troubleshoot than wireless systems; however, a major disadvantage of
hardwired systems is that it may not be possible to hardwire all
locations (e.g., it may be difficult to hardwire remote locations). In
addition, running wires to their required locations can be both time
consuming and costly. The major advantage to using wireless systems is
that they can often be installed in areas where hardwired systems are
not feasible; however, wireless components can be much more expensive
when compared to hardwired systems. Also, in the past it has been
difficult to perform self-diagnostics on wireless systems to confirm
that they are communicating properly with the controller. Currently, the
majority of wireless systems incorporate supervising circuitry, which
allows the subscriber to recognize immediately any problem with the
system (such as a broken detection device or a low battery) or if a
protected door or window has been left open.

Backflow Prevention Devices Backflow prevention devices are designed to
prevent backflow, which is the reversal of the normal and intended
direction of water flow in a water system. Backflow is a potential
problem in a water system because it can spread contaminated water back
through a distribution system. For example, pollution or backflow at
uncontrolled cross-connections (any actual or potential connection
between the public water supply and a source of contamination) can allow
pollutants or contaminants to enter the potable water system. More
specifically, backflow from private plumbing systems, industrial areas,
hospitals, and other hazardous contaminant-containing systems into
public water mains and wells poses serious public health risks and
security problems. Cross-contamination from private plumbing systems can
contain biological hazards (such as bacteria or viruses) or toxic
substances that can contaminate and sicken an entire population in the
event of backflow. The majority of historical incidences of backflow
have been accidental, but growing concern that contaminants could be
intentionally backfed into a system is prompting increased awareness
among private homeowners, businesses, industries, and areas most
vulnerable to intentional strikes; therefore, backflow prevention is a
major component of water system protection. Backflow may occur under two
types of conditions: backpressure and backsiphonage. Backpressure is a
reversal of normal flow direction within a piping system resulting from
the downstream pressure being higher than the supply pressure. These
reductions in the supply pressure occur whenever the amount of water
being used exceeds the amount of water supplied, such as during
water-main flushing, fire fighting, or breaks in water mains.
Backsiphonage is a reversal of normal flow direction within a piping
system caused by negative pressure in the supply piping (the reversal of
normal flow in a system caused by a vacuum or partial vacuum within the
water supply piping). Backsiphonage can occur due to high velocity in a
pipeline, a line repair or break that is lower than a service point, or
a lowered main pressure caused by a high water withdrawal rate, such as
during fire fighting or water-main flushing. To prevent backflow,
various types of backflow preventers are appropriate for use. The
primary types of backflow preventers are

• Air gap drains • Double check valves • Reduced pressure principle
assemblies • Pressure vacuum breakers Biometric Security Systems
Biometrics involves measuring the unique physical characteristics or
traits of the human body. Any aspect of the body that is measurably
different from person to person---for example, fingerprints or eye
characteristics---can serve as unique biometric identifiers for
individuals. Biometric systems recognizing fingerprints, palm shape,
eyes, face, voice, and signature comprise the bulk of the current
biometric systems. Biometric security systems use biometric technology
combined with some type of locking mechanism to control access to
specific assets. To access an asset that is controlled by a biometric
security system, an individual's biometric trait must be matched with an
existing profile stored in a database. If a match between the two is
identified, the locking mechanism (e.g., a physical lock at a doorway,
an electronic lock at a computer terminal) is disengaged, and the
individual is given access to the asset. A biometric security system is
typically comprised of the following components:

• A sensor measures and records a biometric characteristic or trait. • A
control panel serves as the connection point between various system
components. The control panel communicates information back and forth
between the sensor and the host computer and controls access to the
asset by engaging or disengaging the system lock based on internal logic
and information from the host computer. • A host computer processes and
stores the biometric trait in a database. • Specialized software
compares an individual image taken by the sensor with stored profiles. •
A locking mechanism is controlled by the biometric system. • A power
source supplies power to the system. Biometric Hand and Finger Geometry
Recognition Hand and finger geometry recognition is the process of
identifying an individual through the unique geometry (e.g., shape,
thickness, length, width) of that individual's hand or fingers. Hand
geometry recognition has been employed since the early 1980s and is a
widely used biometric technologies for controlling access to important
assets. It is simple to install and use and is appropriate for any
location requiring highly accurate biometric security; for example, it
is currently used in numerous workplaces, daycare facilities, hospitals,
universities, airports, and power plants. One option within hand
geometry recognition technology is finger geometry recognition (not to
be confused with fingerprint recognition). Finger geometry recognition
relies on the same scanning methods and technologies as does hand
geometry recognition, but the scanner scans only two of the user's
fingers, as opposed to the entire hand. Finger geometry recognition has
been in commercial use since the mid1990s and is mainly used in time and
attendance applications (i.e., to track when individuals have entered
and exited a location). The first large-scale commercial application of
two-finger geometry was at Disney World to control access to the park.
Season-pass holders used the geometry of their index and middle finger
to gain access to the facilities, but today single finger scanners are
used. Hand and finger geometry recognition systems can be used in
several different types of applications, including access control and
time and attendance tracking. Although time and attendance tracking can
be used for security purposes, it is primarily used in operations and
payroll areas (e.g., clocking in and clocking out). In contrast, access
control applications are more likely to be security related. Biometric
systems are widely used for access control and can be used to protect
various types of assets, including entryways, computers, and vehicles.
Because of their size, however, hand and finger recognition systems are
primarily limited to use in entryway access control applications.
Biometric Iris Recognition The iris is the colored or pigmented area of
the eye surrounded by the sclera (the white portion of the eye); it is a
muscular membrane that controls the amount of light entering the eye by
contracting or expanding the pupil (the dark center of the eye). The
dense, unique patterns of connective tissue in the human iris were first
noted in 1936, but it was not until 1994, when algorithms for iris
recognition were created and patented, that commercial applications
using biometric iris recognition began to be used extensively. Several
vendors are producing iris recognition technology, including the
original developer of these algorithms as well as companies that have
developed and patented different sets of algorithms for iris
recognition. The iris is an ideal characteristic for identifying
individuals because it is formed in utero, and its unique patterns
stabilize around 8 months after birth. No two irises are alike---neither
an individual's right and left irises nor the irises of identical twins.
The iris is protected by the cornea (the clear covering over the eye);
therefore, it is not subject to the aging or physical changes (and
potential variation) that are common to some other biometric measures,
such as the hand, fingerprints, and the face. Although some limited
changes can occur naturally over time, these changes generally occur in
the melanin of the iris and therefore affect only the eye's color, not
its unique patterns. Because iris scanning uses only black and white
images, color changes do not affect the effectiveness of the scan.
Barring specific injuries or surgeries directly affecting the iris, the
unique patterns of the iris remain relatively unchanged over an
individual's lifetime. Iris recognition systems employ a monochromatic,
or black and white, video camera that uses both visible and
near-infrared light to take a video of an individual's iris. Video is
used rather than still photography as an extra security procedure. The
video is used to confirm the normal continuous fluctuations of the pupil
as the eye focuses, which ensures that the scan is of a living human
being and not a photograph or some other attempted hoax. A
high-resolution image of the iris is then captured or extracted from the
video using a device often referred to as a frame grabber. The unique
characteristics identified in this image are then converted into a
numeric code, which is stored as a template for that user.

Card Identification and Access Tracking Systems A card reader system is
a type of electronic identification system that is used to read a card
and then perform an action associated with that card. Depending on the
system, the card may identify where a person is or where they were at a
certain time, or it may authorize another action such as disengaging a
lock. Security guards, for example, may use their cards at card readers
located throughout a facility to indicate that they have checked certain
locations at certain times. The reader will store the information or
send it to a central location where the data can be checked later to
verify that various areas in the facility have been patrolled. Other
card reader systems can be associated with a lock, such that cardholders
must have their cards read and accepted by the reader before the lock
disengages. A complete card reader system typically consists of the
following components:

• Access cards carried by the user • Card readers, which read the card
signals and send the information to control units • Control units, which
control the response of the card reader to the card • A power source

Numerous types of card reader systems are available. All card systems
are similar with regard to how the card reader and control unit
interact; however, they differ in how data are encoded on the cards and
are transferred between the cards and the card readers, which determines
the types of applications for which they are best suited. Several types
of technologies are available for card reader systems, including the
following:

• Proximity • Wiegand • Smartcard • Magnetic stripe • Bar code •
Infrared • Barium ferrite • Hollerith • Mixed technologies

The level of security required (low, moderate, or high) affects the
choice of card technology (e.g., how simple is it to duplicate a
particular technology and thus bypass the security system).
Vulnerability ratings are based on how easily the card reader can be
damaged due to frequent use or challenging working conditions (e.g.,
weather conditions if the reader is located outside). Often, the
vulnerability of a system is influenced by the number of moving parts in
the system---the more moving parts, the greater the potential
susceptibility to damage. Life-cycle ratings are based on the durability
of a given card reader system over its entire operational period.
Systems requiring frequent physical contact between the reader and the
card often have a shorter life cycle due to the wear and tear to which
the equipment is exposed. For many card reader systems, the
vulnerability rating and life-cycle ratings have a reciprocal
relationship; for example, if a given system has a high vulnerability
rating it will almost always have a shorter life cycle. Card reader
technology can be implemented for facilities of any size and with any
number of users; however, because individual systems vary in the
complexity of their technology and the level of security they can
provide to a facility, individual users must determine the appropriate
system for their needs. Some important questions to consider when
selecting a card reader system include the following: • What level of
technological sophistication and security does the card system have? •
How large is the facility, and what are its security needs? • How
frequently will the card system be used? For systems that will
experience a high frequency of use, it is important to consider a system
that has a longer life cycle and lower vulnerability rating, thus making
it more cost effective to implement. • Under what conditions will the
system be used? For example, will it be installed on the interior or
exterior of buildings? Does it require light or humidity controls? Most
card reader systems can operate under normal environmental conditions;
therefore, this would be a mitigating factor only in extreme conditions.
• What are the system costs?

Exterior Intrusion Sensors Exterior vs.~Interior Intrusion Sensors An
exterior intrusion sensor is a detection device used in an outdoor
environment to detect intrusions into a protected area. Intrusion
sensors are designed to detect an intruder and then communicate an alarm
signal to an alarm system. The alarm system can respond to the intrusion
in many different ways, such as by triggering an audible or visual alarm
signal or by sending an electronic signal to a central monitoring
location that notifies security personnel of the intrusion. Intrusion
sensors can be used to protect many kinds of assets. Intrusion sensors
that protect physical space are classified according to whether they
protect indoor, or interior, spaces (e.g., an entire building or room
within a building), or outdoor, or exterior, spaces (e.g., a fence line
or perimeter). Interior intrusion sensors are designed to protect the
interior space of a facility by detecting an intruder who is attempting
to enter or who has already entered a room or building. In contrast,
exterior intrusion sensors are designed to detect an intrusion into a
protected outdoor/exterior area. Exterior protected areas are typically
arranged as zones or exclusion areas placed so the intruder is detected
early in the intrusion attempt before gaining access to more valuable
assets (e.g., into a building located within the protected area). Early
detection creates additional time for security forces to respond to the
alarm.

Buried Exterior Intrusion Sensors Buried sensors are electronic devices
that are designed to detect potential intruders. The sensors are buried
along the perimeters of sensitive assets and are able to detect intruder
activity both aboveand below-ground. Some of these systems are composed
of individual, stand-alone sensor units, while other sensors consist of
buried cables. Fences A fence is a physical barrier that can be set up
around the perimeter of an asset. Fences often consist of individual
pieces (such as individual pickets in a wooden fence or individual
sections of a wrought iron fence) that are fastened together. Individual
sections of the fence are fastened together using posts, which are sunk
into the ground to provide stability and strength for the sections of
the fence hung between them. Gates are installed between individual
sections of the fence to allow access inside the fenced area. Fences are
often used as decorative architectural features to separate physical
spaces from each other and to mark the location of a boundary (such as a
fence installed along a properly line); however, a fence can also serve
as an effective means for preventing intruders from gaining access to a
water or wastewater asset. Many utilities install fences around their
primary facilities, around remote pump stations, or around hazardous
materials storage areas or sensitive areas within a facility. Access to
the area can be controlled through security located at gates or doors in
the fence (e.g., posting a guard at the gate or locking it). To gain
access to the asset, unauthorized persons would have to find a way
either around or through the fence. Fences are often compared with walls
when determining the appropriate system for perimeter security. Both
fences and walls can provide adequate perimeter security, and fences are
often easier and less expensive to install than walls; however, they do
not usually provide the same physical strength that walls do. In
addition, many types of fences have gaps between the individual pieces
that make up the fence (e.g., the spaces between chain links in a
chain-link fence or the space between pickets in a picket fence); thus,
many types of fences allow the interior of the fenced area to be seen.
This may allow intruders to gather important information about the
locations or defenses of vulnerable areas within the facility. Numerous
types of materials are used to construct fences, including chain link
iron, aluminum, wood, or wire. Some types of fences, such as split rails
or pickets, may not be appropriate for security purposes because they
are traditionally low fences, and they are not physically strong.
Potential intruders may be able to easily defeat these fences either by
jumping or climbing over them or by breaking through them; for example,
the rails in a split fence may be able to be broken easily. Important
security features of a fence include the height to which it can be
constructed, the strength of the material comprising the fence, the
method and strength of attaching the individual sections of the fence
together at the posts, and the ability of the fence to restrict the view
of the assets inside the fence. Additional considerations include the
ease of installing the fence and the ease of removing and reusing
sections of the fence. Some fences can include additional measures to
delay, or even detect, potential intruders. Such measures may include
the addition of barbed wire, razor wire, or other deterrents at the top
of the fence. Barbed wire employed at the base of a fence can also
impede a would-be intruder's access to the fence. Fences can be fitted
with security cameras to provide visual surveillance of the perimeter,
and some facilities have installed motion sensors along their fences to
detect movement on the fence. Several manufacturers have combined these
multiple perimeter security features into one product and offer alarms
and other security features. The correct implementation of a fence can
make it a much more effective security measure. Security experts
recommend the following when a facility constructs a fence:

• The fence should be at least 7 to 9 feet high. • Any outriggers, such
as barbed wire, that are affixed on top of the fence should be angled
out and away from the facility, not in toward the facility. This will
make climbing the fence more difficult and will prevent the placement of
ladders against the fence. • Other types of hardware that can increase
the security of the fence include installing concertina wire along the
fence (this can be done in front of the fence or at the top of the
fence) or adding intrusion sensors, camera, or other hardware to the
fence. • All undergrowth should be cleared for several feet (typically 6
feet) on both sides of the fence. This will allow for a clearer view of
the fence by any patrols in the area. • Any trees with limbs or branches
hanging over the fence should be trimmed so intruders cannot use them to
go over the fence. Also, it should be noted that fallen trees can damage
fences, so management of trees around the fence can be important. This
can be especially important in areas where the fence runs through a
remote area. • Fences that do not block the view from outside the fence
allow security patrols to see inside the fence without having to enter
the facility. • ``No Trespassing'' signs posted along a fence can be a
valuable tool in prosecuting any intruders who claim that the fence was
broken and that they did not enter through the fence illegally. Adding
signs that highlight the local ordinances against trespassing can
further dissuade simple troublemakers from illegally climbing the fence.

Films for Glass Shatter Protection Most water utilities have numerous
windows on the outside of buildings, in doors, and in interior offices.
In addition, many facilities have glass doors or other glass structures,
such as glass walls or display cases. These glass objects are
potentially vulnerable to shattering when heavy objects are thrown or
launched at them, when explosions occur near them, or when there are
high winds. If the glass is shattered, intruders may potentially enter
an area. In addition, shattered glass projected into a room from an
explosion or from an object being thrown through a door or window can
injure and potentially incapacitate personnel in the room. Materials
that prevent glass from shattering can help to maintain the integrity of
the door, window, or other glass object and can delay an intruder from
gaining access. These materials can also prevent flying glass and thus
reduce potential injuries. Materials designed to prevent glass from
shattering include specialized films and coatings. These materials can
be applied to existing glass objects to improve their strength and their
ability to resist shattering. The films have been tested against many
scenarios that could result in glass breakage, including penetration by
blunt objects, bullets, high winds, and simulated explosions. They are
tested against simulated weather scenarios (including high winds and the
force of objects blown into the glass), as well as criminal or terrorist
scenarios where the glass could be subject to explosives or bullets.
Many vendors provide information on the results of these types of tests
so potential users can compare different product lines to determine
which products best suit their needs. The primary considerations with
regard to films for shatter protection are

• The materials from which the film is made • The adhesive that bonds
the film to the glass surface • The thickness of the film

Fire Hydrant Locks Fire hydrants are installed at strategic locations
throughout a community's water distribution system to supply water for
fire fighting. Because the many hydrants in a system are often located
in residential neighborhoods, industrial districts, and other areas
where they cannot be easily observed or guarded, they are potentially
vulnerable to unauthorized access. Many municipalities, states, and
USEPA regions have recognized this potential vulnerability and have
instituted programs to lock hydrants; for example, USEPA Region 1
includes locking hydrants as the seventh item in its Drinking Water
Security and Emergency Preparedness ``top ten'' list for small
groundwater suppliers. A hydrant lock is a physical security device
designed to prevent unauthorized access to the water supply through a
hydrant. Such locks can ensure water and water pressure availability to
fire fighters and prevent water theft and associated lost water revenue.
These locks have been used successfully in numerous municipalities and
in various climates and weather conditions. Fire hydrant locks are
basically steel covers or caps that are locked in place over the
operating nut of a fire hydrant. The lock prevents unauthorized persons
from accessing the operating nut and opening the fire hydrant valve. The
lock also makes it more difficult to remove the bolts from the hydrant
and access the system that way. Finally, hydrant locks shield the valve
from being broken off. Should a vandal attempt to breach the hydrant
lock by force and succeed in breaking the hydrant lock, the vandal will
only succeed in bending the operating valve. If the operating valve of
the hydrant is bent, the hydrant will not be operational, but the water
asset remains protected and inaccessible to vandals; however, the entire
hydrant will have to be replaced. The locking mechanisms for fire
hydrant locking systems ensure that hydrants can only be accessed by
authorized personnel who have the special key wrench required to operate
a hydrant without removing the lock. These specialized wrenches are
generally distributed to the fire department, public works department,
and other authorized persons so they can access the hydrants as needed.
An inventory of wrenches and their serial numbers is generally kept by a
municipality so the location of all wrenches is known. These operating
key wrenches may only be purchased by registered lock owners. The most
important features of hydrant are their strength and the security of
their locking systems. The locks must be strong so they cannot be broken
off. Hydrant locks are constructed from stainless or alloyed steel.
Stainless-steel locks are stronger and are ideal for all climates;
however, they are more expensive than alloy locks Hatch Security A hatch
is basically a door installed on a horizontal plane (such as in a floor,
a paved lot, or a ceiling), instead of on a vertical plane (such as in a
building wall). Hatches are usually used to provide access to assets
that are located underground (e.g., in basements or underground storage
areas) or above ceilings (such as emergency roof exits). At water and
wastewater facilities, hatches are typically used to provide access to
underground vaults containing pumps, valves, or piping, or to the
interior of water tanks or covered reservoirs. Securing a hatch by
locking it or upgrading materials to give the hatch added strength can
help to delay unauthorized access to any asset behind the hatch. Like
all doors, a hatch consists of a frame anchored to the horizontal
structure, a door or doors, hinges connecting the door to the frame, and
a latching or locking mechanism that keeps the hatch door closed. It
should be noted that improving hatch security is straightforward and
that hatches with upgraded security features can be installed new or
they can be retrofit for existing applications. Many municipalities
already have specifications for hatch security at their water and
wastewater utility assets. Depending on the application, the primary
security-related attributes of a hatch are the strength of the door and
frame, its resistance to the elements and corrosion, it ability to be
sealed against water or gas, and its locking features. Hatches must be
both strong and lightweight so that they can withstand typical static
loads (such as people or vehicles walking or driving over them) while
still being easy to open. In addition, because hatches are typically
installed at outdoor locations, they are usually fabricated out of
corrosion-resistant metal that can withstand the elements such as
high-gauge steel or lightweight aluminum. The hatch locking mechanism is
perhaps the most important component of hatch security. A number of
locks can be implemented for hatches, including • Slam locks (internal
locks located within the hatch frame) • Recessed cylinder locks • Bolt
locks • Padlocks

Ladder Access Control Water and wastewater utilities have a number of
assets that are raised above ground level, including raised water tanks,
raised chemical tanks, raised piping systems, and roof access points
into buildings. In addition, communications equipment, antennae, or
other electronic devices may be located on the top of these raised
assets. Typically, these assets are reached by ladders that are
permanently anchored to the asset; for example, raised water tanks
typically are accessed by ladders that are bolted to one of the legs of
the tank. Controlling access to these raised assets by controlling
access to the ladder can increase security at a water or wastewater
utility. A typical ladder access control system consists of some type of
cover that is locked or secured over the ladder. The cover can be a
casing that surrounds most of the ladder or a door or shield that covers
only part of the ladder. In either case, several rungs of the ladder
(the number of rungs depends on the size of the cover) are made
inaccessible by the cover, and these rungs can only be accessed by
opening or removing the cover. The cover is locked so only authorized
personnel can open or remove it and use the ladder. Ladder access
controls are usually installed at several feet above ground level, and
they usually extend several feet up the ladder so they cannot be
circumvented by someone accessing the ladder above the control system.
The covers are constructed from aluminum or some type of steel. This
should provide adequate protection from being pierced or cut through.
The metals are corrosion resistant so they will not corrode or become
weakened due to extreme weather conditions in outdoor applications. The
bolts used to install each of these systems are galvanized steel. In
addition, the bolts for each cover are installed on the inside of the
unit so they cannot be removed from the outside. The important features
of ladder access control are the size and strength of the cover and the
ability to lock or otherwise secure the cover from unauthorized access.

Locks A lock is a type of physical security device that can be used to
delay or prevent the opening, moving, or operation of a door, window,
manhole, filing-cabinet drawer, or some other physical feature. Locks
typically operate by connecting two pieces together, such as connecting
a door to a door jamb or a manhole to its casement. Every lock has two
modes: engaged (or locked) and disengaged (or opened). When a lock is
disengaged, the asset on which the lock is installed can be accessed by
anyone, but when the lock is engaged, only access to the locked asset.
Locks are excellent security features because they have been designed to
function in many ways and to work on many different types of assets.
Locks can also provide different levels of security depending on how
they are designed and implemented. The security provided by a lock is
dependent on several factors, including its ability to withstand
physical damage (e.g., being cut off, broken, or otherwise physically
disabled) as well as its requirements for supervision or operation
(e.g., combinations may have to be changed frequently so they are not
compromised and the locks remain secure). Although no agreed-upon rating
of lock security exists, locks are often designated as being minimum,
medium, or maximum security. Minimum security locks are those that can
be easily disengaged (or ``picked'') without the correct key or code or
those that can be disabled easily (such as small padlocks that can be
cut with bolt cutters). Higher security locks are more complex and thus
are more difficult to pick, or they are sturdier and more resistant to
physical damage. Many locks only have to be unlocked from one side; for
example, most door locks (single-cylinder locks) can be opened on the
outside by inserting a key in the lock. On the inside, a person can
unlock the same lock by pushing a button or turning a knob or handle.
Double cylinder locks require a key to be locked or unlocked from both
sides.

Manhole Intrusion Sensors Manholes are located at strategic locations
throughout most municipal water, wastewater, and other underground
utility systems. Manholes are designed to provide access to underground
utilities, and they represent potential entry points to a system; for
example, manholes in water or wastewater systems may provide access to
sewer lines or vaults containing on/off or pressure-reducing water
valves. Because many utilities run under other infrastructure (roads,
building), manholes also provide potential access points to critical
infrastructure as well as water and wastewater assets. In addition,
because the portion of the system to which manholes provide entry is
primarily located underground, access to a system through a manhole
increases the chances that an intruder will not be seen; therefore,
protecting manholes can be a critical component of guarding an entire
community. The various methods for protecting manholes are designed to
prevent unauthorized personnel from physically accessing the manhole or
to detect attempts at unauthorized access to the manhole. A manhole
intrusion sensor is a physical security device designed to detect
unauthorized access to the utility through a manhole. Monitoring a
manhole that provides access to a water or wastewater system can
mitigate two distinct types of threats. First, monitoring a manhole may
detect access of unauthorized personnel to water or wastewater systems
or assets through the manhole. Second, monitoring manholes may also
allow the detection of the introduction of hazardous substances into the
water system. Several different technologies have been used for manhole
intrusion sensors, including mechanical systems, magnetic systems, and
fiberoptic and infrared sensors. Some of these intrusion sensors have
been specifically designed for manholes, while others consist of
standard, off-theshelf intrusion sensors that have been implemented in a
system specifically designed for application in a manhole.

Manhole Locks A manhole lock is a physical security device designed to
delay unauthorized access to the utility through a manhole. Locking a
manhole that provides access to a water or wastewater system can
mitigate two distinct types of threats. First, locking a manhole may
delay access of unauthorized personnel to water or wastewater systems
through the manhole. Second, locking manholes may also prevent the
introduction of hazardous substances into the wastewater or stormwater
system.

Radiation Detection Equipment for Monitoring Personnel and Packages A
major potential threat that water and wastewater facilities face is
contamination by radioactive substances. Radioactive substances brought
on-site at a facility could be used to contaminate the facility, thereby
preventing workers from safely entering the facility to perform
necessary water treatment tasks. In addition, radioactive substances
brought onsite at a water treatment plant could be discharged into the
water source or the distribution system, contaminating the downstream
water supply; therefore, detection of radioactive substances being
brought on-site can be an important security enhancement. Various
radionuclides have unique properties, and different equipment is
required to detect the different types of radiation; however, it is
impractical and potentially unnecessary to monitor for specific
radionuclides. Instead, for security purposes, it may be more useful to
monitor for gross radiation as an indicator of unsafe substances. To
protect against radioactive materials being brought onsite, a facility
may set up monitoring sites outfitted with radiation detection
instrumentation at entrances to the facility. Depending on the specific
types of equipment chosen, this equipment would detect radiation emitted
from people, packages, or other objects being brought through an
entrance. One of the primary differences between the various types of
detection equipment is the means by which the equipment reads the
radiation. Radiation may be detected by direct measurement or through
sampling. Direct radiation measurement involves measuring radiation
through an external probe on the detection instrumentation. Some direct
measurement equipment detects radiation emitted into the air around the
monitored object. Because this equipment detects radiation in the air,
it does not require that the monitoring equipment make physical contact
with the monitored object. Direct means for detecting radiation include
using either a walk-through, portal-type monitor that detects elevated
radiation levels on a person or in a package or a hand-held detector,
which would be moved or swept over individual objects to locate an
radioactive source. Some types of radiation, such as alpha or low-energy
beta radiation, have a short range and are easily shielded by various
materials. These types of radiation cannot be measured through direct
measurement. Instead, they must be measured through sampling. Sampling
involves wiping the surface to be tested with a special filter cloth and
then exposing the cloth to a special counter; for example, specialized
smear counters measure alpha and low-energy beta radiation.

Reservoir Covers Reservoirs are used to store raw or untreated water.
They can be located underground (buried), at ground level, or on an
elevated surface. Reservoirs can vary significantly in size; small
reservoirs can hold as little as 1000 gallons, and larger reservoirs may
hold many millions of gallons. Reservoirs can be either natural or
manmade. Natural reservoirs can include lakes or other contained water
bodies, whereas manmade reservoirs usually consist of some sort of
engineered structure, such as a tank or other impoundment structure. In
addition to the water containment structure itself, reservoir systems
may also include associated water treatment and distribution equipment,
including intakes, pumps, pump houses, piping systems, chemical
treatment, and chemical storage areas. Drinking water reservoirs are of
particular concern because they are potentially vulnerable to
contamination of the stored water, through direct contamination of the
storage area or via infiltration of the equipment, piping, or chemicals
associated with the reservoir. Because many drinking water reservoirs
are designed as aboveground, open-air structures, they are potentially
vulnerable to airborne deposition, bird and animal wastes, human
activities, and dissipation of chlorine or other treatment chemicals;
however, one of the most serious potential threats to the system is
direct contamination of the stored water through the dumping of
contaminants into the reservoir. Utilities have taken various measures
to mitigate this type of threat, including fencing off the reservoir,
installing cameras to monitor for intruders, and monitoring for changes
in water quality. Another option for enhancing security is covering the
reservoir using some type of manufactured cover to prevent intruders
from gaining physical access to the stored water. Implementing a
reservoir cover may or may not be practical depending on the size of the
reservoir; covers are not typically used on natural reservoirs because
they are too large for the cover to be technically feasible and cost
effective. This section focuses on drinking-water reservoir covers,
where and how they are typically implemented, and how they can be used
to reduce the threat of contamination of the stored water. Although
covers can enhance the security of a reservoir, it should be noted that
covering a reservoir typically changes the operational requirements of
the reservoir; for example, vents must be installed in the cover to
ensure gas exchange between the stored water and the atmosphere. A
reservoir cover is a structure installed on or over the surface of the
reservoir to minimize water quality degradation. The three basic types
of reservoir covers are

• Floating • Fixed • Air-supported

Various materials are used to manufacture a cover, including reinforced
concrete, steel, aluminum, polypropylene, chlorosulfonated polyethylene,
or ethylene interpolymer alloys. Several factors affect the
effectiveness of a reservoir cover and thus its ability to protect the
stored water, including

• The location, size, and shape of the reservoir • The ability to lay or
support a foundation (e.g., footing, soil, and geotechnical support
conditions) • The length of time reservoir can be removed from service
for cover installation or maintenance • Aesthetic considerations •
Economic factors, such as capital and maintenance costs

It may not be practical, for example, to install a fixed cover over a
reservoir if the reservoir is too large or if the local soil conditions
cannot support a foundation. A floating or air-supported cover may be
more appropriate for these types of applications. In addition to the
practical considerations for installation of these types of covers, a
number of operations and maintenance (O\&M) concerns can affect the
utility of a cover for specific applications, including how the various
cover materials available will withstand local climatic conditions, what
types of cleaning and maintenance will be required for each particular
type of cover, and how these factors will affect the covers lifespan and
its ability to be repaired when it is damage. The primary feature
affecting the security of a reservoir cover is its ability to maintain
its integrity. Any type of cover, no matter what its construction
material, will provide good protection from contamination by rainwater
or atmospheric deposition, as well as from intruders attempting to
access the stored water with the intent of causing intentional
contamination. The covers are large and heavy, and it is difficult to
circumvent them to get into the reservoir. At the very least, it would
take a determined intruder, as opposed to a vandal, to defeat the cover.

Security Barriers: Active and Passive One of the most basic threats
facing any facility is from intruders accessing the facility with the
intention of causing damage to its assets. These threats may include
intruders actually entering the facility, as well as intruders attacking
the facility from outside without actually entering it (e.g., detonating
an explosive near enough to the facility to cause damage within its
boundaries). Security barriers are one of the most effective ways to
counter the threat of intruders accessing a facility or the facility
perimeter. Security barriers are large, heavy structures used to control
access through a perimeter by either vehicles or personnel. They can be
used in many different ways depending on how they are installed or where
they are located at the facility; for example, security barriers can be
used on or along driveways or roads to direct traffic to a checkpoint
(e.g., placement of jersey barriers to direct traffic in particular
direction). Other types of security barriers (e.g., crash beams, gates)
can be installed at the checkpoint so guards can regulate which vehicles
can access the facility. Finally, security barriers (e.g., bollards,
security planters) can be used along the facility perimeter to establish
a protective buffer area between the facility and approaching vehicles.
Establishing such a protective buffer can help in mitigating the effects
of an explosive by potentially absorbing some of the blast and by
increasing the stand-off distance between the blast and the facility.
The force of an explosion is reduced as the shock wave travels away from
the source; thus, the greater the distance between the target and an
explosion, the less damage will be incurred. Security barriers can be
either active or passive. Active security barriers, which include gates,
retractable bollards, wedge barriers, and crash barriers, are readily
movable and are typically used in areas where they must be moved often
to allow vehicles to pass---such as in roadways at entrances and exits
to a facility. In contrast to active security barriers, passive security
barriers, which include jersey barriers, bollards, and security
planters, are not designed to be moved on a regular basis and are
typically used in areas where access is not required or allowed---such
as along building perimeters or in traffic-control areas. Passive
security barriers are typically large, heavy structures that are usually
several feet high, and they are designed so even heavy-duty vehicles
cannot go over or though them. They can be placed in a roadway parallel
to the flow of traffic so they direct traffic in a particular direction
(such as to a guardhouse, a gate, or some other sort of checkpoint) or
perpendicular to traffic such that they prevent a vehicle from using a
road or approaching a building or area.

Security for Doorways: Side-Hinged Doors Doorways are the main access
points to a facility or to rooms within a building. They are used on the
exterior or in the interior of buildings to provide privacy and security
for the areas behind them. Different types of doorway security systems
may be installed in various doorways depending on the requirements of
the building or room. For example, exterior doorways tend to have
heavier doors to withstand the elements and to provide some security to
the entrance of the building. Interior doorways in office areas may have
lighter doors that may be primarily designed to provide privacy rather
than security; these doors may be made of glass or lightweight wood.
Doorways in industrial areas may have sturdier doors than do other
interior doorways and may be designed to provide protection or security
for areas behind the doorway; for example, fireproof doors may be
installed in chemical storage areas or in other areas where there is a
danger of fire. Because they are the main entries into a facility or a
room, doorways are often prime targets for unauthorized entry into a
facility or an asset; therefore, securing doorways may be a major step
in providing security at a facility. A doorway includes four main
components: • The door, which blocks the entrance. The primary threat to
the actual door is breaking or piercing through it; therefore, the
primary security features of doors are their strength and resistance to
various physical threats, such as fire or explosions. • The door frame,
which connects the door to the wall. The primary threat to a door frame
is that the door can be pried away from the frame; therefore, the
primary security feature of a door frame is its resistance to prying. •
The hinges, which connect the door to the door frame. The primary threat
to door hinges is that they can be removed or broken, which will allow
intruders to remove the entire door; therefore, security hinges are
designed to be resistant to breaking. They may also be designed to
minimize the threat of removal from the door. • The lock, which connects
the door to the door frame. Use of the lock is controlled through
various security features, such as keys, combinations, etc., such that
only authorized personnel can open the lock and go through the door.
Locks may also incorporate other security features, such as software to
track overall use of the door or to track individuals using the door.

Each of these components is integral to providing security for a
doorway. Upgrading the security of only one of these components while
leaving the other components unprotected may not improve the overall
security of the doorway. Many facilities upgrade door locks as a basic
step toward increasing their security, but if a facility does not also
modify the door hinges or the door frame then the door may remain
vulnerable to being removed from its frame, thus defeating the purpose
of installing the new door lock. The primary attribute for the security
of a door is its strength. Many security doors are 4to 20-gauge hollow
metal doors consisting of steel plates over a hollow cavity reinforced
with steel stiffeners to give the door extra stiffness and rigidity.
This increases resistance to any blunt force applied in an attempt to
penetrate through the door. The space between the stiffeners may be
filled with specialized materials to provide fire, blast, or bullet
resistance to the door. The Window and Door Manufacturers Association
has developed a list of performance attributes for doors:

• Structural resistance • Forced-entry resistance • Hinge-style screw
resistance • Split resistance • Hinge resistance • Security rating •
Fire resistance • Bullet resistance • Blast resistance

The first five items relate to the resistance of a door to standard
physical breaking and prying attacks. Tests are used to evaluate the
strength of the door and the resistance of the hinges and the frame in a
standardized way. The rack load test simulates a prying attack on a
corner of the door. A test panel is restrained at one end, and a third
corner is supported; loads are applied and measured at the fourth
corner. The door impact test uses a steel pendulum to simulate a
battering attack on a door and frame with impacts of 200 ft-lb; the door
must remain fully operable after the test. It should be noted that door
glazing is also rated for resistance to shattering, etc. Manufacturers
will be able to provide security ratings for these features of a door,
as well. Door frames are an integral part of doorway security because
they anchor the door to the wall. Door frames are typically constructed
from wood or steel, and they are installed such that they extend for
several inches over the doorway that has been cut into the wall. For
added security, frames can be designed to have varying degrees of
overlap or wrapping. This overlap can make prying the frame from the
wall more difficult. A frame formed from a continuous piece of metal (as
opposed to a frame constructed from individual metal pieces) will
prevent prying between pieces of the frame. Many security doors can be
retrofit into existing frames; however, many security door installations
require replacement of the door frame as well as the door itself. Bullet
resistance per the Underwriters Laboratory (UL) 752 standard encompasses
the resistance of the door and frame assembly both; thus, replacing the
door only would not meet UL 752 requirements.

Valve Lockout Devices Valves are utilized as control elements in water
and wastewater process piping networks. They regulate the flow of both
liquids and gases by opening, closing, or obstructing a flow passageway.
Valves are typically located where flow control is necessary; they can
be located inline or at pipeline and tank entrance and exit points.
Valves serve multiple purposes in a process pipe network, including

• Redirecting and throttling flow • Preventing backflow • Shutting off
flow to a pipeline or tank (for isolation purposes) • Releasing pressure
• Draining extraneous liquid from pipelines or tanks • Introducing
chemicals into the process network • Serving as access points for
sampling process water

Valves are located at critical junctures throughout water and wastewater
systems, both on-site at treatment facilities and off-site within water
distribution and wastewater collection systems. They may be located
either above or below ground. Because many valves are located within the
community, it is critical to provide protection against valve tampering.
Tampering with a pressure-relief valve, for example, could result in a
pressure buildup and potential explosion in the piping network. On a
larger scale, addition of a pathogen or chemical to the water
distribution system through an unprotected valve could result in the
release of that contaminant to the general population. Various security
products are available to protect aboveground vs.~belowground valves;
for example, valve lockout devices can protect valves and valve controls
located above ground. Vaults containing underground valves can be locked
to prevent access to these valves. Valve-specific lockout devices are
available in a variety of colors, which can be useful in distinguishing
different valves. Different colored lockouts can be used to distinguish
the type of liquid passing through the valve (e.g., treated, untreated,
potable, chemical) or to identify the party responsible for maintaining
the lockout. Implementing a system of colored locks on operating valves
can increase system security by reducing the likelihood of an operator
inadvertently opening the wrong valve and causing a problem in the
system.

Vent Security Vents are installed in aboveground, covered water
reservoirs and in underground reservoirs to allow ventilation of the
stored water. Specifically, vents permit the passage of air that is
being displaced from, or drawn into, the reservoir as the water level in
the reservoir rises and falls due to system demands. Small reservoirs
may require only one vent, whereas larger reservoirs may have multiple
vents throughout the system. The specific vent design for any given
application will vary depending on the design of the reservoir, but
every vent consists of an open-air connection between the reservoir and
the outside environment. These air-exchange vents are an integral part
of covered or underground reservoirs, but they also represent a
potential security threat. Improving vent security by making the vents
tamper resistant or by adding other security features, such as security
screens or security covers, can enhance the security of the entire water
system. Many municipalities already have specifications for vent
security at their water assets. These specifications typically include
the following requirements:

• Vent openings are to be angled down or shielded to minimize the
entrance of surface water or rainwater through the opening. • Vent
designs are to include features that exclude insects, birds, animals,
and dust. • Corrosion-resistant materials are to be used to construct
the vents.

Some states have adopted more specific requirements for added vent
security at their water utility assets. The State of Utah's Department
of Environmental Quality, Division of Drinking Water, Division of
Administrative Rules (DAR), provides specific requirements for public
drinking water storage tanks. The rules for drinkingwater storage tanks
as they apply to venting are set forth in R309-545-15 (Venting) and
include the following requirements:

• Drinking water storage tank vents must have an open discharge on
buried structures. • The vents must be located 24 to 36 inches above the
earthen covering. • The vents must be located and sized to avoid
blockage during winter conditions.

In a second example, Washington State's ``Drinking Water Tech Tips:
Sanitary Protection of Reservoirs'' document states that vents must be
protected to prevent the water supply from being contaminated. The
document indicates that non-corrodible No.~4 mesh may be used to screen
vents on elevated tanks. The document continues to state that the vent
opening for storage facilities located underground or at ground

level should be 24 to 36 inches above the roof or ground and that it
must be protected with a No.~24 inch mesh non-corrodible screen. New
Mexico's administrative Code also specifies that vents must be covered
with No.~24 mesh (NMAC Title 20, Chapter 7, Subpart I, 208.E).
Washington and New Mexico, as well as many other municipalities, require
vents to be screened using a non-corrodible mesh to minimize the entry
of insects, other animals, and rainborne contamination into the vents.
When selecting the appropriate mesh size, it is important to identify
the smallest mesh size that meets both the strength and durability
requirements for that application.

Visual Surveillance Monitoring Visual surveillance is used to detect
threats through continuous observation of important or vulnerable areas
of an asset. The observations can also be recorded for later review or
use (e.g., in court proceedings). Visual surveillance system can be used
to monitor various parts of collection, distribution, or treatment
systems, including the perimeter of a facility, outlying pumping
stations, or entry or access points into specific buildings. These
systems are also useful in recording individuals who enter or leave a
facility, thereby helping to identify unauthorized access. Images can be
transmitted live to a monitoring station, where they can be monitored in
real time, or they can be recorded and reviewed later. Many facilities
have found that a combination of electronic surveillance and security
guards provides an effective means of facility security. Visual
surveillance is provided through a closedcircuit television (CCTV)
system, in which the capture, transmission, and reception of an image is
localized within a closed circuit. This is different than other
broadcast images, such as over-the-air television, which is broadcast
over the air to any receiver within range. At a minimum, a CCTV system
consists of

• One or more cameras • A monitor for viewing the images • A system for
transmitting the images from the camera to the monitor.

Water monitoring DeVices Proper security preparation really comes down
to a three-legged approach: detect, delay, respond. The first leg of
security---detect---is discussed in this section; specifically, this
section deals with the monitoring of water samples to detect toxicity or
contamination. Many of the major monitoring tools that can be used to
identify anomalies in process streams or finished water that may
represent potential threats are discussed, including

• Sensors to monitor chemical, biological, and radiological
contamination • Arsenic measurement systems • Biochemical oxygen demand
(BOD) analyzers • Total organic carbon analyzers • Chlorine measurement
systems • Portable cyanide analyzers • Portable field monitors to
measure volatile organic chemicals (VOCs) • Radiation detection
equipment • Radiation detection equipment for monitoring water assets •
Toxicity monitoring and toxicity meters

communication anD integration This section discusses those devices that
are necessary for communication and the integration of water and
wastewater system operations, such as electronic controllers, two-way
radios, and wireless data communications. Electronic controllers are
used to automatically activate security equipment (such as lights,
surveillance cameras, audible alarms, or locks) when they are triggered.
Triggering could be the result of the tripping of an alarm or a motion
sensor, a window or glass door breaking, variation in vibration sensor
readings, or simply input from a timer. Two-way wireless radios allow
two or more users who have their radios tuned to the same frequency to
communicate instantaneously with each other without the radios being
physically connected with wires or cables. Wireless data communications
devices are used to enable transmission of data between computer systems
or between a SCADA server and its sensing devices, without the
individual components being physically linked together via wires or
cables. In water and wastewater utilities, these devices are often used
to link remote monitoring stations (e.g., SCADA components) or portable
computers to computer networks without the use of physical wiring
connections.

Electronic Controllers An electronic controller is a piece of electronic
equipment that receives incoming electric signals and uses preprogrammed
logic to generate electronic output signals based on the incoming
signals. Electronic controllers can be implemented for any application
that involves inputs and outputs (such as to control equipment in a
factory), but in a security application such controllers essentially act
as the system's brain and can respond to specific security-related
inputs with preprogrammed output responses. These systems combine the
control of electronic circuitry with a logic function such that circuits
are opened and closed (and equipment is turned on and off) through some
preprogrammed logic. The basic principle behind the operation of an
electrical controller is that it receives electronic inputs from sensors
or any device generating an electrical signal (e.g., electrical signals
from motion sensors) and then uses its preprogrammed logic to produce
electrical outputs (e.g., these outputs could turn on power to a
surveillance camera or an audible alarm). Thus, these systems
automatically generate a preprogrammed, logical response to a
preprogrammed input scenario. The three major types of electronic
controllers are timers, electromechanical relays, and programmable logic
controllers (PLCs), which are often called digital relays. Timers use
internal signals or inputs (in contrast to externally generated inputs)
to generate electronic output signals at certain times. More
specifically, timers control electric current flow to any application to
which they are connected and can turn the current on or off on a
schedule prespecified by the user. Typical timer range (amount of time
that can be programmed to elapse before the timer activates linked
equipment) is from 0.2 seconds to 10 hours, although some of the more
advanced timers have ranges of up to 60 hours. Timers are useful in
fixed applications that do not require frequent schedule changes; for
example, a timer can be used to turn on the lights in a room or a
building at a certain time every day. Timers are usually connected to
their own power supply (usually 120 to 240 V). In contrast to timers,
which have internal triggers operating on a regular schedule,
electromechanical relays and PLCs have both external inputs and external
outputs; however, PLCs are more flexible and more powerful than
electromechanical relays and are the predominant technology for
security-related electronic control applications. Electromechanical
relays are simple devices that use a magnetic field to control a switch.
Voltage applied to the input coil of the relay creates a magnetic field
that attracts an internal metal switch. This causes the contacts of the
relay to touch, thus closing the switch and completing the electrical
circuit. This activates any linked equipment. These types of systems are
often used for high-voltage applications, such as in some automotive and
other manufacturing processes.

Two-Way Radios Two-way radios, as discussed here, are limited to direct
unit-to-unit radio communication, either via single unit-to-unit
transmission and reception or via multiple handheld units to a base
station radio contact and distribution system. Radiofrequency spectrum
limitations apply to all hand-held units and are directed by the Federal
Communications Commission (FCC). This discussion also distinguishes
between a hand-held unit and a base station or base station unit, such
as those used by amateur (ham) radio operators, which operate under
different wavelength parameters. Two-way radios allow a user to contact
another user or group of users instantly on the same frequency and to
transmit voice or data without the need for wires. They use half-duplex
communication, which means that they cannot transmit and receive data
simultaneously. In other words, only one person can talk while other
personnel with radios can only listen. To talk, the user depresses the
talk button and speaks into the radio. The audio then transmits the
voice wirelessly to the receiving radios. When the speaker has finished
speaking and the channel has cleared, users on any of the receiving
radios can transmit, either to answer the first transmission or to begin
a new conversation. In addition to carrying voice data, many types of
wireless radios also allow the transmission of digital data, and these
radios may be interfaced with computer networks that can use or track
these data. Some two-way radios can send information such as global
positioning system (GPS) data or the ID of the radio, and others can
send data through a SCADA system. Wireless radios broadcast these voice
or data communications over the airwaves from the transmitter to the
receiver. This can be an advantage in that the signal emanates in all
directions and does not require a direct physical connection to be
received at the receiver, but it can also make the communications
vulnerable to being blocked, intercepted, or otherwise altered.
Additional security features are available, however, to ensure that the
communications are not tampered with. Wireless Data Communications A
wireless data communication system consists of two components---a
wireless access point (WAP) and a wireless network interface card
(sometimes also referred to as a client)---which work together to
complete the communications link. These wireless systems can link
electronic devices, computers, and computer systems together using
radiowaves, thus eliminating the need for the individual components to
be connected physically via wires. Wireless data communications have
found widespread application in water and wastewater systems, but they
also have limitations. First, wireless data connections are limited by
the distance between components (radiowaves scatter over a long distance
and cannot be received efficiently, unless directional antennas are
used). Second, these devices only function if the individual components
are in direct line of sight with each other, because radiowaves are
affected by interference from physical obstructions. In some cases,
however, repeater units can be used to amplify and retransmit wireless
signals to circumvent these problems. The two components of wireless
devices are discussed in more detail below. The wireless access point
(WAP) provides the wireless data communication service. It usually
consists of a housing (constructed from plastic or metal, depending on
the environment in which it will be used) that contains a circuit board,
as well as flash memory that holds the necessary software, one of two
external ports to connect to existing wired networks, a wireless radio
transmitter/receiver, and one or more antenna connections. Typically,
the WAP requires a one-time user configuration to allow the device to
interact with the local area network (LAN). This configuration is
usually accomplished using web-driven software accessed via a computer.
The wireless network interface card or client is a piece of hardware
that is plugged into a computer and enables that computer to make a
wireless network connection. The card consists of a transmitter,
functional circuitry, and a receiver for the wireless signal, all of
which work together to enable communication between the computer, its
wireless transmitter/receiver, and its antenna connection. Wireless
cards are installed in a computer through a variety of connections,
including USB adapters or laptop PCMCIA Cardbus or desktop PCI
peripheral cards. As for the WAP, software is loaded onto the user's
computer, allowing configuration of the card so it may operate over the
wireless network Two of the primary applications for wireless data
communications systems are to enable mobile or remote connections to a
LAN and to establish wireless communications links between SCADA remote
terminal units (RTUs) and sensors in the field. Wireless card
connections are usually used for LAN access from mobile computers.
Wireless cards can also be incorporated into RTUs to allow them to
communicate with sensing devices that are located remotely.

cyber protection DeVices Various cyber protection devices are currently
available for use in protecting utility computer systems. These
protection devices include antivirus and pest eradication software,
firewalls, and network intrusion hardware/software. These products are
discussed in this section. Antivirus and Pest Eradication Software
Antivirus programs are designed to detect, delay, and respond to
programs or pieces of code that are specifically designed to harm
computers. These programs are known as malware. Malware can include
computer viruses, worms, and Trojan horse programs (programs that appear
to be benign but which have hidden harmful effects). Pest eradication
tools are designed to detect, delay, and respond to spyware (strategies
that websites use to track user behavior, such as by sending cookies to
the user's computer) and hacker tools that track keystrokes (keystroke
loggers) or reveal passwords (password crackers). Viruses and pests can
enter a computer system through the Internet or through infected CDs or
flash drives. They can also be placed onto a system by insiders. Some of
these programs, such as viruses and worms, then move throughout the
drives and files of a computer or among networked computers. This
malware can deliberately damage files, utilize memory and network
capacity, crash application programs, and initiate transmissions of
sensitive information from a PC. The specific mechanisms of these
programs differ, but they all can infect files and affect even the basic
operating program of the computer. The most important features of an
antivirus program are its abilities to identify potential malware and to
alert a user before infection occurs, as well as its ability to respond
to a virus already resident on a system. Most of these programs provide
a log so the user can see what viruses have been detected and where they
were detected. After detecting a virus, the antivirus software may
delete the virus automatically, or it may prompt the user to delete the
virus. Some programs will also fix files or programs damaged by the
virus. Various sources of information are available to inform the
general public and computer system operators about new viruses being
detected. Because antivirus programs use signatures (or snippets of code
or data) to detect the presence of a virus, periodic updates are
required to identify new threats. Many antivirus software providers
offer free updates that are able to detect and respond to the latest
viruses.

Firewalls A firewall is an electronic barrier designed to keep computer
hackers, intruders, or insiders from accessing specific data files and
information on a utility's computer network or other electronic/computer
systems. Firewalls operate by evaluating and then filtering information
coming through a public network (such as the Internet) into the
utility's computer or other electronic system. This evaluation can
include identifying the source or destination addresses and ports and
allowing or denying access based on this identification. Two methods are
used by firewalls to limit access to a utility's computers or other
electronic systems from the pubic network:

• The firewall may deny all traffic unless it meets certain criteria. •
The firewall may allow all traffic through unless it meets certain
criteria.

A simple example of the first method is screening requests to ensure
that they come from an acceptable (i.e., previously identified) domain
name and Internet protocol address. Firewalls may also use more complex
rules that analyze the application data to determine if the traffic
should be allowed through; for example, the firewall may require user
authentication (i.e., use of a password) to access the system. How a
firewall determines what traffic to let through depends on which network
layer it operates within and how it is configured. Firewalls may be a
piece of hardware, a software program, or an application card that
contains both. Advanced features that can be incorporated into firewalls
allow for the tracking of attempts to log onto the local area network
system; for example, a report of successful and unsuccessful log-in
attempts may be generated for the computer specialist to analyze. For
systems with mobile users, firewalls allow remote access to the private
network via secure log-on procedures and authentication certificates.
Most firewalls have a graphical user interface for managing the
firewall. In addition, Ethernet firewall cards that fit in the slots of
individual computers bundle additional layers of defense (such as
encryption and permit/deny) for individual computer transmissions to the
network interface function. The cost of these cards is only slightly
higher than for traditional network interface cards.

Network Intrusion Hardware/Software Network intrusion detection and
prevention systems are software and hardware based programs designed to
detect unauthorized attacks on a computer network system. Whereas other
applications, such as firewalls and antivirus software, share similar
objectives with network intrusion systems, network intrusion systems
provide a deeper layer of protection beyond the capabilities of these
other systems because they evaluate patterns of computer activity rather
than specific files. It is worth noting that attacks may come from
either outside or within the system (i.e., from an insider) and that
network intrusion detection systems may be more applicable for detecting
patterns of suspicious activity from inside a facility (e.g., accessing
sensitive data) than other information technology solutions. Network
intrusion detection systems employ a variety of mechanisms to evaluate
potential threats. The types of search and detection mechanisms are
dependent upon the level of sophistication of the system. Some of the
available detection methods include the following:

• Protocol analysis---Protocol analysis is the process of capturing,
decoding, and interpreting electronic traffic. The protocol analysis
method of network intrusion detection involves the analysis of data
captured during transactions between two or more systems or devices and
the evaluation of these data to identify unusual activity and potential
problems. When a problem has been isolated and recorded, potential
threats can be linked to pieces of hardware or software. Sophisticated
protocol analysis will also provide statistics and trend information on
the captured traffic. • Traffic anomaly detection---Traffic anomaly
detection identifies potential threatening activity by comparing
incoming traffic to normal traffic patterns and identifying deviations.
It does this by comparing user characteristics against thresholds and
triggers defined by the network administrator. This method is designed
to detect attacks that span a number of connections, rather than a
single session. • Network honeypot---This method establishes nonexistent
services to identify potential hackers. A network honeypot impersonates
services that do not exist by sending fake information to people
scanning the network. It identifies attackers when they attempt to
connect to the service. There is no reason for legitimate traffic to
access these resources because they do not exist; therefore, any attempt
to access them constitutes an attack. • Anti-intrusion detection system
evasion techniques---These methods are designed to detect attackers who
may be trying to evade intrusion detection system scanning. They include
such methods as IP defragmentation, TCP streams reassembly, and
deobfuscation.

These detection systems are automated, but they can only indicate
patterns of activity, and a computer administer or other experienced
individual must interpret the activities to determine whether or not
they are potentially harmful. Monitoring the logs generated by these
systems can be time consuming, and there may be a learning curve to
determine a baseline of normal traffic patterns from which to
distinguish potential suspicious activity.

SCADA

The Federal Bureau of Investigation (FBI) identified and listed threats
to critical infrastructure (see Table 12.1). In the past few years,
especially since 9/11, it has been somewhat routine for us to pick up a
newspaper or magazine or to view a television news program where a major
topic of discussion is cyber security or the lack

thereof. Many of the cyber intrusion incidents we read or hear about
have added new terms or new uses for old terms to our vocabulary; for
example, old terms such as Trojan horse, worms, and viruses have taken
on new connotations with regard to cyber security issues. Relatively new
terms such as scanners, hacking, mail bombs, sniffers, logic bombs,
nukers, dots, backdoor Trojan, key loggers, Swiss Army knife of hacking,
password crackers, and BIOS crackers are now commonly encountered. Not
all relatively new and universally recognizable cyber terms have
sinister connotation or meaning, of course. Consider, for example, the
following digital terms: app, avatar, backup, binary, bit, byte,
cloaking, CPU, cyberspace, database, email, HTML, icon, IM, memory,
monitor, network, RAM, thumb drive, Wi-Fi, World Wide Web---none of
these terms normally generates thoughts of terrorism in most of us.
There is, however, one digital term, SCADA, that most people have not
heard

of. This is not the case, however, for those who work with the nation's
critical infrastructure, including water/wastewater. SCADA, an acronym
for supervisory control and data acquisition and sometimes referred to
as digital control systems or process control systems, plays an
important role in computer-based control systems. Many water/wastewater
systems use computer-based systems to remotely control sensitive
processes and system equipment previously controlled manually. These
SCADA systems allow a water/wastewater utility to collect data from
sensors and control equipment located at remote sites. Common
water/wastewater system sensors measure elements such as fluid level,
temperature, pressure, water purity, water clarity, and pipeline flow
rates. Common water/wastewater system equipment includes valves, pumps,
and mixers for mixing chemicals in the water supply.

What is scaDa? Simply, SCADA is a computer-based system that remotely
controls processes previously controlled manually. SCADA allows an
operator using a central computer to supervise (control and monitor)
multiple networked computers at remote locations. Each remote computer
can control mechanical processes (pumps, valves, etc.) and collect data
from sensors at its remote location, thus the phrase supervisory control
and data acquisition, or SCADA The central computer is the master
terminal unit (MTU). The operator interfaces with the MTU using software
known as the human-- machine interface (HMI). The remote computer is the
programmable logic controller (PLC) or remote terminal unit (RTU). The
RTU activates a relay (or switch) that turns mechanical equipment on and
off. The RTU also collects data from sensors. In the initial stages,
utilities ran wires (hardwire or land lines) from the central computer
(MTU) to the remote computers (RTUs). Because remote locations can be
located hundreds of miles from the central location, utilities began to
use public phone lines and modems, leased telephone company lines, and
radio and microwave communication. Today, they are also using satellite
links, the Internet, and newly developed wireless technologies. Because
the sensors of SCADA systems provide valuable information, many
utilities established connections between their SCADA system and their
business system. This allowed utility management and other staff access
to valuable statistics, such as water usage. When utilities later
connected their systems to the Internet, they were able to provide
stakeholders with water/wastewater statistics on the utility's web
pages.

scaDa applications in Water systems As stated above, SCADA systems can
be designed to monitor a variety of equipment operating conditions and
parameters, such as volumes, flow rates, or water quality, as well as to
respond to changes in those parameters either by alerting operators or
by modifying system operation through a feedback loop system without
having personnel physically visit each process or piece of equipment on
a daily basis to check it to ensure that it is functioning properly.
SCADA systems can also be used to automate certain functions, so they
can be performed without having to be initiated by an operator (e.g.,
injecting chlorine in response to periodic low chlorine levels in a
distribution system or turning on a pump in response to low water levels
in a storage tank). In addition to process equipment, SCADA systems can
also integrate specific security alarms and equipment, such as cameras,
motion sensors, lights, data from card-reading systems, etc., thereby
providing a clear picture of what is happening at areas throughout a
facility. Finally, SCADA systems also provide constant, real-time data
on processes, equipment, location access, etc., so the necessary
response can be made quickly. This can be extremely useful during
emergency conditions, such as when distribution mains break or when
potentially disruptive BOD spikes appear in wastewater influent. Because
these systems can monitor multiple processes, equipment, and
infrastructure and then provide quick notification of or response to
problems or disruptions, SCADA systems typically provide the first line
of detection for atypical or abnormal conditions. For example, a
real-time, customized operator interface screen can display critical
system monitoring parameters, while a SCADA system connected to sensors
that measure the water quality parameters can report findings measured
outside of a specific range. The system can transmit warning signals
back to the operators, such as by initiating a call to a personal pager.
This might allow the operators to initiate actions to prevent
contamination and disruption of the water supply. Further automation of
the system could ensure that the system initiates measures to rectify
the problem. Preprogrammed control functions (e.g., shutting a valve,
controlling flow, increasing chlorination, or adding other chemicals)
can be triggered and operated based on SCADA utility.

scaDa Vulnerabilities SCADA networks were developed with little
attention paid to security, thus the security of these systems can often
be weak. Studies have found that, although technological advancements
introduced vulnerabilities, many water/wastewater utilities have spent
little time securing their SCADA networks, many of which may be
susceptible to attacks and misuse. Remote monitoring and supervisory
control of processes were initially developed in the early 1960s and
have since adopted many technological advancements. The advent of
minicomputers made it possible to automate a vast number of once
manually operated switches. Advancements in radio technology reduced the
communication costs associated with installing and maintaining buried
cable in remote areas. SCADA systems continued to adopt new
communication methods, including satellite and cellular. As the price of
computers and communications dropped, it became economically feasible to
distribute operations and to expand SCADA networks to include even
smaller facilities (USEPA, 2005). Advances in information technology and
the necessity of improved efficiency have resulted in increasingly
automated and interlinked infrastructures and created new
vulnerabilities due to equipment failure, human error, weather and other
natural causes, and physical and cyber attacks. Some examples of
possible SCADA vulnerabilities include the following:

• Humans---People can be tricked or corrupted and may commit errors. •
Communications---Messages can be fabricated, intercepted, changed,
deleted, or blocked.\\
• Hardware---Security features are not easily adapted to small
self-contained units with limited power supplies. • Physical
threats---Intruders can break into a facility to steal or damage SCADA
equipment. • Natural threats---Tornadoes, floods, earthquakes, and other
natural disasters can damage equipment and connections. •
Software---Programs can be poorly written.

An early survey among water utilities found that they were doing little
to secure their SCADA network vulnerabilities (Ezell, 1998); for
example, many respondents reported that they had remote access, which
can allow an unauthorized person to access the system without being
physically present. More than 60\% of the respondents believed that
their systems were not safe from unauthorized access and use, and 20\%
of the respondents reported known attempts and successful unauthorized
access to their systems. Yet, 22 of 43 respondents reported that they
did not spend any time ensuring the safety of their networks, and 18 of
43 respondents reported that they spent less than 10\% of their time
ensuring network safety. SCADA system computers and their connections
are susceptible to a variety of information system attacks and misuse,
such as system penetration and unauthorized access to information. The
Computer Security Institute has conducted annual computer crime and
security surveys. A recent survey reported that malware and phishing
were the attacks experienced by respondents most often (CSI, 2011). The
study also found that 7\% of the respondents reported exploited wireless
networks, components of a SCADA system. That number is down, however,
from a reported 16\% in 2005. On average, respondents from all sectors
did not believe that their organization invested enough in security
awareness. Utilities as a group reported a lower average computer
security expenditure/investment per employee than many other sectors
such as transportation, telecommunications, and financial. Sandia
National Laboratories categorized common problems it has identified in
the following five categories (Stamp et al., 2003):

\begin{enumerate}
\item
  System data---Important data attributes for security include
  availability, authenticity, integrity, and confidentiality. Data
  should be categorized according to their sensitivity, and ownership
  and responsibility must be assigned; however, SCADA data are often not
  classified at all, making it difficult to identify where security
  precautions are appropriate.
\item
  Security administration---Vulnerabilities emerge because many systems
  lack a properly structured security policy, equipment and system
  implementation guides, configuration management, training, and
  enforcement and compliance auditing.
\item
  Architecture---Many common practices negatively affect SCADA security;
  for example, although it is convenient to use SCADA capabilities for
  other purposes such as fire and security systems, these practices
  create single points of failure. Also, the connection of SCADA
  networks to other automation systems and business networks introduces
  multiple entry points for potential adversaries.
\item
  Network (including communication links)---Legacy system hardware and
  software have very limited security capabilities, and the
  vulnerabilities of contemporary systems (based on modern information
  technology) are publicized. Wireless and shared links are susceptible
  to eavesdropping and data manipulation.
\item
  Platforms---Many platform vulnerabilities exist, including default
  configurations still in place, poor password practices, shared
  accounts, inadequate protection for hardware, and nonexistent security
  monitoring controls. In most cases, important security patches are not
  installed, often due to concern about negatively impacting system
  operation; in some cases, technicians are contractually forbidden from
  updating systems by their vendor agreements.
\end{enumerate}

The following incident helps to illustrate some of the risks associated
with SCADA vulnerabilities (USEPA, 2005):

During the course of conducting a vulnerability assessment, a contractor
stated that personnel from his company penetrated the information system
of a utility within minutes. Contractor personnel drove to a remote
substation and noticed a wireless network antenna. Without leaving their
vehicle, they plugged in their wireless radios and connected to the
network within 5 minutes. Within 20 minutes they had mapped the network,
including SCADA equipment, and accessed the business network and data.

INCREASING RISK Historically, security concerns about control systems
(SCADA included) were related primarily to protecting against physical
attack and misuse of refining and processing sites or distribution and
holding facilities (GAO, 2003). More recently, however, there has been a
growing recognition that control systems are now vulnerable to cyber
attacks from numerous sources, including hostile governments, terrorist
groups, disgruntled employees, and other malicious intruders. In
addition to control system vulnerabilities mentioned earlier, several
factors have contributed to the escalation of risk to control systems:
(1) the adoption of standardized technologies with known
vulnerabilities, (2) the connectivity of control systems to other
networks, (3) constraints on the implementation of existing security
technologies and practices, (4) insecure remote connections, and (5) the
widespread availability of technical information about control systems.

aDoption of technologies With knoWn Vulnerabilities When a technology is
not well known, not widely used, not understood, or not publicized, it
is difficult to penetrate it and thus disable it. Historically,
proprietary hardware, software, and network protocols made it difficult
to understand how control systems operated---and therefore how to hack
into them. Today, however, to reduce costs and improve performance,
organizations have been transitioning from proprietary systems to less
expensive, standardized technologies such as Microsoft's Windows and
Unix-like operating systems and the common networking protocols used by
the Internet. These widely used standardized technologies have commonly
known vulnerabilities, and sophisticated and effective exploitation
tools are widely available and relatively easy to use. As a consequence,
both the number of people with the knowledge to wage attacks and the
number of systems subject to attack have increased. Also, common
communication protocols and the use of Extensible Markup Language (XML)
can make it easier for a hacker to interpret the content of
communications among the components of a control system. Control systems
are often connected to other networks, as enterprises often integrate
their control systems with their enterprise networks. This increased
connectivity has significant advantages, including providing decision
makers with access to realtime information and allowing engineers to
monitor and control the process control system from different points on
the enterprise network. In addition, the enterprise networks are often
connected to the networks of strategic partners and to the Internet.
Further, control systems are increasingly using wide area networks and
the Internet to transmit data to their remote or local stations and
individual devices. This convergence of control networks with public and
enterprise networks potentially exposes the control systems to
additional security vulnerabilities. Unless appropriate security
controls are deployed in the enterprise network and the control system
network, breaches in enterprise security can affect the operation of
controls system. According to industry experts, the use of existing
security technologies, as well as strong user authentication and patch
management practices, are generally not implemented in control systems
because control systems operate in real time, typically are not designed
with cyber security in mind, and usually have limited processing
capabilities. Existing security technologies such as authorization,
authentication, encryption, intrusion detection, and filtering of
network traffic and communications require more bandwidth, processing
power, and memory than control system components typically have. Because
controller stations are generally designed to do specific tasks, they
use low-cost, resource-constrained microprocessors. In fact, some
devices in the electrical industry still use the Intel 8088 processor
first introduced in 1978; consequently, it is difficult to install
existing security technologies without seriously degrading the
performance of the control system. Further, complex passwords and other
strong password practices are not always used to prevent unauthorized
access to control systems, in part because this could hinder a rapid
response to safety procedures during an emergency. As a result,
according to experts, weak passwords that are easy to guess, shared, and
infrequently changed are reportedly common in control systems, as well
as the use of default passwords or even no password at all. In addition,
although modern control systems are based on standard operating systems,
they are typically customized to support control system applications;
consequently, vendor-provided software patches are generally either
incompatible or cannot be implemented without compromising service by
shutting down always-on systems or affecting interdependent operations.
Insecure connections exacerbate potential vulnerabilities. Organizations
often leave access links open for remote diagnostics, maintenance, and
examination of system status. Such links may not be protected with
authentication or encryption which increases the risk that hackers could
use these insecure connections to break into remotely controlled
systems. Also, control systems often use wireless communications
systems, which are especially vulnerable to attack, or leased lines that
pass through commercial telecommunications facilities. Without
encryption to protect data as it flows through these insecure
connections or authentication mechanisms to limit access, there is
limited protection for the integrity of the information being
transmitted. Public information about infrastructures and control
systems is available to potential hackers and intruders. The relatively
easy availability of such data was demonstrated by a university graduate
student whose dissertation reportedly mapped every business and
industrial sector in the American economy to the fiberoptic network that
connects them---using material that was available publicly on the
Internet, none of which was classified. Many of the electric utility
officials who were interviewed for the National Security
Telecommunications Advisory Committee's Information Assurance Task
Force's Electric Power Risk Assessment expressed concern over the amount
of information about their infrastructure that is readily available to
the public. In the electric power industry, open sources of information,
such as product data and educational videotapes from engineering
associations, can be used to understand the basics of the electrical
grid. Other publicly available information, including filings of the
Federal Energy Regulatory Commission (FERC), industry publications,
maps, and material available on the Internet, is sufficient to allow
someone to identify the most heavily loaded transmission lines and the
most critical substations in the power grid. Significant information on
control systems is publicly available, including design and maintenance
documents, technical standards for the interconnection of control
systems and remote terminal units, and standards for communication among
control devises---all of which could assist hackers in determining how
to attack them. Moreover, numerous former employees, vendors, support
contractors, and other end users of the same equipment worldwide have
inside knowledge of the operation of control systems.

cyber threats to control systems There is a general consensus---and
increasing concern---among government officials and experts on control
systems regarding potential cyber threats to the control systems that
govern our critical infrastructures. As components of control systems
increasingly make critical decisions that were once made by humans, the
potential effect of a cyber threat becomes more devastating. Such cyber
threats could come from numerous sources, ranging from hostile
governments and terrorist groups to disgruntled employees and other
malicious intruders. Based on interviews and discussions with
representatives throughout the electric power industry, the Information
Assurance Task Force of the National Security Telecommunications
Advisory Committee concluded that an organization with sufficient
resources, such as a foreign intelligence service or a well-supported
terrorist group, could conduct a structured attack on the electric power
grid electronically, with a high degree of anonymity and without having
to set foot in the target nation. The National Infrastructure Protection
Center (NIPC, 2002) reported that the potential for compound cyber and
physical attacks, referred to as swarming attacks, is a significant
threat to the U.S. critical infrastructure. The effects of a swarming

attack include slowing or complicating any response to the physical
attack. An example would be a cyber attack that disabled the water
supply or the electrical system in conjunction with a physical attack
that denied emergency services the resources necessary to manage the
consequences, such as controlling fires, coordinating actions, and
generating light. Control systems, such as SCADA, can be vulnerable to
cyber attacks. Entities or individuals with malicious intent might take
one or more of the following actions to successfully attack control
systems:

• Disrupt the operation of control systems by delaying or blocking the
flow of information through control networks, thereby denying
availability of the networks to control system operations. • Make
unauthorized changes to programmed instructions in PLCs, RTUs, or
distributed control system (DCS) controllers; change alarm thresholds;
or issue unauthorized commands to control equipment, which could
potentially result in damage to equipment (if tolerances are exceeded),
premature shutdown of processes (such as prematurely shutting down
transmission lines), or even disabling control equipment. • Send false
information to control system operators either to disguise unauthorized
changes or to initiate inappropriate actions by system operators. •
Modify control system software, producing unpredictable results. •
Interfere with the operation of safety systems.

Also, in control systems that cover a wide geographic area, the remote
sites are often unstaffed and may not be physically monitored. If such
remote systems are physically breached, the attackers could establish a
cyber connection to the control network.

securing control systems Several challenges must be addressed to
effectively secure control systems against cyber threats. These
challenges include: (1) limitations of current security technologies
with regard to securing control systems, (2) the perception that
securing control systems may not be economically justifiable, and (3)
conflicting priorities within organizations regarding the security of
control systems. A significant challenge in effectively securing control
systems is the lack of specialized security technologies for these
systems. The computing resources in control systems that are required to
perform security functions tend to be quite limited, making it very
difficult to use security technologies within control system networks
without severely hindering performance. Securing control systems may not
be perceived as economically justifiable. Experts and industry
representatives have indicated that organizations may be reluctant to
spend more money to secure control systems. Hardening the security of
control systems would require industries to expend more resources,
including acquiring more personnel, providing training for personnel,
and potentially prematurely replacing current systems that typically
have a lifespan of about 20 years. Finally, several experts and industry
representatives have indicated that the responsibility for securing
control systems typically includes two separate groups: IT security
personnel

and control system engineers and operators. IT security personnel tend
to focus on securing enterprise systems, while control system engineers
and operators tend to be more concerned with the reliable performance of
their control systems. Further, they indicate that, as a result, those
two groups do not always fully understand each other's requirements and
do not collaborate to implement secure control systems.

steps to improVe scaDa security The President's Critical Infrastructure
Protection Board and the Department of Energy (DOE) developed the steps
outlined below to help organizations improve the security of their SCADA
networks. These steps are not meant to be prescriptive or all inclusive;
however, they do address essential actions to be taken to improve the
protection of SCADA networks. The steps are divided into two categories:
specific actions to improve implementation and actions to establish
essential underlying management processes and policies (DOE, 2001).

21 Steps to Increase SCADA Security The following steps focus on
specific actions to be taken to increase the security of SCADA networks
(DOE, 2001):

\begin{enumerate}
\item
  Identify all connections to SCADA networks. Conduct a thorough risk
  analysis to assess the risk and necessity of each connection to the
  SCADA network. Develop a comprehensive understanding of all
  connections to the SCADA network and how well those connections are
  protected. Identify and evaluate the following types of connections: •
  Internal local area and wide area networks, including business
  networks • The Internet • Wireless network devices, including
  satellite uplinks • Modem or dial-up connections • Connections to
  business partners, vendors, or regulatory agencies
\item
  Disconnect unnecessary connections to the SCADA network. To ensure the
  highest degree of security of SCADA systems, isolate the SCADA network
  from other network connections to as great a degree as possible. Any
  connection to another network introduces security risks, particularly
  if the connection creates a pathway from or to the Internet. Although
  direct connections with other networks may allow important information
  to be passed efficiently and conveniently, insecure connections are
  simply not worth the risk; isolation of the SCADA network must be a
  primary goal to provide needed protection. Strategies such as the
  utilization of demilitarized zones (DMZs) and data warehousing can
  facilitate the secure transfer of data from the SCADA network to
  business networks; however, they must be designed and implemented
  properly to avoid the introduction of additional risk through improper
  configuration.
\item
  Evaluate and strengthen the security of any remaining connections to
  the SCADA networks. Conduct penetration testing or vulnerability
  analysis of any remaining connections to the SCADA network to evaluate
  the
\end{enumerate}

protection posture associated with these pathways. Use this information
in conjunction with risk-management processes to develop a robust
protection strategy for any pathways to the SCADA network. Because the
SCADA network is only as secure as its weakest connecting point, it is
essential to implement firewalls, intrusion detection systems (IDSs),
and other appropriate security measures at each point of entry.
Configure firewall rules to prohibit access from and to the SCADA
network, and be as specific as possible when permitting approved
connections. For example, an Independent System Operator (ISO) should
not be granted blanket network access simply because of a need for a
connection to certain components of the SCADA system. Strategically
place IDSs at each entry point to alert security personnel of potential
breaches of network security. Organization management must understand
and accept responsibility or risks associated with any connection to the
SCADA network. 4. Harden SCADA networks by removing or disabling
unnecessary services. SCADA control servers built on commercial or
open-source operating systems can be exposed to attack through default
network services. To the greatest degree possible, remove or disable
unused services and network demons to reduce the risk of direct attack.
This is particularly important when SCADA networks are interconnected
with other networks. Do not permit a service or feature on a SCADA
network unless a thorough risk assessment of the consequences of
allowing the service or feature shows that the benefits of the service
or feature far outweigh the potential for vulnerability exploitation.
Examples of services to remove from SCADA networks include automated
meter reading/remote billing systems, e-mail services, and Internet
access. An example of a feature to disable is remote maintenance. Refer
to the National Security Agency's series of security guides.
Additionally, work closely with SCADA vendors to identify secure
configurations and coordinate any and all changes to operational systems
to ensure that removing or disabling services does not cause downtime,
interruption of service, or loss of support. 5. Do not rely on
proprietary protocols to protect your system. Some SCADA systems are
unique, proprietary protocols for communications between field devices
and servers. Often the security of SCADA systems is based solely on the
secrecy of these protocols. Unfortunately, obscure protocols provide
very little real security. Do not rely on proprietary protocols or
factor default configuration settings to protect your system.
Additionally, demand that vendors disclose any backdoors or vendor
interfaces to your SCADA systems, and expect them to provide systems
that are capable of being secured. 6. Implement the security features
provided by device and system vendors. Older SCADA systems (most systems
in use) have no security features whatsoever. SCADA system owners must
insist that their system vendors implement security features in the form
of product patches or upgrades. Some newer SCADA devices are shipped
with basic security features, but these are usually disabled to ensure
ease of installation. Analyze each SCADA

device to determine whether security features are present. Additionally,
factory default security settings (such as in computer network
firewalls) are often set to provide maximum usability but minimal
security. Set all security features to provide the maximum security only
after a thorough risk assessment of the consequences of reducing the
security level. 7. Establish strong controls over any medium that is
used as a backdoor into the SCADA network. Where backdoors or vendor
connections do exist in SCADA systems, strong authentication must be
implemented to ensure secure communications. Modems, wireless, and wired
networks used for communications and maintenance represent a significant
vulnerability to the SCADA network and remote sites. Successful ``war
dialing'' or ``war driving'' attacks could allow an attacker to bypass
all of other controls and have direct access to the SCADA network or
resources. To minimize the risk of such attacks, disable inbound access
and replace it with some type of callback system. 8. Implement internal
and external intrusion detection systems and establish 24-hour-a-day
incident monitoring. To be able to effectively respond to cyber attacks,
establish an intrusion detection strategy that includes alerting network
administrators of malicious network activity originating from internal
or external sources. Intrusion detection system monitoring is essential
24 hours a day; this capability can be easily set up through a pager.
Additionally, incident response procedures must be in place to allow an
effective response to any attack. To complement network monitoring,
enable logging on all systems and audit system logs daily to detect
suspicious activity as soon as possible. 9. Perform technical audits of
SCADA devices and networks, and any other connected networks, to
identify security concerns. Technical audits of SCADA devices and
networks are critical to ongoing security effectiveness. Many commercial
and open-sourced security tools are available that allow system
administrators to conduct audits of their systems and networks to
identify active services, patch level, and common vulnerabilities. The
use of these tools will not solve systemic problems but will eliminate
the paths of least resistance that an attacker could exploit. Analyze
identified vulnerabilities to determine their significance, and take
corrective actions as appropriate. Track corrective actions and analyze
this information to identify trends. Additionally, retest systems after
corrective actions have been taken to ensure that vulnerabilities were
actually eliminated. Scan nonproduction environments actively to
identify and address potential problems. 10. Conduct physical security
surveys and assess all remote sites connected to the SCADA network to
evaluate their security. Any location that has a connection to the SCADA
network is a target, especially unmanned or unguarded remote sites.
Conduct a physical security survey and inventory access points at each
facility that has a connection to the SCADA system. Identify and assess
any source of information, including remote telephone/ computer
network/fiberoptic cables, that could be tapped; radio and microwave
links that are exploitable; computer terminals that could be accessed;
and wireless local area network access points. Identify and eliminate
single

points of failure. The security of the site must be adequate to detect
or prevent unauthorized access. Do not allow live network access points
at remote, unguarded sites simply for convenience. 11. Establish SCADA
``Red Teams'' to identify and evaluate possible attack scenarios.
Establish a ``Red Team'' to identify potential attack scenarios and
evaluate potential system vulnerabilities. Use a variety of people who
can provide insight into weaknesses of the overall network, SCADA
system, physical systems, and security controls. People who work on the
system every day have great insight into the vulnerabilities of the
SCADA network and should be consulted when identifying potential attack
scenarios and possible consequences. Also, ensure that the risk from a
malicious insider is fully evaluated, given that this represents one of
the greatest threats to an organization. Feed information resulting from
the ``Red Team'' evaluation into risk-management processes to assess the
information and establish appropriate protection strategies.

The following steps focus on management actions to establish an
effective cyber security program:

\begin{enumerate}
\item
  Clearly define cyber security roles, responsibilities, and authorities
  for managers, system administrators, and users. Organization personnel
  need to understand the specific expectations associated with
  protecting information technology resources through the definition of
  clear and logical roles and responsibilities. In addition, key
  personnel need to be given sufficient authority to carry out their
  assigned responsibilities. Too often, good cyber security is left up
  to the initiative of the individual, which usually leads to
  inconsistent implementations and ineffective security. Establish a
  cyber security organizational structure that defines roles and
  responsibilities and clearly identifies how cyber security issues are
  escalated and who is notified in an emergency.
\item
  Document network architecture and identify systems that serve critical
  functions or contain sensitive information that require additional
  levels of protection. Develop and document a robust information
  security architecture as part of a process to establish an effective
  protection strategy. It is essential that organizations design their
  network with security in mind and continue to have a strong
  understanding of their network architecture throughout its lifecycle.
  Of particular importance, an in-depth understanding of the functions
  that the systems perform and the sensitivity of the stored information
  is required. Without this understanding, risk cannot be properly
  assessed and protection strategies may not be sufficient. Documenting
  the information security architecture and its components is critical
  to understanding the overall protection strategy and identifying
  single points of failure.
\item
  Establish a rigorous, ongoing risk-management process. A thorough
  understanding of the risks to network computing resources from
  denial-of-service attacks and the vulnerability of sensitive
  information to compromise is
\end{enumerate}

essential to an effective cyber security program. Risk assessments form
the technical basis of this understanding and are critical to
formulating effective strategies to mitigate vulnerabilities and
preserve the integrity of computing resources. Initially, perform a
baseline risk analysis based on current threat assessment to use for
developing a network protection strategy. Due to rapidly changing
technology and the emergence of new threats on a daily basis, an ongoing
risk-assessment process is also needed so routine changes can be made to
the protection strategy to ensure it remains effective. Fundamental to
risk management is identification of residual risk with a network
protection strategy in place and acceptance of that risk by management.
15. Establish a network protection strategy based on the principle of
defense in depth. A fundamental principle that must be part of any
network protection strategy is defense in depth. Defense in depth must
be considered early in the design phase of the development process and
must be an integral consideration in all technical decision-making
associated with the network. Utilize technical and administrative
controls to mitigate threats from identified risks to as great a degree
as possible at all levels of the network. Single points of failure must
be avoided, and cyber security defense must be layered to limit and
contain the impact of any security incidents. Additionally, each layer
must be protected against other systems at the same layer. For example,
to protect against the inside threat, restrict users to access only
those resources necessary to perform their job functions. 16. Clearly
identity cyber security requirements. Organizations and companies need
structured security programs with mandated requirements to establish
expectations and allow personnel to be held accountable. Formalized
policies and procedures are typically used to establish and
institutionalize a cyber security program. A formal program is essential
to establishing a consistent, standards-based approach to cyber security
through an organization and eliminates sole dependence on individual
initiative. Policies and procedures also inform employees of their
specific cyber security responsibilities and the consequences of failing
to meet those responsibilities. They also provide guidance regarding
actions to be taken during a cyber security incident and promote
efficient and effective actions during a time of crisis. As part of
identifying cyber security requirements, include user agreements and
notification and warning banners. Establish requirements to minimize the
threat from malicious insiders, including the need for conducting
background checks and limiting network privileges to those absolutely
necessary. 17. Establish effective configuration management processes. A
fundamental management process needed to maintain a secure network is
configuration management. Configuration management must cover both
hardware configurations and software configurations. Changes to hardware
or software can easily introduce vulnerabilities that undermine network
security. Processes are required to evaluate and control any change to
ensure that the network remains secure. Configuration management begins
with well-tested and documented security baselines for your various
systems.

\begin{enumerate}
\item
  Conduct routine self-assessments. Robust performance evaluation
  processes are needed to provide organizations with feedback on the
  effectiveness of cyber security policy and technical implementation. A
  sign of a mature organization is one that is able to identify issues,
  conduct root-cause analyses, and implement effective corrective
  actions that address individual and systemic problems. Self-assessment
  processes that are normally part of an effective cyber security
  program include routine scanning for vulnerabilities, automated
  auditing of the network, and self-assessments of organizational and
  individual performance.
\item
  Establish system backups and disaster recovery plans. Establish a
  disaster recovery plan that allows for rapid recovery from any
  emergency (including a cyber attack). System backups are an essential
  part of any plan and allow rapid reconstruction of the network.
  Routinely exercise disaster recovery plans to ensure that they work
  and that personnel are familiar with them. Make appropriate changes to
  disaster recovery plans based on lessons learned from exercises.
\item
  Senior organizational leadership should establish expectations for
  cyber security performance and hold individuals accountable for their
  performance. Effective cyber security performance requires commitment
  and leadership from senior managers in the organization. It is
  essential that senior management establish an expectation for strong
  cyber security and communicate this to their subordinate managers
  throughout the organization. It is also essential that senior
  organizational leadership establish a structure for implementation of
  a cyber security program. This structure will promote consistent
  implementation and the ability to sustain a strong cyber security
  program. It is then important for individuals to be held accountable
  for their performance as it relates to cyber security. This includes
  managers, system administrators, technicians, and users/operators.
\item
  Establish policies and conduct training to minimize the likelihood
  that organizational personnel will inadvertently disclose sensitive
  information regarding SCADA system design, operations, or security
  controls. Release data related to the SCADA network only on a strict,
  need-to-know basis and only to persons explicitly authorized to
  receive such information. ``Social engineering,'' the gathering of
  information about a computer or computer network via questions to
  naïve users, is often the first step in a malicious attack on computer
  networks. The more information revealed about a computer or network,
  the more vulnerable the computer or network is. Never divulge data
  revealed to a SCADA network, including names and contact information
  about the system operators and administrators, computer operating
  systems, or physical and logical locations of computers and network
  systems over the telephone or to personnel unless they are explicitly
  authorized to receive such information. Any requests for information
  by unknown persons should be sent to a central network security
  location for verification and fulfillment. People can be a weak link
  in an otherwise secure network. Conduct training and information
  awareness campaigns to ensure that personnel remain diligent in
  guarding sensitive network information, particularly their passwords.
\end{enumerate}

THE BOTTOM LINE Again, when it comes to the security of our nation and
even of water/wastewater treatment facilities, few have summed it up
better than Tom Ridge (Henry, 2002):

Now, obviously, the further removed we get from September 11, the
natural tendency is to let down our guard. Unfortunately, we cannot do
that. \ldots{} The government will continue to do everything we can to
find and stop those who seek to harm us. And I believe we owe it to the
American people to remind them that they must be vigilant, as well.
\end{document}